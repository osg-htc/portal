{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"hpc_administration/administrators/osg-flock/","text":"Submit Node Flocking to OSG \u00b6 This page has moved to https://opensciencegrid.org/docs/submit/osg-flock/","title":"Osg flock"},{"location":"hpc_administration/administrators/osg-flock/#submit-node-flocking-to-osg","text":"This page has moved to https://opensciencegrid.org/docs/submit/osg-flock/","title":"Submit Node Flocking to OSG"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/","text":"Simple Example of a DAGMan Workflow \u00b6 This guide walks you step-by-step through the construction and submission of a simple DAGMan workflow. We recommend this guide if you are interested in automating your job submissions. Overview \u00b6 In this guide: Introduction Structure of the DAG The Minimal DAG Input File The Submit Files Running the Simple DAG Monitoring the Simple DAG Wrapping Up For the full details on various DAGMan features, see the HTCondor manual pages: HTCondor's DAGMan Documentation 1. Introduction \u00b6 Consider the case of two HTCondor jobs that use the submit files A.sub and B.sub . Let's say that A.sub generates an output file ( output.txt ) that B.sub will analyze. To run this workflow manually, we would Submit the first HTCondor job with condor_submit A.sub . Wait for the first HTCondor job to complete successfully. Submit the second HTCondor job with condor_submit B.sub . If the first HTCondor job using A.sub is fairly short, then manually running this workflow is not a big deal. But if the first HTCondor job takes a long time to complete (maybe takes several hours to run, or has to wait for special resources), this can be very inconvenient. Instead, we can use DAGMan to automatically submit B.sub once the first HTCondor job using A.sub has completed successfully. This guide walks through the process of creating such a DAGMan workflow. 2. Structure of the DAG \u00b6 In this scenario, our workflow could be described as a DAG consisting of two nodes ( A.sub and B.sub ) connected by a single edge ( output.txt ). To represent this relationship, we will define nodes A and B - corresponding to A.sub and B.sub , respectively - and connect them with a line pointing from A and B , like in this figure: In order to use DAGMan to run this workflow, we need to communicate this structure to DAGMan via the .dag input file. 3. The Minimal DAG Input File \u00b6 Let's call the input file simple.dag . At minimum, the contents of the simple.dag input file are # simple.dag # Define the DAG jobs JOB A A.sub JOB B B.sub # Define the connections PARENT A CHILD B In a DAGMan input file, a node is defined using the JOB keyword, followed by the name of the node and the name of the corresponding submit file. In this case, we have created a node named A and instructed DAGMan to use the submit file A.sub for executing that node. We have similarly created node B and instructed DAGMan to use the submit file B.sub . (While there is no requirement that the name of the node match the name of the corresponding submit file, it is convenient to use a consistent naming scheme.) To connect the nodes, we use the PARENT .. CHILD .. syntax. Since node B requires that node A has completed successfully, we say that node A is the PARENT while node B is the CHILD . Note that we do not need to define why node B is dependent on node A , only that it is. 4. The Submit Files \u00b6 Now let's define simple examples of the submit files A.sub and B.sub . Node A \u00b6 First, the submit file A.sub uses the executable A.sh , which will generate the file called output.txt . We have explicitly told HTCondor to transfer back this file by using the transfer_output_files command. # A.sub executable = A.sh log = A.log output = A.out error = A.err transfer_output_files = output.txt +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue The executable file simply saves the hostname of the machine running the script: #!/bin/bash # A.sh hostname > output.txt sleep 1m # so we can see the job in \"running\" status Node B \u00b6 Second, the submit file B.sub uses the executable B.sh to print a message using the contents of the output.txt file generated by A.sh . We have explicitly told HTCondor to transfer output.txt as an input file for this job, using the transfer_input_files command. Thus we have finally defined the \"edge\" that connects nodes A and B : the use of output.txt . # B.sub executable = B.sh log = B.log output = B.out error = B.err transfer_input_files = output.txt +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue The executable file contains the command for printing the desired message, which will be printed to B.out . #!/bin/bash # B.sh echo \"The previous job was executed on the following machine:\" cat output.txt sleep 1m # so we can see the job in \"running\" status The directory structure \u00b6 Based on the contents of simple.dag , DAGMan is expecting that the submit files A.sub and B.sub are in the same directory as simple.dag . The submit files in turn are expecting A.sh and B.sh be in the same directory as A.sub and B.sub . Thus, we have the following directory structure: DAG_simple/ |-- A.sh |-- A.sub |-- B.sh |-- B.sub |-- simple.dag It is possible to organize each job into its own directory, but for now we will use this simple, flat organization. 5. Running the Simple DAG \u00b6 To run the DAG workflow described by simple.dag , we use the HTCondor command condor_submit_dag : condor_submit_dag simple.dag The DAGMan utility will then parse the input file and generate an assortment of related files that it will use for monitoring and managing your workflow. Here is the output of running the above command: [user@ap40 DAG_simple]$ condor_submit_dag simple.dag Loading classad userMap 'checkpoint_destination_map' ts=1699037029 from /etc/condor/checkpoint-destination-mapfile ----------------------------------------------------------------------- File for submitting this DAG to HTCondor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of HTCondor library output : simple.dag.lib.out Log of HTCondor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 562265. ----------------------------------------------------------------------- The output shows the list of standard files that are created with every DAG submission along with brief descriptions. A couple of additional files, some of them temporary, will be created during the lifetime of the DAG. 6. Monitoring the Simple DAG \u00b6 You can see the status of the DAG in your queue just like with any other HTCondor job submission. [user@ap40 DAG_simple]$ condor_q -- Schedd: ap40.uw.osg-htc.org : <128.105.68.92:9618?... @ 12/14/23 11:26:51 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS user simple.dag+562265 12/14 11:26 _ _ 1 2 562279.0 There are a couple of things to note about the condor_q output: The BATCH_NAME for the DAGMan job is the name of the input DAG file, simple.dag , plus the Job ID of the DAGMan scheduler job ( 562265 in this case): simple.dag+562265 . The total number of jobs for simple.dag+562265 corresponds to the total number of nodes in the DAG (2). Only 1 node is listed as \"Idle\", meaning that DAGMan has only submitted 1 job so far. This is consistent with the fact that node A has to complete before DAGMan can submit the job for node B . Note that if you are very quick to run your condor_q command after running your condor_submit_dag command, then you may see only the DAGMan scheduler job. It may take a few seconds for DAGMan to start up and submit the HTCondor job associated with the first node. To see more detailed information about the DAG workflow, use condor_q -nob -dag . For example, [user@ap40 DAG_simple]$ condor_q -dag -nob -- Schedd: ap40.uw.osg-htc.org : <128.105.68.92:9618?... @ 12/14/23 11:27:03 ID OWNER/NODENAME SUBMITTED RUN_TIME ST PRI SIZE CMD 562265.0 user 12/14 11:26 0+00:00:37 R 0 0.5 condor_dagman -p 0 -f -l . -Loc 562279.0 |-A 12/14 11:26 0+00:00:00 I 0 0.0 A.sh In this case, the first entry is the DAGMan scheduler job that you created when you first submitted the DAG. The following entries correspond to the nodes whose jobs are currently in the queue. Nodes that have not yet been submitted by DAGMan or that have completed and thus left the queue will not show up in your condor_q output. 7. Wrapping Up \u00b6 After waiting enough time, this simple DAG workflow should complete without any issues. But of course, that will not be the case for every DAG, especially as you start to create your own. DAGMan has a lot more features for managing and submitting DAG workflows, ranging from how to handle errors, combining DAG workflows, and restarting failed DAG workflows. For now, we recommend that you continue exploring DAGMan by going through our Intermediate DAGMan Tutorial . There is also our guide Overview: Submit Workflows with HTCondor's DAGMan , which contains links to more resources in the More Resources section. Finally, the definitive guide to DAGMan and DAG workflows is HTCondor's DAGMan Documentation .","title":"Simple Example of a DAGMan Workflow"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#simple-example-of-a-dagman-workflow","text":"This guide walks you step-by-step through the construction and submission of a simple DAGMan workflow. We recommend this guide if you are interested in automating your job submissions.","title":"Simple Example of a DAGMan Workflow"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#overview","text":"In this guide: Introduction Structure of the DAG The Minimal DAG Input File The Submit Files Running the Simple DAG Monitoring the Simple DAG Wrapping Up For the full details on various DAGMan features, see the HTCondor manual pages: HTCondor's DAGMan Documentation","title":"Overview"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#1-introduction","text":"Consider the case of two HTCondor jobs that use the submit files A.sub and B.sub . Let's say that A.sub generates an output file ( output.txt ) that B.sub will analyze. To run this workflow manually, we would Submit the first HTCondor job with condor_submit A.sub . Wait for the first HTCondor job to complete successfully. Submit the second HTCondor job with condor_submit B.sub . If the first HTCondor job using A.sub is fairly short, then manually running this workflow is not a big deal. But if the first HTCondor job takes a long time to complete (maybe takes several hours to run, or has to wait for special resources), this can be very inconvenient. Instead, we can use DAGMan to automatically submit B.sub once the first HTCondor job using A.sub has completed successfully. This guide walks through the process of creating such a DAGMan workflow.","title":"1. Introduction"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#2-structure-of-the-dag","text":"In this scenario, our workflow could be described as a DAG consisting of two nodes ( A.sub and B.sub ) connected by a single edge ( output.txt ). To represent this relationship, we will define nodes A and B - corresponding to A.sub and B.sub , respectively - and connect them with a line pointing from A and B , like in this figure: In order to use DAGMan to run this workflow, we need to communicate this structure to DAGMan via the .dag input file.","title":"2. Structure of the DAG"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#3-the-minimal-dag-input-file","text":"Let's call the input file simple.dag . At minimum, the contents of the simple.dag input file are # simple.dag # Define the DAG jobs JOB A A.sub JOB B B.sub # Define the connections PARENT A CHILD B In a DAGMan input file, a node is defined using the JOB keyword, followed by the name of the node and the name of the corresponding submit file. In this case, we have created a node named A and instructed DAGMan to use the submit file A.sub for executing that node. We have similarly created node B and instructed DAGMan to use the submit file B.sub . (While there is no requirement that the name of the node match the name of the corresponding submit file, it is convenient to use a consistent naming scheme.) To connect the nodes, we use the PARENT .. CHILD .. syntax. Since node B requires that node A has completed successfully, we say that node A is the PARENT while node B is the CHILD . Note that we do not need to define why node B is dependent on node A , only that it is.","title":"3. The Minimal DAG Input File"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#4-the-submit-files","text":"Now let's define simple examples of the submit files A.sub and B.sub .","title":"4. The Submit Files"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#node-a","text":"First, the submit file A.sub uses the executable A.sh , which will generate the file called output.txt . We have explicitly told HTCondor to transfer back this file by using the transfer_output_files command. # A.sub executable = A.sh log = A.log output = A.out error = A.err transfer_output_files = output.txt +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue The executable file simply saves the hostname of the machine running the script: #!/bin/bash # A.sh hostname > output.txt sleep 1m # so we can see the job in \"running\" status","title":"Node A"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#node-b","text":"Second, the submit file B.sub uses the executable B.sh to print a message using the contents of the output.txt file generated by A.sh . We have explicitly told HTCondor to transfer output.txt as an input file for this job, using the transfer_input_files command. Thus we have finally defined the \"edge\" that connects nodes A and B : the use of output.txt . # B.sub executable = B.sh log = B.log output = B.out error = B.err transfer_input_files = output.txt +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue The executable file contains the command for printing the desired message, which will be printed to B.out . #!/bin/bash # B.sh echo \"The previous job was executed on the following machine:\" cat output.txt sleep 1m # so we can see the job in \"running\" status","title":"Node B"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#the-directory-structure","text":"Based on the contents of simple.dag , DAGMan is expecting that the submit files A.sub and B.sub are in the same directory as simple.dag . The submit files in turn are expecting A.sh and B.sh be in the same directory as A.sub and B.sub . Thus, we have the following directory structure: DAG_simple/ |-- A.sh |-- A.sub |-- B.sh |-- B.sub |-- simple.dag It is possible to organize each job into its own directory, but for now we will use this simple, flat organization.","title":"The directory structure"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#5-running-the-simple-dag","text":"To run the DAG workflow described by simple.dag , we use the HTCondor command condor_submit_dag : condor_submit_dag simple.dag The DAGMan utility will then parse the input file and generate an assortment of related files that it will use for monitoring and managing your workflow. Here is the output of running the above command: [user@ap40 DAG_simple]$ condor_submit_dag simple.dag Loading classad userMap 'checkpoint_destination_map' ts=1699037029 from /etc/condor/checkpoint-destination-mapfile ----------------------------------------------------------------------- File for submitting this DAG to HTCondor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of HTCondor library output : simple.dag.lib.out Log of HTCondor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 562265. ----------------------------------------------------------------------- The output shows the list of standard files that are created with every DAG submission along with brief descriptions. A couple of additional files, some of them temporary, will be created during the lifetime of the DAG.","title":"5. Running the Simple DAG"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#6-monitoring-the-simple-dag","text":"You can see the status of the DAG in your queue just like with any other HTCondor job submission. [user@ap40 DAG_simple]$ condor_q -- Schedd: ap40.uw.osg-htc.org : <128.105.68.92:9618?... @ 12/14/23 11:26:51 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS user simple.dag+562265 12/14 11:26 _ _ 1 2 562279.0 There are a couple of things to note about the condor_q output: The BATCH_NAME for the DAGMan job is the name of the input DAG file, simple.dag , plus the Job ID of the DAGMan scheduler job ( 562265 in this case): simple.dag+562265 . The total number of jobs for simple.dag+562265 corresponds to the total number of nodes in the DAG (2). Only 1 node is listed as \"Idle\", meaning that DAGMan has only submitted 1 job so far. This is consistent with the fact that node A has to complete before DAGMan can submit the job for node B . Note that if you are very quick to run your condor_q command after running your condor_submit_dag command, then you may see only the DAGMan scheduler job. It may take a few seconds for DAGMan to start up and submit the HTCondor job associated with the first node. To see more detailed information about the DAG workflow, use condor_q -nob -dag . For example, [user@ap40 DAG_simple]$ condor_q -dag -nob -- Schedd: ap40.uw.osg-htc.org : <128.105.68.92:9618?... @ 12/14/23 11:27:03 ID OWNER/NODENAME SUBMITTED RUN_TIME ST PRI SIZE CMD 562265.0 user 12/14 11:26 0+00:00:37 R 0 0.5 condor_dagman -p 0 -f -l . -Loc 562279.0 |-A 12/14 11:26 0+00:00:00 I 0 0.0 A.sh In this case, the first entry is the DAGMan scheduler job that you created when you first submitted the DAG. The following entries correspond to the nodes whose jobs are currently in the queue. Nodes that have not yet been submitted by DAGMan or that have completed and thus left the queue will not show up in your condor_q output.","title":"6. Monitoring the Simple DAG"},{"location":"htc_workloads/automated_workflows/dagman-simple-example/#7-wrapping-up","text":"After waiting enough time, this simple DAG workflow should complete without any issues. But of course, that will not be the case for every DAG, especially as you start to create your own. DAGMan has a lot more features for managing and submitting DAG workflows, ranging from how to handle errors, combining DAG workflows, and restarting failed DAG workflows. For now, we recommend that you continue exploring DAGMan by going through our Intermediate DAGMan Tutorial . There is also our guide Overview: Submit Workflows with HTCondor's DAGMan , which contains links to more resources in the More Resources section. Finally, the definitive guide to DAGMan and DAG workflows is HTCondor's DAGMan Documentation .","title":"7. Wrapping Up"},{"location":"htc_workloads/automated_workflows/dagman-workflows/","text":"Overview: Submit Workflows with HTCondor's DAGMan \u00b6 If you want to automate job submission, keep reading to learn about HTCondor's DAGMan utility. Overview \u00b6 In this guide: Introduction What is DAGMan? The Basics of the DAG Input File Running a DAG Workflow DAGMan Features More Resources Introduction \u00b6 If your work requires jobs that run in a particular sequence, you may benefit from a workflow tool that submits and monitors jobs for you in the correct order. HTCondor has a built in utility called \"DAGMan\" that automates the job submission of such a workflow. This talk (originally presented at HTCondor Week 2020) gives a good introduction to DAGMan and its most useful features: DAGMan can be a powerful tool for creating large and complex HTCondor workflows. What is DAGMan? \u00b6 DAGMan is short for \"DAG Manager\", and is a utility built into HTCondor for automatically running a workflow (DAG) of jobs, where the results of an earlier job are required for running a later job. This workflow is similar to a flowchart with a definite beginning and ending. More specificially, \"DAG\" is an acronym for Directed Acyclic Graph , a concept from the mathematic field of graph theory: Graph: a collection of points (\"nodes\" or \"vertices\") connected to each other by lines (\"edges\"). Directed: the edges between nodes have direction, that is, each edge begins on one node and ends on a different node. Acyclic: the graph does not have a cycle - or loop - where the graph returns to a previous node. By using a directed acyclic graph, we can guarantee that the workflow has a defined 'start' and 'end'. In DAGMan, each node in the workflow corresponds to a job submission (i.e., condor_submit ). Each edge in the workflow corresponds to a set of files that are the output of one job submission and the input of another job submission. For convenience, we refer to such a workflow and the files necessary to execute it as \"the DAG\". The Basics of the DAG Input File \u00b6 The purpose of the DAG input file (typically .dag ) is to instruct DAGMan on the structure of the workflow you want to run. Additional instructions can be included in the DAG input file about how to manage the job submissions, rerun jobs (nodes), or to run pre- or post-processing scripts. In general, the structure of the .dag input file consists of one instruction per line, with each line starting with a keyword defining the type of instruction. 1. Defining the DAG jobs \u00b6 To define a DAG job, we begin a new line with JOB then provide the name, the submit file, and any additional options. The syntax is JOB JobName JobSubmitFile [additional options] where you need to replace JobName with the name you would like the DAG job to have, and JobSubmitFile with the name or path of the corresponding submit file. Both JobName and JobSubmitFile need to be specified. Every node in your workflow must have a JOB entry in the .dag input file. While there are other instructions that can reference a particular node, they will only work if the node in question has a corresponding JOB entry. 2. Defining the connections \u00b6 To define the relationship between DAG jobs in a workflow, we begin a new line with PARENT then the name of the first DAG job, followed by CHILD and the name of the second DAG job. That is, the PARENT DAG job must complete successfully before DAGMan will submit the CHILD DAG job. In fact, you can define such relationship for many DAG jobs (nodes) at the same time. Thus, the syntax is PARENT p1 [p2 ...] CHILD c1 [c2 ...] where you replace p# with the JobName for each parent DAG job, and c# with the JobName for each child DAG job. The child DAG jobs will only be submitted if all of the parent DAG jobs are completed successfully. Each JobName you provide must have a corresponding JOB entry elsewhere in the .dag input file. Technically, DAGMan does not require that each DAG job in a workflow is connected to another DAG job. This allows you to submit many unrelated DAG jobs at one time using DAGMan. Note that in defining the PARENT - CHILD relationship, there is no definition of how they are related. Effectively, DAGMan does not need to know the reason why the PARENT DAG jobs must complete successfully in order to submit the CHILD DAG jobs. There can be many reasons why you might want to execute the DAG jobs in this order, although the most common reason is that the PARENT DAG jobs create files that are required by the CHILD DAG jobs. In that case, it is up to you to organize the submit files of those DAG jobs in such a way that the output of the PARENT DAG jobs can be used as the input of the CHILD DAG jobs. In the DAGMan Features section, we will discuss tools that can assist you with this endeavor. Running a DAG Workflow \u00b6 1. Submitting the DAG \u00b6 Because the DAG workflow represents a special type of job, a special command is used to submit it. To submit the DAG workflow, use condor_submit_dag example.dag where example.dag is the name of your DAG input file containing the JOB and PARENT - CHILD definitions for your workflow. This will create and submit a \"DAGMan job\" that will in turn be responsible for submitting and monitoring the job nodes described in your DAG input file. A set of files is created for every DAG submission, and the output of the condor_submit_dag lists the files with a brief description. For the above submit command, the output will look like: ------------------------------------------------------------------------ File for submitting this DAG to HTCondor : example.dag.condor.sub Log of DAGMan debugging messages : example.dag.dagman.out Log of HTCondor library output : example.dag.lib.out Log of HTCondor library error messages : example.dag.lib.err Log of the life of condor_dagman itself : example.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster ######. ------------------------------------------------------------------------ 2. Monitoring the DAG \u00b6 The DAGMan job is actually a \"scheduler\" job (described by example.dag.condor.sub ) and the status and progress of the DAGMan job is saved to example.dag.dagman.out . Using condor_q or condor_watch_q , the DAGMan job will be under the name example.dag+###### , where ###### is the Cluster ID of the DAGMan scheduler job. Each job submitted by DAGMan, however, will be assigned a separate Cluster ID. For a more detailed status display, you can use condor_q -dag -nobatch If you want to see the status of just the DAGMan job proper, use condor_q -dag -nobatch -constr 'JobUniverse == 7' (Technically, this shows all \"scheduler\" type HTCondor jobs, but for most users this will only include DAGMan jobs.) For even more details about the execution of the DAG workflow, you can examine the contents of the example.dag.dagman.out file. The file contains timestamped log information of the execution and status of nodes in the DAG, along with statistics. As the DAG progresses, it will also create the files example.dag.metrics and example.dag.nodes.log , where the metrics file contains the current statistics of the DAG and the log file is an aggregate of the individual nodes' user log files. If you want to see the status of a specific node, use condor_q -dag -nobatch -constr 'DAGNodeName == \"YourNodeName\"' where YourNodeName should be replaced with the name of the node you want to know the status of. Note that this works only for jobs that are currently in the queue; if the node has not yet been submitted, or if it has completed and thus exited the queue, then you will not see the node using this command. To see if the node has completed, you should examine the contents of the .dagman.out file. A simple way to see the relevant log messages is to use a command like grep \"Node YourNodeName\" example.dag.dagman.out If you'd like to monitor the status of the individual nodes in your DAG workflow using condor_watch_q , then wait long enough for the .nodes.log file to be generated. Then run condor_watch_q -file example.dag.nodes.log Now condor_watch_q will update when DAGMan submits another job. 3. Removing the DAG \u00b6 To remove the DAG, you need to condor_rm the Cluster ID corresponding to the DAGMan scheduler job. This will also remove the jobs that the DAGMan scheduler job submitted as part of executing the DAG workflow. A removed DAG is almost always marked as a failed DAG, and as such will generate a rescue DAG (see below). DAGMan Features \u00b6 1. Pre- and post-processing for DAG jobs \u00b6 You can tell DAGMan to execute a script before or after it submits the HTCondor job for a particular node. Such a script will be executed on the submit server itself and can be used to set up the files needed for the HTCondor job, or to clean up or validate the files after a successful HTCondor job. The instructions for executing these scripts are placed in the input .dag file. You must specify the name of the node the script is attached to and whether the script is to be executed before ( PRE ) or after ( POST ) the HTCondor job. Here is a simple example: # Define the node (required) (example node named \"my_node\") JOB my_node run.sub # Define the script for executing before submitting run.sub (optional) SCRIPT PRE my_node setup.sh # Define a script for executing after run.sub has completed (optional) SCRIPT POST my_node cleanup.sh In this example, when it is time for DAGMan to execute the node my_node , it will take the following steps: Execute setup.sh (the PRE script) Submit the HTCondor job run.sub (the node's JOB ) Wait for the HTCondor job to complete Execute cleanup.sh (the POST script) All of these steps count as part of DAGMan's attempt to execute the node my_node and may affect whether DAGMan considers the node to have succeeded or failed. For more information on PRE and POST scripts as well as other scripts that DAGMan can use, see the HTCondor documentation . 2. Retrying failed nodes \u00b6 You can tell DAGMan to automatically retry a node if it fails. This way you don't have to manually restart the DAG if the job failed due to a transient issue. The instructions for how many times to retry a node go in the input .dag file. You must specify the node and the maximum number of times that DAGMan should attempt to retry that node. Here is a simple example: # Define the node (required) (example node named \"my_node\") JOB my_node run.sub # Define the number of times to retry \"my_node\" RETRY my_node 2 In this example, if the job associated with node my_node fails for some reason, then DAGMan will resubmit run.sub up to 2 more times. You can also apply the retry for statement to all nodes in the DAG by specifying ALL_NODES instead of a specific node name. For example, RETRY ALL_NODES 2 As a general rule, you should not set the number of retry attempts to more than 1 or 2 times. If a job is failing repeatedly, it is better to troubleshoot the cause of that failure. This is especially true when you applying the RETRY statement to all of the nodes in your DAG. DAGMan considers the exit code of the last executed step when it considers the success or failure of the node overall. There are various possible combinations that can determine the success or failure of the node itself, as discussed in the HTCondor documentation here . DAGMan only considers the success/failure of the node as a whole when deciding if it needs to attempt a retry. Importantly, if the .sub file for a node submits multiple HTCondor jobs, when any one of those jobs fails, DAGMan considers all of the jobs to have failed and will remove them from queue. Finally, note that DAGMan does not consider an HTCondor job with a \"hold\" status as being completed. In that case, you can include a command in the submit file to automatically remove a held job from the queue. When a job is removed from the queue, DAGMan considers that job to be failed (though as noted above, failure of the HTCondor job does not necessarily mean the node has failed). For more information on the RETRY statement, see the HTCondor documentation . 3. Restarting a failed DAG \u00b6 Generally, a DAG is considered failed if any one of its component nodes has failed. That does not mean, however, that DAGMan immediately stops the DAG. Instead, when DAGMan encounters a failed node, it will attempt to complete as much of the DAG as possible that does not require that node. Only then will DAGMan stop running the workflow. When the DAGMan job exits from a failed DAG, it generates a report of the status of the nodes in a file called a \"Rescue DAG\" with the extension .rescue### , starting from .rescue001 and counting up each time a Rescue DAG is generated. The Rescue DAG can then be used by DAGMan to restart the DAG, skipping over nodes that are marked as completed successfully and jumping directly to the failed nodes that need to be resubmitted. The power of this feature is that DAGMan will not duplicate the work of already completed nodes, which is especially useful when there is an issue at the end of a large DAG. DAGMan will automatically use a Rescue DAG if it exists when you use condor_submit_dag to submit the original .dag input file. If more than one Rescue DAG exists for a given .dag input file, then DAGMan will use the most recent Rescue DAG (the one with the highest number at the end of .rescue### ). # Automatically use the Rescue DAG if it exists condor_submit_dag example.dag If you do NOT want DAGMan to use an existing Rescue DAG, then you can use the `-force` option to start the DAG completely from scratch: # Do NOT use the Rescue DAG if it exists condor_submit_dag -force example.dag For more information on Rescue DAGs and how to explicitly control them, see the HTCondor documentation . If the DAGMan scheduler job itself crashes (or is placed on hold) and is unable to write a Rescue DAG, then when the DAGMan job is resubmitted (or released), DAGMan will go into \"recovery mode\". Essentially this involves DAGMan reconstructing the Rescue DAG that should have been written, but wasn't due to the job interruption. DAGMan will then resume the DAG based on its analysis of the files that do exist. More Resources \u00b6 Tutorials \u00b6 If you are interested in using DAGMan to automatically run a workflow, we highly recommend that you first go through our tutorial Simple Example of a DAG Workflow . This tutorial takes you step by step through the mechanics of creating and submitting a DAG. Once you've understood the basics from the simple tutorial, you are ready to explore more examples and scenarios in our Intermediate DAGMan Tutorial . Trainings & Videos \u00b6 A recent live training covering the materials in the Intermediate DAGMan Tutorial was held by the current lead developer for HTCondor's DAGMan utility: DAGMan: HTCondor's Workflow Manager . An introductory tutorial to DAGMan previously presented at HTCondor Week was recorded and is available on YouTube: HTCondor DAGMan Workflows tutorial . More recently, the current lead developer of HTCondor's DAGMan utility gave an intermediate tutorial: HTC23 DAGMan intermediate . Documentation \u00b6 HTCondor's DAGMan Documentation The HTCondor documentation is the definitive guide to DAGMan and contains a wealth of information about DAGMan, its features, and its behaviors.","title":"Overview: Submit Workflows with HTCondor's DAGMan"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#overview-submit-workflows-with-htcondors-dagman","text":"If you want to automate job submission, keep reading to learn about HTCondor's DAGMan utility.","title":"Overview: Submit Workflows with HTCondor's DAGMan"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#overview","text":"In this guide: Introduction What is DAGMan? The Basics of the DAG Input File Running a DAG Workflow DAGMan Features More Resources","title":"Overview"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#introduction","text":"If your work requires jobs that run in a particular sequence, you may benefit from a workflow tool that submits and monitors jobs for you in the correct order. HTCondor has a built in utility called \"DAGMan\" that automates the job submission of such a workflow. This talk (originally presented at HTCondor Week 2020) gives a good introduction to DAGMan and its most useful features: DAGMan can be a powerful tool for creating large and complex HTCondor workflows.","title":"Introduction"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#what-is-dagman","text":"DAGMan is short for \"DAG Manager\", and is a utility built into HTCondor for automatically running a workflow (DAG) of jobs, where the results of an earlier job are required for running a later job. This workflow is similar to a flowchart with a definite beginning and ending. More specificially, \"DAG\" is an acronym for Directed Acyclic Graph , a concept from the mathematic field of graph theory: Graph: a collection of points (\"nodes\" or \"vertices\") connected to each other by lines (\"edges\"). Directed: the edges between nodes have direction, that is, each edge begins on one node and ends on a different node. Acyclic: the graph does not have a cycle - or loop - where the graph returns to a previous node. By using a directed acyclic graph, we can guarantee that the workflow has a defined 'start' and 'end'. In DAGMan, each node in the workflow corresponds to a job submission (i.e., condor_submit ). Each edge in the workflow corresponds to a set of files that are the output of one job submission and the input of another job submission. For convenience, we refer to such a workflow and the files necessary to execute it as \"the DAG\".","title":"What is DAGMan?"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#the-basics-of-the-dag-input-file","text":"The purpose of the DAG input file (typically .dag ) is to instruct DAGMan on the structure of the workflow you want to run. Additional instructions can be included in the DAG input file about how to manage the job submissions, rerun jobs (nodes), or to run pre- or post-processing scripts. In general, the structure of the .dag input file consists of one instruction per line, with each line starting with a keyword defining the type of instruction.","title":"The Basics of the DAG Input File"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#1-defining-the-dag-jobs","text":"To define a DAG job, we begin a new line with JOB then provide the name, the submit file, and any additional options. The syntax is JOB JobName JobSubmitFile [additional options] where you need to replace JobName with the name you would like the DAG job to have, and JobSubmitFile with the name or path of the corresponding submit file. Both JobName and JobSubmitFile need to be specified. Every node in your workflow must have a JOB entry in the .dag input file. While there are other instructions that can reference a particular node, they will only work if the node in question has a corresponding JOB entry.","title":"1. Defining the DAG jobs"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#2-defining-the-connections","text":"To define the relationship between DAG jobs in a workflow, we begin a new line with PARENT then the name of the first DAG job, followed by CHILD and the name of the second DAG job. That is, the PARENT DAG job must complete successfully before DAGMan will submit the CHILD DAG job. In fact, you can define such relationship for many DAG jobs (nodes) at the same time. Thus, the syntax is PARENT p1 [p2 ...] CHILD c1 [c2 ...] where you replace p# with the JobName for each parent DAG job, and c# with the JobName for each child DAG job. The child DAG jobs will only be submitted if all of the parent DAG jobs are completed successfully. Each JobName you provide must have a corresponding JOB entry elsewhere in the .dag input file. Technically, DAGMan does not require that each DAG job in a workflow is connected to another DAG job. This allows you to submit many unrelated DAG jobs at one time using DAGMan. Note that in defining the PARENT - CHILD relationship, there is no definition of how they are related. Effectively, DAGMan does not need to know the reason why the PARENT DAG jobs must complete successfully in order to submit the CHILD DAG jobs. There can be many reasons why you might want to execute the DAG jobs in this order, although the most common reason is that the PARENT DAG jobs create files that are required by the CHILD DAG jobs. In that case, it is up to you to organize the submit files of those DAG jobs in such a way that the output of the PARENT DAG jobs can be used as the input of the CHILD DAG jobs. In the DAGMan Features section, we will discuss tools that can assist you with this endeavor.","title":"2. Defining the connections"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#running-a-dag-workflow","text":"","title":"Running a DAG Workflow"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#1-submitting-the-dag","text":"Because the DAG workflow represents a special type of job, a special command is used to submit it. To submit the DAG workflow, use condor_submit_dag example.dag where example.dag is the name of your DAG input file containing the JOB and PARENT - CHILD definitions for your workflow. This will create and submit a \"DAGMan job\" that will in turn be responsible for submitting and monitoring the job nodes described in your DAG input file. A set of files is created for every DAG submission, and the output of the condor_submit_dag lists the files with a brief description. For the above submit command, the output will look like: ------------------------------------------------------------------------ File for submitting this DAG to HTCondor : example.dag.condor.sub Log of DAGMan debugging messages : example.dag.dagman.out Log of HTCondor library output : example.dag.lib.out Log of HTCondor library error messages : example.dag.lib.err Log of the life of condor_dagman itself : example.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster ######. ------------------------------------------------------------------------","title":"1. Submitting the DAG"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#2-monitoring-the-dag","text":"The DAGMan job is actually a \"scheduler\" job (described by example.dag.condor.sub ) and the status and progress of the DAGMan job is saved to example.dag.dagman.out . Using condor_q or condor_watch_q , the DAGMan job will be under the name example.dag+###### , where ###### is the Cluster ID of the DAGMan scheduler job. Each job submitted by DAGMan, however, will be assigned a separate Cluster ID. For a more detailed status display, you can use condor_q -dag -nobatch If you want to see the status of just the DAGMan job proper, use condor_q -dag -nobatch -constr 'JobUniverse == 7' (Technically, this shows all \"scheduler\" type HTCondor jobs, but for most users this will only include DAGMan jobs.) For even more details about the execution of the DAG workflow, you can examine the contents of the example.dag.dagman.out file. The file contains timestamped log information of the execution and status of nodes in the DAG, along with statistics. As the DAG progresses, it will also create the files example.dag.metrics and example.dag.nodes.log , where the metrics file contains the current statistics of the DAG and the log file is an aggregate of the individual nodes' user log files. If you want to see the status of a specific node, use condor_q -dag -nobatch -constr 'DAGNodeName == \"YourNodeName\"' where YourNodeName should be replaced with the name of the node you want to know the status of. Note that this works only for jobs that are currently in the queue; if the node has not yet been submitted, or if it has completed and thus exited the queue, then you will not see the node using this command. To see if the node has completed, you should examine the contents of the .dagman.out file. A simple way to see the relevant log messages is to use a command like grep \"Node YourNodeName\" example.dag.dagman.out If you'd like to monitor the status of the individual nodes in your DAG workflow using condor_watch_q , then wait long enough for the .nodes.log file to be generated. Then run condor_watch_q -file example.dag.nodes.log Now condor_watch_q will update when DAGMan submits another job.","title":"2. Monitoring the DAG"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#3-removing-the-dag","text":"To remove the DAG, you need to condor_rm the Cluster ID corresponding to the DAGMan scheduler job. This will also remove the jobs that the DAGMan scheduler job submitted as part of executing the DAG workflow. A removed DAG is almost always marked as a failed DAG, and as such will generate a rescue DAG (see below).","title":"3. Removing the DAG"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#dagman-features","text":"","title":"DAGMan Features"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#1-pre-and-post-processing-for-dag-jobs","text":"You can tell DAGMan to execute a script before or after it submits the HTCondor job for a particular node. Such a script will be executed on the submit server itself and can be used to set up the files needed for the HTCondor job, or to clean up or validate the files after a successful HTCondor job. The instructions for executing these scripts are placed in the input .dag file. You must specify the name of the node the script is attached to and whether the script is to be executed before ( PRE ) or after ( POST ) the HTCondor job. Here is a simple example: # Define the node (required) (example node named \"my_node\") JOB my_node run.sub # Define the script for executing before submitting run.sub (optional) SCRIPT PRE my_node setup.sh # Define a script for executing after run.sub has completed (optional) SCRIPT POST my_node cleanup.sh In this example, when it is time for DAGMan to execute the node my_node , it will take the following steps: Execute setup.sh (the PRE script) Submit the HTCondor job run.sub (the node's JOB ) Wait for the HTCondor job to complete Execute cleanup.sh (the POST script) All of these steps count as part of DAGMan's attempt to execute the node my_node and may affect whether DAGMan considers the node to have succeeded or failed. For more information on PRE and POST scripts as well as other scripts that DAGMan can use, see the HTCondor documentation .","title":"1. Pre- and post-processing for DAG jobs"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#2-retrying-failed-nodes","text":"You can tell DAGMan to automatically retry a node if it fails. This way you don't have to manually restart the DAG if the job failed due to a transient issue. The instructions for how many times to retry a node go in the input .dag file. You must specify the node and the maximum number of times that DAGMan should attempt to retry that node. Here is a simple example: # Define the node (required) (example node named \"my_node\") JOB my_node run.sub # Define the number of times to retry \"my_node\" RETRY my_node 2 In this example, if the job associated with node my_node fails for some reason, then DAGMan will resubmit run.sub up to 2 more times. You can also apply the retry for statement to all nodes in the DAG by specifying ALL_NODES instead of a specific node name. For example, RETRY ALL_NODES 2 As a general rule, you should not set the number of retry attempts to more than 1 or 2 times. If a job is failing repeatedly, it is better to troubleshoot the cause of that failure. This is especially true when you applying the RETRY statement to all of the nodes in your DAG. DAGMan considers the exit code of the last executed step when it considers the success or failure of the node overall. There are various possible combinations that can determine the success or failure of the node itself, as discussed in the HTCondor documentation here . DAGMan only considers the success/failure of the node as a whole when deciding if it needs to attempt a retry. Importantly, if the .sub file for a node submits multiple HTCondor jobs, when any one of those jobs fails, DAGMan considers all of the jobs to have failed and will remove them from queue. Finally, note that DAGMan does not consider an HTCondor job with a \"hold\" status as being completed. In that case, you can include a command in the submit file to automatically remove a held job from the queue. When a job is removed from the queue, DAGMan considers that job to be failed (though as noted above, failure of the HTCondor job does not necessarily mean the node has failed). For more information on the RETRY statement, see the HTCondor documentation .","title":"2. Retrying failed nodes"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#3-restarting-a-failed-dag","text":"Generally, a DAG is considered failed if any one of its component nodes has failed. That does not mean, however, that DAGMan immediately stops the DAG. Instead, when DAGMan encounters a failed node, it will attempt to complete as much of the DAG as possible that does not require that node. Only then will DAGMan stop running the workflow. When the DAGMan job exits from a failed DAG, it generates a report of the status of the nodes in a file called a \"Rescue DAG\" with the extension .rescue### , starting from .rescue001 and counting up each time a Rescue DAG is generated. The Rescue DAG can then be used by DAGMan to restart the DAG, skipping over nodes that are marked as completed successfully and jumping directly to the failed nodes that need to be resubmitted. The power of this feature is that DAGMan will not duplicate the work of already completed nodes, which is especially useful when there is an issue at the end of a large DAG. DAGMan will automatically use a Rescue DAG if it exists when you use condor_submit_dag to submit the original .dag input file. If more than one Rescue DAG exists for a given .dag input file, then DAGMan will use the most recent Rescue DAG (the one with the highest number at the end of .rescue### ). # Automatically use the Rescue DAG if it exists condor_submit_dag example.dag If you do NOT want DAGMan to use an existing Rescue DAG, then you can use the `-force` option to start the DAG completely from scratch: # Do NOT use the Rescue DAG if it exists condor_submit_dag -force example.dag For more information on Rescue DAGs and how to explicitly control them, see the HTCondor documentation . If the DAGMan scheduler job itself crashes (or is placed on hold) and is unable to write a Rescue DAG, then when the DAGMan job is resubmitted (or released), DAGMan will go into \"recovery mode\". Essentially this involves DAGMan reconstructing the Rescue DAG that should have been written, but wasn't due to the job interruption. DAGMan will then resume the DAG based on its analysis of the files that do exist.","title":"3. Restarting a failed DAG"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#more-resources","text":"","title":"More Resources"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#tutorials","text":"If you are interested in using DAGMan to automatically run a workflow, we highly recommend that you first go through our tutorial Simple Example of a DAG Workflow . This tutorial takes you step by step through the mechanics of creating and submitting a DAG. Once you've understood the basics from the simple tutorial, you are ready to explore more examples and scenarios in our Intermediate DAGMan Tutorial .","title":"Tutorials"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#trainings-videos","text":"A recent live training covering the materials in the Intermediate DAGMan Tutorial was held by the current lead developer for HTCondor's DAGMan utility: DAGMan: HTCondor's Workflow Manager . An introductory tutorial to DAGMan previously presented at HTCondor Week was recorded and is available on YouTube: HTCondor DAGMan Workflows tutorial . More recently, the current lead developer of HTCondor's DAGMan utility gave an intermediate tutorial: HTC23 DAGMan intermediate .","title":"Trainings &amp; Videos"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#documentation","text":"HTCondor's DAGMan Documentation The HTCondor documentation is the definitive guide to DAGMan and contains a wealth of information about DAGMan, its features, and its behaviors.","title":"Documentation"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/","text":"Intermediate DAGMan: Uses and Features \u00b6 This tutorial helps you explore HTCondor's DAGMan its many features. You can download the tutorial materials with the following command: $ git clone https://github.com/OSGConnect/tutorial-dagman-intermediate Now move into the new directory to see the contents of the tutorial: $ cd tutorial-dagman-intermediate At the top level is a worked example of a \"Diamond DAG\" that summarizes the basic components of a creating, submitting, and managing DAGMan workflows. In the lower level additional_examples directory are more worked examples with their own README s highlighting specific features that can be used with DAGMan. Brief descriptions of these examples are provided in the Additional Examples section at the end of this tutorial. Before working on this tutorial, we recommend that you read through our other DAGMan guides: Overview: Submit Workflows with HTCondor's DAGMan Simple Example of a DAGMan Workflow The definitive guide to DAGMan is HTCondor's DAGMan Documentation . Types of DAGs \u00b6 While any workflow that satisfies the definition of a \"Directed Acyclic Graph\" (DAG) can be executed using DAGMan, there are certain types that are the most commonly used: Sequential DAG : all the nodes are connected in a sequence of one after the other, with no branching or splitting. This is good for conducting increasingly refined analyses of a dataset or initial result, or chaining together a long-running calculation. The simplest example of this type is used in the guide Simple Example of a DAGMan Workflow . Split and recombine DAG : the first node is connected to many nodes of the same layer (split) which then all connect back to the final node (recombine). Here, you can set up the shared environment in the first node and use it to parallelize the work into many individual jobs, then finally combine/analyze the results in the final node. The simplest example of this type is the \"Diamond DAG\" - the subject of this tutorial. Collection DAG : no node is connected to any other node. This is good for the situation where you need to run a bunch of otherwise unrelated jobs, perhaps ones that are competing for a limited resource. The simplest example of this type is a DAG consisting of a single node. These types are by no means \"official\", nor are they the only types of structure that a DAG can take. Rather, they serve as starting points from which you can build your own DAG workflow, which will likely consist of some combination of the above elements. The Diamond DAG \u00b6 As mentioned above, the \"Diamond DAG\" is the simplest example of a \"split and recombine\" DAG. In this case, the first node TOP is connected to two nodes LEFT and RIGHT (the \"split\"), which are then connected to the final node BOTTOM (the \"recombine\"). To describe the flow of the DAG and the parts needed to execute it, DAGMan uses a custom description language in an input file, typically named <DAG Name>.dag . The two most important commands in the DAG description language are: JOB <NodeName> <NodeSubmitFile> - Describes a node and the submit file it will use to run the node. PARENT <NodeName1> CHILD <NodeName2> - Describes the edge starting from <NodeName1> and pointing to <NodeName2> . These commands have been used to construct the Diamond DAG and are saved in the file diamond.dag . To view the contents of diamond.dag , run $ cat diamond.dag Before you continue, we recommend that you closely examine the contents of diamond.dag and identify its components. Furthermore, try to identify the submit file for each node, and use that submit file to determine the nature of the HTCondor job that will be submitted for each node. Submitting a DAG \u00b6 To submit a DAGMan workflow to HTCondor, you can use one of the following commands: $ condor_submit_dag diamond.dag or $ htcondor dag submit diamond.dag What Happens? \u00b6 When a DAG is submitted to HTCondor a special job is created to run DAGMan on behalf of you the user. This job runs the provided HTCSS DAGMan executable in the AP job queue. This is an actual job that can be queried and acted upon. You may also notice that lots of files are created. These files are all part of DAGMan and have various purposes. In general, the files that should always exist are as follows: DAGMan job proper files <DAG Name>.condor.sub - Submit file for the DAGMan job proper <DAG Name>.dagman.log - Job event log file for the DAGMan job proper <DAG Name>.lib.err - Standard error stream file for the DAGMan job proper <DAG Name>.lib.out - Standard output stream file for the DAGMan job proper Informational DAGMan files <DAG Name>.dagman.out - General DAGMan process logging file <DAG Name>.nodes.log - Collective job event log file for all managed jobs (Heart of DAGMan) <DAG Name>.metrics - JSON formatted information about the DAG Of these files, the two most important are the <DAG Name>.dagman.out and <DAG Name>.nodes.log . The .dagman.out file contains the entire history and status of DAGMan's execution of your workflow. The .nodes.log file on the other hand is the accumulated log entries for every HTCondor job that DAGMan submitted, and DAGMan monitors the contents of this file to generate the contents of the .dagman.out file. Note: these are not all the files that DAGMan can produce. Depending on the options and features you employ in your DAG input file, more files with different purposes can be created. Monitoring DAGMan \u00b6 The DAGMan job and the jobs in the DAG workflow can be found in the AP job queue and so the normal methods of job monitoring work. That also means that you can interact with these jobs, though in a more limited fashion than a regular job (see Running and Managing DAGMan for more details). A plain condor_q command will show a condensed batch view of the jobs submitted, running, and managed by the DAGMan job proper. For more information about jobs running under DAGMan, use the -nobatch and -dag flags: # Basic job query (Batched/Condensed) $ condor_q # Non-Batched query $ condor_q -nobatch # Increased information $ condor_q -nobatch -dag You can also watch the progress of the DAG and the jobs running under it by running: $ condor_watch_q Note that condor_watch_q works by monitoring the log files of jobs that are in the queue, but only at the time of its execution. Additional jobs submitted by DAGMan while condor_watch_q is running will not appear in condor_watch_q . To see additional jobs as they are submitted, wait for DAGMan to create the .nodes.log file, then run $ condor_watch_q -files *.log For more detail about the status and progress of your DAG workflow, you can use the noun-verb command: $ htcondor dag status DAGManJobID where DAGManJobID is the ID for the DAGMan job proper. Note that the information in the output of this command does not update frequently, and so it is not suited for short-lived DAG workflows such as the current example. When your DAG workflow has completed, the DAGMan job proper will disappear from the queue. If the DAG workflow completed successfully, then the .dag.dagman.out file should have a message that All jobs Completed! , though it may be difficult to find manually (try using grep \"All jobs Completed!\" *.dag.dagman.out instead). If the DAG workflow was aborted due to an error, then the .dag.dagman.out file should have the message Aborting DAG... . Assuming that the DAGMan job proper did not crash, then regardless the final line of the .dag.dagman.out file should contain (condor_DAGMAN) pid ####### EXITING WITH STATUS # , where the number after STATUS is the exit code (0 if success, not 0 if failure). How DAGMan Handles Relative Paths \u00b6 By default, the directory that DAGMan submits all jobs from is the same directory you are in when you run condor_submit_dag . This directory (let's call it the submit directory) is the starting directory for any relative path in the .dag input file or in the node .sub files that DAGMan submits . This can be observed by inspecting the sleep.sub submit file in the SleepJob sub-directory and by inspecting the diamond.dag input file. In the diamond.dag file, the jobs are declared using a relative path. For example: JOB TOP ./SleepJob/sleep.sub This tells DAGMan that the submit file for the JOB TOP is sleep.sub , located in the SleepJob in the submit directory ( . ). Similarly, the submit file sleep.sub uses paths relative to the submit directory for defining the save locations for the .log , .out , and .err files, i.e., log = ./SleepJob/$(JOB).log This behavior is consistent with submission of regular (non-DAGMan) jobs, e.g. condor_submit SleepJob/sleep.sub . Contrary to the above behavior, the .dag.* log/output files generated by the DAGMan job proper will always be in the same directory as the .dag input file. This is just the default behavior, and there are ways to make the location of job submission/management more obvious. See the HTCondor documentation for more details: File Paths in DAGs . Additional Examples \u00b6 Additional examples that cover various topics related to DAGMan are provided in the folder additional_examples with corresponding READMEs. The following order of the examples is recommended: RescueDag - Example for DAGs that don't exit successfully PreScript - Example using a pre-script for a node PostScript - Example using a post-script for a node Retry - Example for retrying a failed node VARS - Example of reusing a single submit file for multiple nodes with differing variables SubDAG (advanced) - Example using a subDAG Splice (advanced) - Example of using DAG splices","title":"Intermediate DAGMan: Uses and Features"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#intermediate-dagman-uses-and-features","text":"This tutorial helps you explore HTCondor's DAGMan its many features. You can download the tutorial materials with the following command: $ git clone https://github.com/OSGConnect/tutorial-dagman-intermediate Now move into the new directory to see the contents of the tutorial: $ cd tutorial-dagman-intermediate At the top level is a worked example of a \"Diamond DAG\" that summarizes the basic components of a creating, submitting, and managing DAGMan workflows. In the lower level additional_examples directory are more worked examples with their own README s highlighting specific features that can be used with DAGMan. Brief descriptions of these examples are provided in the Additional Examples section at the end of this tutorial. Before working on this tutorial, we recommend that you read through our other DAGMan guides: Overview: Submit Workflows with HTCondor's DAGMan Simple Example of a DAGMan Workflow The definitive guide to DAGMan is HTCondor's DAGMan Documentation .","title":"Intermediate DAGMan: Uses and Features"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#types-of-dags","text":"While any workflow that satisfies the definition of a \"Directed Acyclic Graph\" (DAG) can be executed using DAGMan, there are certain types that are the most commonly used: Sequential DAG : all the nodes are connected in a sequence of one after the other, with no branching or splitting. This is good for conducting increasingly refined analyses of a dataset or initial result, or chaining together a long-running calculation. The simplest example of this type is used in the guide Simple Example of a DAGMan Workflow . Split and recombine DAG : the first node is connected to many nodes of the same layer (split) which then all connect back to the final node (recombine). Here, you can set up the shared environment in the first node and use it to parallelize the work into many individual jobs, then finally combine/analyze the results in the final node. The simplest example of this type is the \"Diamond DAG\" - the subject of this tutorial. Collection DAG : no node is connected to any other node. This is good for the situation where you need to run a bunch of otherwise unrelated jobs, perhaps ones that are competing for a limited resource. The simplest example of this type is a DAG consisting of a single node. These types are by no means \"official\", nor are they the only types of structure that a DAG can take. Rather, they serve as starting points from which you can build your own DAG workflow, which will likely consist of some combination of the above elements.","title":"Types of DAGs"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#the-diamond-dag","text":"As mentioned above, the \"Diamond DAG\" is the simplest example of a \"split and recombine\" DAG. In this case, the first node TOP is connected to two nodes LEFT and RIGHT (the \"split\"), which are then connected to the final node BOTTOM (the \"recombine\"). To describe the flow of the DAG and the parts needed to execute it, DAGMan uses a custom description language in an input file, typically named <DAG Name>.dag . The two most important commands in the DAG description language are: JOB <NodeName> <NodeSubmitFile> - Describes a node and the submit file it will use to run the node. PARENT <NodeName1> CHILD <NodeName2> - Describes the edge starting from <NodeName1> and pointing to <NodeName2> . These commands have been used to construct the Diamond DAG and are saved in the file diamond.dag . To view the contents of diamond.dag , run $ cat diamond.dag Before you continue, we recommend that you closely examine the contents of diamond.dag and identify its components. Furthermore, try to identify the submit file for each node, and use that submit file to determine the nature of the HTCondor job that will be submitted for each node.","title":"The Diamond DAG"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#submitting-a-dag","text":"To submit a DAGMan workflow to HTCondor, you can use one of the following commands: $ condor_submit_dag diamond.dag or $ htcondor dag submit diamond.dag","title":"Submitting a DAG"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#what-happens","text":"When a DAG is submitted to HTCondor a special job is created to run DAGMan on behalf of you the user. This job runs the provided HTCSS DAGMan executable in the AP job queue. This is an actual job that can be queried and acted upon. You may also notice that lots of files are created. These files are all part of DAGMan and have various purposes. In general, the files that should always exist are as follows: DAGMan job proper files <DAG Name>.condor.sub - Submit file for the DAGMan job proper <DAG Name>.dagman.log - Job event log file for the DAGMan job proper <DAG Name>.lib.err - Standard error stream file for the DAGMan job proper <DAG Name>.lib.out - Standard output stream file for the DAGMan job proper Informational DAGMan files <DAG Name>.dagman.out - General DAGMan process logging file <DAG Name>.nodes.log - Collective job event log file for all managed jobs (Heart of DAGMan) <DAG Name>.metrics - JSON formatted information about the DAG Of these files, the two most important are the <DAG Name>.dagman.out and <DAG Name>.nodes.log . The .dagman.out file contains the entire history and status of DAGMan's execution of your workflow. The .nodes.log file on the other hand is the accumulated log entries for every HTCondor job that DAGMan submitted, and DAGMan monitors the contents of this file to generate the contents of the .dagman.out file. Note: these are not all the files that DAGMan can produce. Depending on the options and features you employ in your DAG input file, more files with different purposes can be created.","title":"What Happens?"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#monitoring-dagman","text":"The DAGMan job and the jobs in the DAG workflow can be found in the AP job queue and so the normal methods of job monitoring work. That also means that you can interact with these jobs, though in a more limited fashion than a regular job (see Running and Managing DAGMan for more details). A plain condor_q command will show a condensed batch view of the jobs submitted, running, and managed by the DAGMan job proper. For more information about jobs running under DAGMan, use the -nobatch and -dag flags: # Basic job query (Batched/Condensed) $ condor_q # Non-Batched query $ condor_q -nobatch # Increased information $ condor_q -nobatch -dag You can also watch the progress of the DAG and the jobs running under it by running: $ condor_watch_q Note that condor_watch_q works by monitoring the log files of jobs that are in the queue, but only at the time of its execution. Additional jobs submitted by DAGMan while condor_watch_q is running will not appear in condor_watch_q . To see additional jobs as they are submitted, wait for DAGMan to create the .nodes.log file, then run $ condor_watch_q -files *.log For more detail about the status and progress of your DAG workflow, you can use the noun-verb command: $ htcondor dag status DAGManJobID where DAGManJobID is the ID for the DAGMan job proper. Note that the information in the output of this command does not update frequently, and so it is not suited for short-lived DAG workflows such as the current example. When your DAG workflow has completed, the DAGMan job proper will disappear from the queue. If the DAG workflow completed successfully, then the .dag.dagman.out file should have a message that All jobs Completed! , though it may be difficult to find manually (try using grep \"All jobs Completed!\" *.dag.dagman.out instead). If the DAG workflow was aborted due to an error, then the .dag.dagman.out file should have the message Aborting DAG... . Assuming that the DAGMan job proper did not crash, then regardless the final line of the .dag.dagman.out file should contain (condor_DAGMAN) pid ####### EXITING WITH STATUS # , where the number after STATUS is the exit code (0 if success, not 0 if failure).","title":"Monitoring DAGMan"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#how-dagman-handles-relative-paths","text":"By default, the directory that DAGMan submits all jobs from is the same directory you are in when you run condor_submit_dag . This directory (let's call it the submit directory) is the starting directory for any relative path in the .dag input file or in the node .sub files that DAGMan submits . This can be observed by inspecting the sleep.sub submit file in the SleepJob sub-directory and by inspecting the diamond.dag input file. In the diamond.dag file, the jobs are declared using a relative path. For example: JOB TOP ./SleepJob/sleep.sub This tells DAGMan that the submit file for the JOB TOP is sleep.sub , located in the SleepJob in the submit directory ( . ). Similarly, the submit file sleep.sub uses paths relative to the submit directory for defining the save locations for the .log , .out , and .err files, i.e., log = ./SleepJob/$(JOB).log This behavior is consistent with submission of regular (non-DAGMan) jobs, e.g. condor_submit SleepJob/sleep.sub . Contrary to the above behavior, the .dag.* log/output files generated by the DAGMan job proper will always be in the same directory as the .dag input file. This is just the default behavior, and there are ways to make the location of job submission/management more obvious. See the HTCondor documentation for more details: File Paths in DAGs .","title":"How DAGMan Handles Relative Paths"},{"location":"htc_workloads/automated_workflows/tutorial-dagman-intermediate/#additional-examples","text":"Additional examples that cover various topics related to DAGMan are provided in the folder additional_examples with corresponding READMEs. The following order of the examples is recommended: RescueDag - Example for DAGs that don't exit successfully PreScript - Example using a pre-script for a node PostScript - Example using a post-script for a node Retry - Example for retrying a failed node VARS - Example of reusing a single submit file for multiple nodes with differing variables SubDAG (advanced) - Example using a subDAG Splice (advanced) - Example of using DAG splices","title":"Additional Examples"},{"location":"htc_workloads/automated_workflows/tutorial-pegasus/","text":"Pegasus Workflows \u00b6 Introduction \u00b6 The Pegasus project encompasses a set of technologies that help workflow-based applications execute in a number of different environments including desktops, campus clusters, grids, and clouds. Pegasus bridges the scientific domain and the execution environment by automatically mapping high-level workflow descriptions onto distributed resources. It automatically locates the necessary input data and computational resources necessary for workflow execution. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware. Some of the advantages of using Pegasus include: Portability / Reuse - User created workflows can easily be run in different environments without alteration. Pegasus currently runs workflows on compute systems scheduled via HTCondor, including the OSPool, and other other systems or via other schedulers (e.g. XSEDE resources, Amazon EC2, Google Cloud, and many campus clusters). The same workflow can run on a single system or across a heterogeneous set of resources. Performance - The Pegasus mapper can reorder, group, and prioritize tasks in order to increase the overall workflow performance. Scalability - Pegasus can easily scale both the size of the workflow, and the resources that the workflow is distributed over. Pegasus runs workflows ranging from just a few computational tasks up to 1 million tasks. The number of resources involved in executing a workflow can scale as needed without any impediments to performance. Provenance - By default, all jobs in Pegasus are launched via the kickstart process that captures runtime provenance of the job and helps in debugging. The provenance data is collected in a database, and the data can be summarized with tools such as pegasus-statistics or directly with SQL queries. Data Management - Pegasus handles replica selection, data transfers and output registrations in data catalogs. These tasks are added to a workflow as auxiliary jobs by the Pegasus planner. Reliability - Jobs and data transfers are automatically retried in case of failures. Debugging tools such as pegasus-analyzer help the user to debug the workflow in case of non-recoverable failures. Error Recovery - When errors occur, Pegasus tries to recover when possible by retrying tasks, retrying the entire workflow, providing workflow-level checkpointing, re-mapping portions of the workflow, trying alternative data sources for staging data, and, when all else fails, providing a rescue workflow containing a description of only the work that remains to be done. Pegasus keeps track of what has been done (provenance) including the locations of data used and produced, and which software was used with which parameters. As mentioned earlier in this book, OSG has no read/write enabled shared file system across the resources. Jobs are required to either bring inputs along with the job, or as part of the job stage the inputs from a remote location. The following examples highlight how Pegasus can be used to manage workloads in such an environment by providing an abstraction layer around things like data movements and job retries, enabling the users to run larger workloads, spending less time developing job management tools and babysitting their computations. Pegasus workflows have 4 components: Site Catalog - Describes the execution environment in which the workflow will be executed. Transformation Catalog - Specifies locations of the executables used by the workflow. Replica Catalog - Specifies locations of the input data to the workflow. Workflow Description - An abstract workflow description containing compute steps and dependencies between the steps. We refer to this workflow as abstract because it does not contain data locations and available software. When developing a Pegasus Workflow using the Python API , all four components may be defined in the same file. For details, please refer to the Pegasus documentation . Wordfreq Workflow \u00b6 wordfreq is an example application and workflow that can be used to introduce Pegasus tools and concepts. The application is available on the OSG Access Points. This example uses a custom container to run jobs. The container capability is provided by OSG ( Containers - Apptainer/Singularity ) and is used by setting HTCondor properties when defining your workflow. Exercise 1 : create a copy of the Pegasus tutorial and change the working directory to the wordfreq workflow by running the following commands: $ git clone https://github.com/OSGConnect/tutorial-pegasus $ cd tutorial-pegasus/wordfreq In the wordfreq directory, you will find: wordfreq/ \u251c\u2500\u2500 bin | \u251c\u2500\u2500 summarize | \u2514\u2500\u2500 wordfreq \u251c\u2500\u2500 inputs | \u251c\u2500\u2500 Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt | \u251c\u2500\u2500 Dracula_by_Bram_Stoker.txt | \u251c\u2500\u2500 Pride_and_Prejudice_by_Jane_Austen.txt | \u251c\u2500\u2500 The_Adventures_of_Huckleberry_Finn_by_Mark_Twain.txt | \u251c\u2500\u2500 Ulysses_by_James_Joyce.txt | \u2514\u2500\u2500 Visual_Signaling_By_Signal_Corps_United_States_Army.txt \u251c\u2500\u2500 many-more-inputs | \u2514\u2500\u2500 ... \u2514\u2500\u2500 workflow.py The inputs/ directory contains 6 public domain ebooks. The wordreq workflow uses the two executables in the bin/ directory. bin/wordfreq takes a text file as input and produces a summary output file containting the counts and names of the top five most frequently used words from the input file. A wordfreq job is created for each file in inputs/ . bin/summarize concatenates the the output of each wordfreq job into a single output file called summary.txt . This workflow structure, which is a set of independent tasks joining into a single summary or analysis type of task, is a common use case on OSG and therefore this workflow can be thought of as a template for such problems. For example, instead of using wordfreq on ebooks, the application could be protein folding on a set of input structures. When invoked, the workflow script ( workflow.py ) does the following major steps: Generates a site catalog, which describes the execution environment in which the workflow will be run. def generate_site_catalog(self): username = getpass.getuser() local = ( Site(\"local\") .add_directories( Directory( Directory.SHARED_STORAGE, self.output_dir ).add_file_servers( FileServer(f\"file://{self.output_dir}\", Operation.ALL) ) ) .add_directories( Directory( Directory.SHARED_SCRATCH, self.scratch_dir ).add_file_servers( FileServer(f\"file://{self.scratch_dir}\", Operation.ALL) ) ) ) condorpool = ( Site(\"condorpool\") .add_pegasus_profile(style=\"condor\") .add_condor_profile( universe=\"vanilla\", requirements=\"HAS_SINGULARITY == True\", request_cpus=1, request_memory=\"1 GB\", request_disk=\"1 GB\", ) .add_profiles( Namespace.CONDOR, key=\"+SingularityImage\", value='\"/cvmfs/singularity.opensciencegrid.org/htc/rocky:9\"' ) ) self.sc.add_sites(local, condorpool) In order for the workflow to use the container capability provided by OSG ( Containers - Apptainer/Singularity ), the following HTCondor profiles must be added to the condorpool execution site: +SingularityImage='\"/cvmfs/singularity.opensciencegrid.org/htc/rocky:9\"' . Generates the transformation catalog, which specifies the executables used in the workflow and contains the locations where they are physically located. In this example, we have two entries: wordfreq and summarize . def generate_transformation_catalog(self): wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=self.TOP_DIR / \"bin/wordfreq\", is_stageable=True ).add_pegasus_profile(clusters_size=1) summarize = Transformation( name=\"summarize\", site=\"local\", pfn=self.TOP_DIR / \"bin/summarize\", is_stageable=True ) self.tc.add_transformations(wordfreq, summarize) Generates the replica catalog, which specifies the physical locations of any input files used by the workflow. In this example, there is an entry for each file in the inputs/ directory. def generate_replica_catalog(self): input_files = [File(f.name) for f in (self.TOP_DIR / \"inputs\").iterdir() if f.name.endswith(\".txt\")] for f in input_files: self.rc.add_replica(site=\"local\", lfn=f, pfn=self.TOP_DIR / \"inputs\" / f.lfn) Builds the wordfreq workflow. Note that in this step there is no mention of data movement and job details as these are added by Pegasus when the workflow is planned into an executable workflow. As part of the planning process, additional jobs which handle scratch directory creation, data staging, and data cleanup are added to the workflow. def generate_workflow(self): # last job, child of all others summarize_job = ( Job(\"summarize\") .add_outputs(File(\"summary.txt\")) ) self.wf.add_jobs(summarize_job) input_files = [File(f.name) for f in (self.TOP_DIR / \"inputs\").iterdir() if f.name.endswith(\".txt\")] for f in input_files: out_file = File(f.lfn + \".out\") wordfreq_job = ( Job(\"wordfreq\") .add_args(f, out_file) .add_inputs(f) .add_outputs(out_file) ) self.wf.add_jobs(wordfreq_job) # establish the relationship between the jobs summarize_job.add_inputs(out_file) Exercise 2: Submit the workflow by executing workflow.py . $ ./workflow.py Note that when Pegasus plans/submits a workflow, a workflow directory is created and presented in the output. In the following example output, the workflow directory is /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 . 2020.12.18 14:33:07.059 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:07.064 CST: File for submitting this DAG to HTCondor : wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:07.070 CST: Log of DAGMan debugging messages : wordfreq-workflow-0.dag.dagman.out 2020.12.18 14:33:07.075 CST: Log of HTCondor library output : wordfreq-workflow-0.dag.lib.out 2020.12.18 14:33:07.080 CST: Log of HTCondor library error messages : wordfreq-workflow-0.dag.lib.err 2020.12.18 14:33:07.086 CST: Log of the life of condor_dagman itself : wordfreq-workflow-0.dag.dagman.log 2020.12.18 14:33:07.091 CST: 2020.12.18 14:33:07.096 CST: -no_submit given, not submitting DAG to HTCondor. You can do this with: 2020.12.18 14:33:07.107 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:10.381 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.347 CST: Created Pegasus database in: sqlite:////home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db 2020.12.18 14:33:11.352 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.404 CST: Output replica catalog set to jdbc:sqlite:/home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db [WARNING] Submitting to condor wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:12.060 CST: Time taken to execute is 5.818 seconds Your workflow has been started and is running in the base directory: /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To monitor the workflow you can run *** pegasus-status -l /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To remove your workflow run *** pegasus-remove /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 This directory is the handle to the workflow instance and is used by Pegasus command line tools. Some useful tools to know about: pegasus-status -v [wfdir] Provides status on a currently running workflow. ( more ) pegasus-analyzer [wfdir] Provides debugging clues why a workflow failed. Run this after a workflow has failed. ( more ) pegasus-statistics [wfdir] Provides statistics, such as walltimes, on a workflow after it has completed. ( more ) pegasus-remove [wfdir] Removes a workflow from the system. ( more ) Exercise 3: Check the status of the workflow: $ pegasus-status [wfdir] You can keep checking the status periodically to see that the workflow is making progress. Exercise 4: Examine a submit file and the *.dag.dagman.out files. Do these look familiar to you from previous modules in the book? Pegasus is based on HTCondor and DAGMan. $ cd [wfdir] $ cat 00/00/summarize_ID0000001.sub ... $ cat *.dag.dagman.out ... Exercise 5: Keep checking progress with pegasus-status . Once the workflow is done, display statistics with pegasus-statistics : $ pegasus-status [wfdir] $ pegasus-statistics [wfdir] ... Exercise 6: cd to the output directory and look at the outputs. Which is the most common word used in the 6 books? Hint: $ cd $HOME/workflows/outputs $ head -n 5 *.out Exercise 7: Want to try something larger? Copy the additional 994 ebooks from \\ the many-more-inputs/ directory to the inputs/ directory: $ cp many-more-inputs/* inputs/ As these tasks are really short, let's tell Pegasus to cluster multiple tasks together into jobs. If you do not do this step, the jobs will still run, but not very efficiently. This is because every job has a small scheduling overhead. For short jobs, the overhead is obvious. If we make the jobs longer, the scheduling overhead becomes negligible. To enable the clustering feature, edit the workflow.py script. Find the section under Transformations : wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=self.TOP_DIR / \"bin/wordfreq\", is_stageable=True ).add_pegasus_profile(clusters_size=1) Change clusters_size=1 to clusters_size=50 . This informs Pegasus that it is ok to cluster up to 50 of the jobs which use the wordfreq executable. Save the file and re-run the script: $ ./workflow.py Use pegasus-status and pegasus-statistics to monitor your workflow. Using pegasus-statistics , determine how many jobs ended up in your workflow and see how this compares with our initial workflow run. Variant Calling Workflow \u00b6 This workflow is based on the Data Carpentry lesson Lesson Data Wrangling and Processing for Genomics . This workflow downloads and aligns SRA data to the E. coli REL606 reference genome, and checks what differences exist in our reads versus the genome. The workflow also performs perform variant calling to see how the population changed over time. The inputs are controlled by the recipe.json file. With 3 SRA inputs, the structure of the workflow becomes: Rendering the workflow with data: Compared to the wordfreq example, a difference is the use of (OSDF)[https://osg-htc.org/services/osdf.html] for intermediate data transfers/storage. Note the extra site in the site catalog: osdf = ( Site(\"osdf\") .add_directories( Directory( Directory.SHARED_SCRATCH, f\"{osdf_local_base}/staging\" ).add_file_servers( FileServer(f\"osdf://{osdf_local_base}/staging\", Operation.ALL) ) ) ) Which is then referenced when planning the workflow: self.wf.plan( dir=str(self.runs_dir), output_dir=str(self.output_dir), sites=[\"condorpool\"], staging_sites={\"condorpool\": \"osdf\"}, OSDF is recommended for data sizes over 1 GB. To plan the workflow: $ ./workflow.py --recipe recipe.json Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the user documentation .","title":"Use Pegasus to Manage Workflows on OSPool Access Points"},{"location":"htc_workloads/automated_workflows/tutorial-pegasus/#pegasus-workflows","text":"","title":"Pegasus Workflows"},{"location":"htc_workloads/automated_workflows/tutorial-pegasus/#introduction","text":"The Pegasus project encompasses a set of technologies that help workflow-based applications execute in a number of different environments including desktops, campus clusters, grids, and clouds. Pegasus bridges the scientific domain and the execution environment by automatically mapping high-level workflow descriptions onto distributed resources. It automatically locates the necessary input data and computational resources necessary for workflow execution. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware. Some of the advantages of using Pegasus include: Portability / Reuse - User created workflows can easily be run in different environments without alteration. Pegasus currently runs workflows on compute systems scheduled via HTCondor, including the OSPool, and other other systems or via other schedulers (e.g. XSEDE resources, Amazon EC2, Google Cloud, and many campus clusters). The same workflow can run on a single system or across a heterogeneous set of resources. Performance - The Pegasus mapper can reorder, group, and prioritize tasks in order to increase the overall workflow performance. Scalability - Pegasus can easily scale both the size of the workflow, and the resources that the workflow is distributed over. Pegasus runs workflows ranging from just a few computational tasks up to 1 million tasks. The number of resources involved in executing a workflow can scale as needed without any impediments to performance. Provenance - By default, all jobs in Pegasus are launched via the kickstart process that captures runtime provenance of the job and helps in debugging. The provenance data is collected in a database, and the data can be summarized with tools such as pegasus-statistics or directly with SQL queries. Data Management - Pegasus handles replica selection, data transfers and output registrations in data catalogs. These tasks are added to a workflow as auxiliary jobs by the Pegasus planner. Reliability - Jobs and data transfers are automatically retried in case of failures. Debugging tools such as pegasus-analyzer help the user to debug the workflow in case of non-recoverable failures. Error Recovery - When errors occur, Pegasus tries to recover when possible by retrying tasks, retrying the entire workflow, providing workflow-level checkpointing, re-mapping portions of the workflow, trying alternative data sources for staging data, and, when all else fails, providing a rescue workflow containing a description of only the work that remains to be done. Pegasus keeps track of what has been done (provenance) including the locations of data used and produced, and which software was used with which parameters. As mentioned earlier in this book, OSG has no read/write enabled shared file system across the resources. Jobs are required to either bring inputs along with the job, or as part of the job stage the inputs from a remote location. The following examples highlight how Pegasus can be used to manage workloads in such an environment by providing an abstraction layer around things like data movements and job retries, enabling the users to run larger workloads, spending less time developing job management tools and babysitting their computations. Pegasus workflows have 4 components: Site Catalog - Describes the execution environment in which the workflow will be executed. Transformation Catalog - Specifies locations of the executables used by the workflow. Replica Catalog - Specifies locations of the input data to the workflow. Workflow Description - An abstract workflow description containing compute steps and dependencies between the steps. We refer to this workflow as abstract because it does not contain data locations and available software. When developing a Pegasus Workflow using the Python API , all four components may be defined in the same file. For details, please refer to the Pegasus documentation .","title":"Introduction"},{"location":"htc_workloads/automated_workflows/tutorial-pegasus/#wordfreq-workflow","text":"wordfreq is an example application and workflow that can be used to introduce Pegasus tools and concepts. The application is available on the OSG Access Points. This example uses a custom container to run jobs. The container capability is provided by OSG ( Containers - Apptainer/Singularity ) and is used by setting HTCondor properties when defining your workflow. Exercise 1 : create a copy of the Pegasus tutorial and change the working directory to the wordfreq workflow by running the following commands: $ git clone https://github.com/OSGConnect/tutorial-pegasus $ cd tutorial-pegasus/wordfreq In the wordfreq directory, you will find: wordfreq/ \u251c\u2500\u2500 bin | \u251c\u2500\u2500 summarize | \u2514\u2500\u2500 wordfreq \u251c\u2500\u2500 inputs | \u251c\u2500\u2500 Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt | \u251c\u2500\u2500 Dracula_by_Bram_Stoker.txt | \u251c\u2500\u2500 Pride_and_Prejudice_by_Jane_Austen.txt | \u251c\u2500\u2500 The_Adventures_of_Huckleberry_Finn_by_Mark_Twain.txt | \u251c\u2500\u2500 Ulysses_by_James_Joyce.txt | \u2514\u2500\u2500 Visual_Signaling_By_Signal_Corps_United_States_Army.txt \u251c\u2500\u2500 many-more-inputs | \u2514\u2500\u2500 ... \u2514\u2500\u2500 workflow.py The inputs/ directory contains 6 public domain ebooks. The wordreq workflow uses the two executables in the bin/ directory. bin/wordfreq takes a text file as input and produces a summary output file containting the counts and names of the top five most frequently used words from the input file. A wordfreq job is created for each file in inputs/ . bin/summarize concatenates the the output of each wordfreq job into a single output file called summary.txt . This workflow structure, which is a set of independent tasks joining into a single summary or analysis type of task, is a common use case on OSG and therefore this workflow can be thought of as a template for such problems. For example, instead of using wordfreq on ebooks, the application could be protein folding on a set of input structures. When invoked, the workflow script ( workflow.py ) does the following major steps: Generates a site catalog, which describes the execution environment in which the workflow will be run. def generate_site_catalog(self): username = getpass.getuser() local = ( Site(\"local\") .add_directories( Directory( Directory.SHARED_STORAGE, self.output_dir ).add_file_servers( FileServer(f\"file://{self.output_dir}\", Operation.ALL) ) ) .add_directories( Directory( Directory.SHARED_SCRATCH, self.scratch_dir ).add_file_servers( FileServer(f\"file://{self.scratch_dir}\", Operation.ALL) ) ) ) condorpool = ( Site(\"condorpool\") .add_pegasus_profile(style=\"condor\") .add_condor_profile( universe=\"vanilla\", requirements=\"HAS_SINGULARITY == True\", request_cpus=1, request_memory=\"1 GB\", request_disk=\"1 GB\", ) .add_profiles( Namespace.CONDOR, key=\"+SingularityImage\", value='\"/cvmfs/singularity.opensciencegrid.org/htc/rocky:9\"' ) ) self.sc.add_sites(local, condorpool) In order for the workflow to use the container capability provided by OSG ( Containers - Apptainer/Singularity ), the following HTCondor profiles must be added to the condorpool execution site: +SingularityImage='\"/cvmfs/singularity.opensciencegrid.org/htc/rocky:9\"' . Generates the transformation catalog, which specifies the executables used in the workflow and contains the locations where they are physically located. In this example, we have two entries: wordfreq and summarize . def generate_transformation_catalog(self): wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=self.TOP_DIR / \"bin/wordfreq\", is_stageable=True ).add_pegasus_profile(clusters_size=1) summarize = Transformation( name=\"summarize\", site=\"local\", pfn=self.TOP_DIR / \"bin/summarize\", is_stageable=True ) self.tc.add_transformations(wordfreq, summarize) Generates the replica catalog, which specifies the physical locations of any input files used by the workflow. In this example, there is an entry for each file in the inputs/ directory. def generate_replica_catalog(self): input_files = [File(f.name) for f in (self.TOP_DIR / \"inputs\").iterdir() if f.name.endswith(\".txt\")] for f in input_files: self.rc.add_replica(site=\"local\", lfn=f, pfn=self.TOP_DIR / \"inputs\" / f.lfn) Builds the wordfreq workflow. Note that in this step there is no mention of data movement and job details as these are added by Pegasus when the workflow is planned into an executable workflow. As part of the planning process, additional jobs which handle scratch directory creation, data staging, and data cleanup are added to the workflow. def generate_workflow(self): # last job, child of all others summarize_job = ( Job(\"summarize\") .add_outputs(File(\"summary.txt\")) ) self.wf.add_jobs(summarize_job) input_files = [File(f.name) for f in (self.TOP_DIR / \"inputs\").iterdir() if f.name.endswith(\".txt\")] for f in input_files: out_file = File(f.lfn + \".out\") wordfreq_job = ( Job(\"wordfreq\") .add_args(f, out_file) .add_inputs(f) .add_outputs(out_file) ) self.wf.add_jobs(wordfreq_job) # establish the relationship between the jobs summarize_job.add_inputs(out_file) Exercise 2: Submit the workflow by executing workflow.py . $ ./workflow.py Note that when Pegasus plans/submits a workflow, a workflow directory is created and presented in the output. In the following example output, the workflow directory is /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 . 2020.12.18 14:33:07.059 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:07.064 CST: File for submitting this DAG to HTCondor : wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:07.070 CST: Log of DAGMan debugging messages : wordfreq-workflow-0.dag.dagman.out 2020.12.18 14:33:07.075 CST: Log of HTCondor library output : wordfreq-workflow-0.dag.lib.out 2020.12.18 14:33:07.080 CST: Log of HTCondor library error messages : wordfreq-workflow-0.dag.lib.err 2020.12.18 14:33:07.086 CST: Log of the life of condor_dagman itself : wordfreq-workflow-0.dag.dagman.log 2020.12.18 14:33:07.091 CST: 2020.12.18 14:33:07.096 CST: -no_submit given, not submitting DAG to HTCondor. You can do this with: 2020.12.18 14:33:07.107 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:10.381 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.347 CST: Created Pegasus database in: sqlite:////home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db 2020.12.18 14:33:11.352 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.404 CST: Output replica catalog set to jdbc:sqlite:/home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db [WARNING] Submitting to condor wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:12.060 CST: Time taken to execute is 5.818 seconds Your workflow has been started and is running in the base directory: /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To monitor the workflow you can run *** pegasus-status -l /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To remove your workflow run *** pegasus-remove /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 This directory is the handle to the workflow instance and is used by Pegasus command line tools. Some useful tools to know about: pegasus-status -v [wfdir] Provides status on a currently running workflow. ( more ) pegasus-analyzer [wfdir] Provides debugging clues why a workflow failed. Run this after a workflow has failed. ( more ) pegasus-statistics [wfdir] Provides statistics, such as walltimes, on a workflow after it has completed. ( more ) pegasus-remove [wfdir] Removes a workflow from the system. ( more ) Exercise 3: Check the status of the workflow: $ pegasus-status [wfdir] You can keep checking the status periodically to see that the workflow is making progress. Exercise 4: Examine a submit file and the *.dag.dagman.out files. Do these look familiar to you from previous modules in the book? Pegasus is based on HTCondor and DAGMan. $ cd [wfdir] $ cat 00/00/summarize_ID0000001.sub ... $ cat *.dag.dagman.out ... Exercise 5: Keep checking progress with pegasus-status . Once the workflow is done, display statistics with pegasus-statistics : $ pegasus-status [wfdir] $ pegasus-statistics [wfdir] ... Exercise 6: cd to the output directory and look at the outputs. Which is the most common word used in the 6 books? Hint: $ cd $HOME/workflows/outputs $ head -n 5 *.out Exercise 7: Want to try something larger? Copy the additional 994 ebooks from \\ the many-more-inputs/ directory to the inputs/ directory: $ cp many-more-inputs/* inputs/ As these tasks are really short, let's tell Pegasus to cluster multiple tasks together into jobs. If you do not do this step, the jobs will still run, but not very efficiently. This is because every job has a small scheduling overhead. For short jobs, the overhead is obvious. If we make the jobs longer, the scheduling overhead becomes negligible. To enable the clustering feature, edit the workflow.py script. Find the section under Transformations : wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=self.TOP_DIR / \"bin/wordfreq\", is_stageable=True ).add_pegasus_profile(clusters_size=1) Change clusters_size=1 to clusters_size=50 . This informs Pegasus that it is ok to cluster up to 50 of the jobs which use the wordfreq executable. Save the file and re-run the script: $ ./workflow.py Use pegasus-status and pegasus-statistics to monitor your workflow. Using pegasus-statistics , determine how many jobs ended up in your workflow and see how this compares with our initial workflow run.","title":"Wordfreq Workflow"},{"location":"htc_workloads/automated_workflows/tutorial-pegasus/#variant-calling-workflow","text":"This workflow is based on the Data Carpentry lesson Lesson Data Wrangling and Processing for Genomics . This workflow downloads and aligns SRA data to the E. coli REL606 reference genome, and checks what differences exist in our reads versus the genome. The workflow also performs perform variant calling to see how the population changed over time. The inputs are controlled by the recipe.json file. With 3 SRA inputs, the structure of the workflow becomes: Rendering the workflow with data: Compared to the wordfreq example, a difference is the use of (OSDF)[https://osg-htc.org/services/osdf.html] for intermediate data transfers/storage. Note the extra site in the site catalog: osdf = ( Site(\"osdf\") .add_directories( Directory( Directory.SHARED_SCRATCH, f\"{osdf_local_base}/staging\" ).add_file_servers( FileServer(f\"osdf://{osdf_local_base}/staging\", Operation.ALL) ) ) ) Which is then referenced when planning the workflow: self.wf.plan( dir=str(self.runs_dir), output_dir=str(self.output_dir), sites=[\"condorpool\"], staging_sites={\"condorpool\": \"osdf\"}, OSDF is recommended for data sizes over 1 GB. To plan the workflow: $ ./workflow.py --recipe recipe.json","title":"Variant Calling Workflow"},{"location":"htc_workloads/automated_workflows/tutorial-pegasus/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the user documentation .","title":"Getting Help"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/","text":"Transfer Smaller Job Files To and From /home \u00b6 As described in the Overview: Data Staging and Transfer to Jobs any data, files, or even software that is <1GB should be staged in your /home directory on your Access Point. Files in your /home directory can be transferred to jobs via your HTCondor submit file. Transfer Files From /home Using HTCondor \u00b6 Transfer Input Files from /home \u00b6 To transfer input files from /home , list the files by name in the transfer_input_files submit file option. You can use either absolute or relative paths to your input files. Multiple files can be specified using a comma-separated list. To transfer files from your /home directory use the transfer_input_files statement in your HTCondor submit file. For example: # submit file example # transfer small file from /home transfer_input_files = my_data.csv Multiple files can be specified using a comma-separated list, for example: # transfer multiple files from /home transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py When using transfer_input_files to transfer files located in /home , keep in mind that the path to the file is relative to the location of the submit file. If you have files located in a different /home subdirectory, we recommend specifying the full path to those files, which is also a matter of good practice, for example: transfer_input_files = /home/username/path/to/my_software.tar.gz Note that the path is not replicated on the remote side. The job will only see my_software.tar.gz in the top level job directory. Above, username refers to your access point username. Use HTCondor To Transfer Outputs \u00b6 By default, HTCondor will transfer any new or modified files in the job's top-level directory back to your /home directory location from which the condor_submit command was performed. This behavior only applies to files in the top-level directory of where your job executes, meaning HTCondor will ignore any files created in subdirectories of the job's top-level directory. Several options exist for modifying this default output file transfer behavior, including those described in this guide. What is the top-level directory of a job? \u00b6 Before executing a job, HTCondor will create a new directory on the execute node just for your job - this is the top-level directory of the job and the path is stored in the environment variable _CONDOR_SCRATCH_DIR . All of the input files transferred via transfer_input_files will first be written to this directory and it is from this path that a job starts to execute. After a job has completed the top-level directory and all of it's contents are deleted. Select Specific Output Files To Transfer to /home Using HTCondor \u00b6 As described above, HTCondor will, by default, transfer any files that are generated during the execution of your job(s) back to your /home directory. If your job(s) will produce multiple output files but you only need to retain a subset of these output files, you can use a submit file option to only transfer back this file: transfer_output_files = output.svg Alternatively, you can delete the unrequired output files or move them to a subdirectory as a step in the bash executable script of your job - only the output files that remain in the top-level directory will be transferred back to your /home directory. Organize Output Files in /home \u00b6 By default, output files will be copied back to the directory in /home where you ran the condor_submit command. To modify these behavior, you can use the transfer_output_remaps option in the HTCondor submit file. The syntax for transfer_output_remaps is: transfer_output_remaps = \"Output1.txt = path/to/save/file/under/output.txt; Output2.txt = path/to/save/file/under/RenamedOutput.txt\" What if my output file(s) are not written to the top-level directory? \u00b6 If your output files are written to a subdirectory, use the steps described below to convert the output directory to a \"tarball\" that is written to the top-level directory. Alternatively, you can include steps in the executable bash script of your job to move (i.e. mv ) output files from a subdirectory to the top-level directory. For example, if there is an output file that needs to be transferred back to the login node named job_output.txt written to job_output/ : #! /bin/bash # various commands needed to run your job # move csv files to scratch dir mv job_output/job_output.txt $_CONDOR_SCRATCH_DIR Group Multiple Output Files For Convenience \u00b6 If your jobs will generate multiple output files, we recommend combining all output into a compressed tar archive for convenience, particularly when transferring your results to your local computer from your login node. To create a compressed tar archive, include commands in your your bash executable script to create a new subdirectory, move all of the output to this new subdirectory, and create a tar archive. For example: #! /bin/bash # various commands needed to run your job # create output tar archive mkdir my_output mv my_job_output.csv my_job_output.svg my_output/ tar -czf my_job.output.tar.gz my_ouput/ The example above will create a file called my_job.output.tar.gz that contains all the output that was moved to my_output . Be sure to create my_job.output.tar.gz in the top-level directory of where your job executes and HTCondor will automatically transfer this tar archive back to your /home directory.","title":"Transfer Smaller Job Files to and from /home"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#transfer-smaller-job-files-to-and-from-home","text":"As described in the Overview: Data Staging and Transfer to Jobs any data, files, or even software that is <1GB should be staged in your /home directory on your Access Point. Files in your /home directory can be transferred to jobs via your HTCondor submit file.","title":"Transfer Smaller Job Files To and From /home"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#transfer-files-from-home-using-htcondor","text":"","title":"Transfer Files From /home Using HTCondor"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#transfer-input-files-from-home","text":"To transfer input files from /home , list the files by name in the transfer_input_files submit file option. You can use either absolute or relative paths to your input files. Multiple files can be specified using a comma-separated list. To transfer files from your /home directory use the transfer_input_files statement in your HTCondor submit file. For example: # submit file example # transfer small file from /home transfer_input_files = my_data.csv Multiple files can be specified using a comma-separated list, for example: # transfer multiple files from /home transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py When using transfer_input_files to transfer files located in /home , keep in mind that the path to the file is relative to the location of the submit file. If you have files located in a different /home subdirectory, we recommend specifying the full path to those files, which is also a matter of good practice, for example: transfer_input_files = /home/username/path/to/my_software.tar.gz Note that the path is not replicated on the remote side. The job will only see my_software.tar.gz in the top level job directory. Above, username refers to your access point username.","title":"Transfer Input Files from /home"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#use-htcondor-to-transfer-outputs","text":"By default, HTCondor will transfer any new or modified files in the job's top-level directory back to your /home directory location from which the condor_submit command was performed. This behavior only applies to files in the top-level directory of where your job executes, meaning HTCondor will ignore any files created in subdirectories of the job's top-level directory. Several options exist for modifying this default output file transfer behavior, including those described in this guide.","title":"Use HTCondor To Transfer Outputs"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#what-is-the-top-level-directory-of-a-job","text":"Before executing a job, HTCondor will create a new directory on the execute node just for your job - this is the top-level directory of the job and the path is stored in the environment variable _CONDOR_SCRATCH_DIR . All of the input files transferred via transfer_input_files will first be written to this directory and it is from this path that a job starts to execute. After a job has completed the top-level directory and all of it's contents are deleted.","title":"What is the top-level directory of a job?"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#select-specific-output-files-to-transfer-to-home-using-htcondor","text":"As described above, HTCondor will, by default, transfer any files that are generated during the execution of your job(s) back to your /home directory. If your job(s) will produce multiple output files but you only need to retain a subset of these output files, you can use a submit file option to only transfer back this file: transfer_output_files = output.svg Alternatively, you can delete the unrequired output files or move them to a subdirectory as a step in the bash executable script of your job - only the output files that remain in the top-level directory will be transferred back to your /home directory.","title":"Select Specific Output Files To Transfer to /home Using HTCondor"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#organize-output-files-in-home","text":"By default, output files will be copied back to the directory in /home where you ran the condor_submit command. To modify these behavior, you can use the transfer_output_remaps option in the HTCondor submit file. The syntax for transfer_output_remaps is: transfer_output_remaps = \"Output1.txt = path/to/save/file/under/output.txt; Output2.txt = path/to/save/file/under/RenamedOutput.txt\"","title":"Organize Output Files in /home"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#what-if-my-output-files-are-not-written-to-the-top-level-directory","text":"If your output files are written to a subdirectory, use the steps described below to convert the output directory to a \"tarball\" that is written to the top-level directory. Alternatively, you can include steps in the executable bash script of your job to move (i.e. mv ) output files from a subdirectory to the top-level directory. For example, if there is an output file that needs to be transferred back to the login node named job_output.txt written to job_output/ : #! /bin/bash # various commands needed to run your job # move csv files to scratch dir mv job_output/job_output.txt $_CONDOR_SCRATCH_DIR","title":"What if my output file(s) are not written to the top-level directory?"},{"location":"htc_workloads/managing_data/file-transfer-via-htcondor/#group-multiple-output-files-for-convenience","text":"If your jobs will generate multiple output files, we recommend combining all output into a compressed tar archive for convenience, particularly when transferring your results to your local computer from your login node. To create a compressed tar archive, include commands in your your bash executable script to create a new subdirectory, move all of the output to this new subdirectory, and create a tar archive. For example: #! /bin/bash # various commands needed to run your job # create output tar archive mkdir my_output mv my_job_output.csv my_job_output.svg my_output/ tar -czf my_job.output.tar.gz my_ouput/ The example above will create a file called my_job.output.tar.gz that contains all the output that was moved to my_output . Be sure to create my_job.output.tar.gz in the top-level directory of where your job executes and HTCondor will automatically transfer this tar archive back to your /home directory.","title":"Group Multiple Output Files For Convenience"},{"location":"htc_workloads/managing_data/file-transfer-via-http/","text":"Transfer HTTP-available Files up to 1GB In Size \u00b6 Overview \u00b6 If some of the data or software your jobs depend on is available via the web, you can have such files transferred by HTCondor using the appropriate HTTP address! Important Considerations \u00b6 While our Overview of Data Mangement on the OSPool describes how you can stage data, files, or even software on OSG data locations, any web-accessible file can be transferred directly to your jobs IF : the file is accessible via an HTTP address the file is less than 1GB in size (if larger, you'll need to pre-stage them for OSDF ) the server or website they're on can handle large numbers of your jobs accessing them simultaneously Importantly, you'll also want to make sure your job executable knows how to handle the file (un-tar, etc.) from within the working directory of the job, just like it would for any other input file. Transfer Files via HTTP \u00b6 To download a file available by HTTP into a job, use an HTTP URL in combination with the transfer_input_files statement in your HTCondor submit file. For example: # submit file example # transfer software tarball from public via http transfer_input_files = http://www.website.com/path/file.tar.gz ...other submit file details... Multiple URLs can be specified using a comma-separated list, and a combination of URLs and files from /home directory can be provided in a comma separated list. For example, # transfer software tarball from public via http # transfer additional data from AP /home via htcondor file transfer transfer_input_files = http://www.website.com/path/file1.tar.gz, http://www.website.com/path/file2.tar.gz, my_data.csv","title":"Transfer HTTP-available Files up to 1GB In Size"},{"location":"htc_workloads/managing_data/file-transfer-via-http/#transfer-http-available-files-up-to-1gb-in-size","text":"","title":"Transfer HTTP-available Files up to 1GB In Size"},{"location":"htc_workloads/managing_data/file-transfer-via-http/#overview","text":"If some of the data or software your jobs depend on is available via the web, you can have such files transferred by HTCondor using the appropriate HTTP address!","title":"Overview"},{"location":"htc_workloads/managing_data/file-transfer-via-http/#important-considerations","text":"While our Overview of Data Mangement on the OSPool describes how you can stage data, files, or even software on OSG data locations, any web-accessible file can be transferred directly to your jobs IF : the file is accessible via an HTTP address the file is less than 1GB in size (if larger, you'll need to pre-stage them for OSDF ) the server or website they're on can handle large numbers of your jobs accessing them simultaneously Importantly, you'll also want to make sure your job executable knows how to handle the file (un-tar, etc.) from within the working directory of the job, just like it would for any other input file.","title":"Important Considerations"},{"location":"htc_workloads/managing_data/file-transfer-via-http/#transfer-files-via-http","text":"To download a file available by HTTP into a job, use an HTTP URL in combination with the transfer_input_files statement in your HTCondor submit file. For example: # submit file example # transfer software tarball from public via http transfer_input_files = http://www.website.com/path/file.tar.gz ...other submit file details... Multiple URLs can be specified using a comma-separated list, and a combination of URLs and files from /home directory can be provided in a comma separated list. For example, # transfer software tarball from public via http # transfer additional data from AP /home via htcondor file transfer transfer_input_files = http://www.website.com/path/file1.tar.gz, http://www.website.com/path/file2.tar.gz, my_data.csv","title":"Transfer Files via HTTP"},{"location":"htc_workloads/managing_data/osdf/","text":"Transfer Larger Job Files and Containers Using OSDF \u00b6 For input files >1GB and output files >1GB in size, the default HTCondor file transfer mechanisms run the risk of over-taxing the Access Point and their network capacity. And this is exactly why the OSDF ( Open Science Data Federation ) exists for researchers with larger per-job data! The OSDF is a network of data origins and caches for data distribution. If you have an account on an OSG Access Point, you have access to an OSDF data origin, specifically a directory that can be used to stage input and output data for jobs, accessible via the OSDF. This guide describes general tips for using the OSDF, where to stage your files, and how to access files from jobs. Important Considerations and Best Practices \u00b6 Use OSDF locations for larger files and containers : We recommend using the OSDF for files larger than 1GB (input or output) and all container files. OSDF files are cached across the Open Science Pool, any changes or modifications that you make might not be propagated. This means that if you add a new version of a file the OSDF directory, it must first be given a unique name (or directory path) to distinguish it from previous versions of that file. Adding a date or version number to directories or file names is strongly encouraged to manage your files uniqness. This is especially important when using the OSDF for software and containers. Never submit jobs from the OSDF locations; always submit jobs from within the /home directory. All log , error , output files and any other files smaller than the above values should ONLY ever exist within the user's /home directory. Files placed within a public OSDF directory are publicly accessible , discoverable and readable by anyone, via the web. At the moment, most default OSDF locations are not public. Where to Put Your Files \u00b6 Data origins and local mount points varies between the different access points. See the list below for the \"Local Path\" to use, based on your access point. Access Point OSDF Origin ap40.uw.osg-htc.org Accessible to user only: Local Path: /ospool/ap40/data/[USERNAME] Base OSDF URL: osdf:///ospool/ap40/data/[USERNAME] ap20.uc.osg-htc.org Accessible to user only: Local Path: /ospool/ap20/data/[USERNAME] Base OSDF URL: osdf:///ospool/ap20/data/[USERNAME] Accessible to project group only: Local Path: /ospool/uc-shared/projects/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/projects/[PROJECT] Public space for projects: Local Path: /ospool/uc-shared/public/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/public/[PROJECT] ap21.uc.osg-htc.org Accessible to user only: Local Path: /ospool/ap21/data/[USERNAME] Base OSDF URL: osdf:///ospool/ap21/data/[USERNAME] Accessible to project group only: Local Path: /ospool/uc-shared/project/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/project/[PROJECT] Public space for projects: Local Path: /ospool/uc-shared/public/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/public/[PROJECT] Transfer Files To/From Jobs Using the OSDF \u00b6 Use an 'osdf://' URL to Transfer Large Input Files and Containers \u00b6 Jobs will transfer data from the OSDF directory when files are indicated with an appropriate osdf:// URL (or the older stash:// ) in the transfer_input_files line of the submit file. Make sure to customize the base URL based on your Access Point, as described in the table above . Some examples: Transferring one file from /ospool/apXX/data/ transfer_input_files = osdf:///ospool/apXX/data/<username>/InFile.txt When using multiple files from /ospool/apXX/data/ , it can be useful to use HTCondor submit file variables to make your list of files more readable: # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///ospool/apXX/data/<username> transfer_input_files = $(OSDF_LOCATION)/InputFile.txt, $(OSDF_LOCATION)/database.sql Transferring a folder from /ospool/apXX/data/ transfer_input_files = osdf:///ospool/apXX/data/<username>/<folder>?recursive Please note that for transferring a folder using OSDF ?recursive needs to added after the folder name. Use transfer_output_remaps and 'osdf://' URL for Large Output Files \u00b6 To move output files into an OSDF directory, users should use the transfer_output_remaps option within their job's submit file, which will transfer the user's specified file to the specific location in the data origin. By using transfer_output_remaps , it is possible to specify what path to save a file to and what name to save it under. Using this approach, it is possible to save files back to specific locations in your OSDF directory (as well as your /home directory, if desired). The general syntax for transfer_output_remaps is: transfer_output_remaps = \"Output1.txt = path/to/save/file/under/output.txt; Output2.txt = path/to/save/file/under/RenamedOutput.txt\" When saving large output files back to /ospool/apXX/data/ , the path provided will look like: transfer_output_remaps = \"Output.txt = osdf:///ospool/apXX/data/<username>/Output.txt\" Some examples: Transferring one output file ( OutFile.txt ) back to /ospool/apXX/data/ : transfer_output_remaps = \"OutFile.txt=osdf:///ospool/apXX/data/<username>/OutFile.txt\" When using multiple files from /ospool/apXX/data/ , it can be useful to use HTCondor submit file variables to make your list of files more readable. Also note the semi-colon separator in the list of output files. # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///ospool/apXX/data/<username> transfer_output_remaps = \"file1.txt = $(OSDF_LOCATION)/file1.txt; file2.txt = $(OSDF_LOCATION)/file2.txt; file3.txt = $(OSDF_LOCATION)/file3.txt\" Phase out of stash:/// and stashcp command \u00b6 Historically, output files could be transferred from a job to an' OSDF location using the stashcp command within the job's executable. However, this mechanism is no longer encouraged for OSPool users. Instead, jobs should use transfer_output_remaps (an HTCondor feature) to transfer output files to your assigned OSDF origin. By using transfer_output_remaps , HTCondor will manage the output data transfer for your jobs. Data transferred via HTCondor is more likely to be transferred successfully and errors with transfer are more likely to be reported to the user. osdf:// is the new format for these kind of transfers, and is equivalent of the old stash:// format (which will keep on being supported for the short term).","title":"Transfer Larger Job Files and Containers Using OSDF"},{"location":"htc_workloads/managing_data/osdf/#transfer-larger-job-files-and-containers-using-osdf","text":"For input files >1GB and output files >1GB in size, the default HTCondor file transfer mechanisms run the risk of over-taxing the Access Point and their network capacity. And this is exactly why the OSDF ( Open Science Data Federation ) exists for researchers with larger per-job data! The OSDF is a network of data origins and caches for data distribution. If you have an account on an OSG Access Point, you have access to an OSDF data origin, specifically a directory that can be used to stage input and output data for jobs, accessible via the OSDF. This guide describes general tips for using the OSDF, where to stage your files, and how to access files from jobs.","title":"Transfer Larger Job Files and Containers Using OSDF"},{"location":"htc_workloads/managing_data/osdf/#important-considerations-and-best-practices","text":"Use OSDF locations for larger files and containers : We recommend using the OSDF for files larger than 1GB (input or output) and all container files. OSDF files are cached across the Open Science Pool, any changes or modifications that you make might not be propagated. This means that if you add a new version of a file the OSDF directory, it must first be given a unique name (or directory path) to distinguish it from previous versions of that file. Adding a date or version number to directories or file names is strongly encouraged to manage your files uniqness. This is especially important when using the OSDF for software and containers. Never submit jobs from the OSDF locations; always submit jobs from within the /home directory. All log , error , output files and any other files smaller than the above values should ONLY ever exist within the user's /home directory. Files placed within a public OSDF directory are publicly accessible , discoverable and readable by anyone, via the web. At the moment, most default OSDF locations are not public.","title":"Important Considerations and Best Practices"},{"location":"htc_workloads/managing_data/osdf/#where-to-put-your-files","text":"Data origins and local mount points varies between the different access points. See the list below for the \"Local Path\" to use, based on your access point. Access Point OSDF Origin ap40.uw.osg-htc.org Accessible to user only: Local Path: /ospool/ap40/data/[USERNAME] Base OSDF URL: osdf:///ospool/ap40/data/[USERNAME] ap20.uc.osg-htc.org Accessible to user only: Local Path: /ospool/ap20/data/[USERNAME] Base OSDF URL: osdf:///ospool/ap20/data/[USERNAME] Accessible to project group only: Local Path: /ospool/uc-shared/projects/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/projects/[PROJECT] Public space for projects: Local Path: /ospool/uc-shared/public/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/public/[PROJECT] ap21.uc.osg-htc.org Accessible to user only: Local Path: /ospool/ap21/data/[USERNAME] Base OSDF URL: osdf:///ospool/ap21/data/[USERNAME] Accessible to project group only: Local Path: /ospool/uc-shared/project/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/project/[PROJECT] Public space for projects: Local Path: /ospool/uc-shared/public/[PROJECT] Base OSDF URL: osdf:///ospool/uc-shared/public/[PROJECT]","title":"Where to Put Your Files"},{"location":"htc_workloads/managing_data/osdf/#transfer-files-tofrom-jobs-using-the-osdf","text":"","title":"Transfer Files To/From Jobs Using the OSDF"},{"location":"htc_workloads/managing_data/osdf/#use-an-osdf-url-to-transfer-large-input-files-and-containers","text":"Jobs will transfer data from the OSDF directory when files are indicated with an appropriate osdf:// URL (or the older stash:// ) in the transfer_input_files line of the submit file. Make sure to customize the base URL based on your Access Point, as described in the table above . Some examples: Transferring one file from /ospool/apXX/data/ transfer_input_files = osdf:///ospool/apXX/data/<username>/InFile.txt When using multiple files from /ospool/apXX/data/ , it can be useful to use HTCondor submit file variables to make your list of files more readable: # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///ospool/apXX/data/<username> transfer_input_files = $(OSDF_LOCATION)/InputFile.txt, $(OSDF_LOCATION)/database.sql Transferring a folder from /ospool/apXX/data/ transfer_input_files = osdf:///ospool/apXX/data/<username>/<folder>?recursive Please note that for transferring a folder using OSDF ?recursive needs to added after the folder name.","title":"Use an 'osdf://' URL to Transfer Large Input Files and Containers"},{"location":"htc_workloads/managing_data/osdf/#use-transfer_output_remaps-and-osdf-url-for-large-output-files","text":"To move output files into an OSDF directory, users should use the transfer_output_remaps option within their job's submit file, which will transfer the user's specified file to the specific location in the data origin. By using transfer_output_remaps , it is possible to specify what path to save a file to and what name to save it under. Using this approach, it is possible to save files back to specific locations in your OSDF directory (as well as your /home directory, if desired). The general syntax for transfer_output_remaps is: transfer_output_remaps = \"Output1.txt = path/to/save/file/under/output.txt; Output2.txt = path/to/save/file/under/RenamedOutput.txt\" When saving large output files back to /ospool/apXX/data/ , the path provided will look like: transfer_output_remaps = \"Output.txt = osdf:///ospool/apXX/data/<username>/Output.txt\" Some examples: Transferring one output file ( OutFile.txt ) back to /ospool/apXX/data/ : transfer_output_remaps = \"OutFile.txt=osdf:///ospool/apXX/data/<username>/OutFile.txt\" When using multiple files from /ospool/apXX/data/ , it can be useful to use HTCondor submit file variables to make your list of files more readable. Also note the semi-colon separator in the list of output files. # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///ospool/apXX/data/<username> transfer_output_remaps = \"file1.txt = $(OSDF_LOCATION)/file1.txt; file2.txt = $(OSDF_LOCATION)/file2.txt; file3.txt = $(OSDF_LOCATION)/file3.txt\"","title":"Use transfer_output_remaps and 'osdf://' URL for Large Output Files"},{"location":"htc_workloads/managing_data/osdf/#phase-out-of-stash-and-stashcp-command","text":"Historically, output files could be transferred from a job to an' OSDF location using the stashcp command within the job's executable. However, this mechanism is no longer encouraged for OSPool users. Instead, jobs should use transfer_output_remaps (an HTCondor feature) to transfer output files to your assigned OSDF origin. By using transfer_output_remaps , HTCondor will manage the output data transfer for your jobs. Data transferred via HTCondor is more likely to be transferred successfully and errors with transfer are more likely to be reported to the user. osdf:// is the new format for these kind of transfers, and is equivalent of the old stash:// format (which will keep on being supported for the short term).","title":"Phase out of stash:/// and stashcp command"},{"location":"htc_workloads/managing_data/overview/","text":"Overview: Data Staging and Transfer to Jobs \u00b6 Overview \u00b6 As a distributed system, jobs on the OSPool will run in different physical locations, where the computers that are executing jobs don't have direct access to the files placed on the Access Point (e.g. in a /home directory). In order to run on this kind of distributed system, jobs need to \"bring along\" the data, code, packages, and other files from the access point (where the job is submitted) to the execute points (where the job will run). HTCondor's file transfer tools and plugins make this possible; input and output files are specified as part of the job submission and then moved to and from the execution location. This guide describes where to place files on the access points, and how to use these files within jobs, with links to a more detailed guide for each use case. Jobs should always be submitted from the /home directory Regardless of where your data is placed, you should only submit jobs ( condor_submit ) from the /home directory. Use /home for Smaller Files \u00b6 You should use your /home directory to stage input and output files where: individual input files per job are less than 1GB per file, and if there are multiple files, they total less than 1GB output files per job are less than 1GB per file Files can to be transferred to and from the /home directory using HTCondor's file transfer mechanism, which can easily handle smaller files (<1GB). By default, files created by your job will automatically be returned to your /home directory. See our Transfer Files To and From /home guide for complete details on managing your files this way. Use the OSDF for Larger Files and Containers \u00b6 You should use the OSDF ( Open Science Data Federation ) to stage job files where: individual input files per job are greater than 1GB per file an input file (of any size) is used by many jobs output files per job are greater than 1GB per file a Singularity/Apptainer container image ( .sif ) is used Important Note Files in OSDF are cached , so it is important to use a descriptive file name (i.e. version, dates) or a directory structure with unique names to ensure you know what version of the file you are using within your job. See our guide on where to place your files in the OSDF and how to use OSDF URLs in transfer_input_files / transfer_output_files . OSDF guide Quotas \u00b6 The /home directory and OSDF origins all have quota limits. The /home directory is limited to 50 GBs, while OSDF limits vary. You can view your current usage on the access point with the command quota or quota -vs . Jobs will go on hold if quotas are exceeded. To request an increase in your quota, please send a request with justification to the ticket system at support@osg-htc.org . External Data Transfer to/from Access Point \u00b6 Most common Unix tools or file transfer programs such as rsync , scp , Putty, WinSCP, gFTP , etc. can be used to upload data from your computer to access point, or to download files from the access point. See our Data Transfer Guide for more details. FAQ \u00b6 For additional data information, see also the \"Data Storage and Transfer\" section of our FAQ . Data Policies \u00b6 Please see the OSPool Polices for important usage polices.","title":"Overview: Data Staging and Transfer to Jobs"},{"location":"htc_workloads/managing_data/overview/#overview-data-staging-and-transfer-to-jobs","text":"","title":"Overview: Data Staging and Transfer to Jobs"},{"location":"htc_workloads/managing_data/overview/#overview","text":"As a distributed system, jobs on the OSPool will run in different physical locations, where the computers that are executing jobs don't have direct access to the files placed on the Access Point (e.g. in a /home directory). In order to run on this kind of distributed system, jobs need to \"bring along\" the data, code, packages, and other files from the access point (where the job is submitted) to the execute points (where the job will run). HTCondor's file transfer tools and plugins make this possible; input and output files are specified as part of the job submission and then moved to and from the execution location. This guide describes where to place files on the access points, and how to use these files within jobs, with links to a more detailed guide for each use case. Jobs should always be submitted from the /home directory Regardless of where your data is placed, you should only submit jobs ( condor_submit ) from the /home directory.","title":"Overview"},{"location":"htc_workloads/managing_data/overview/#use-home-for-smaller-files","text":"You should use your /home directory to stage input and output files where: individual input files per job are less than 1GB per file, and if there are multiple files, they total less than 1GB output files per job are less than 1GB per file Files can to be transferred to and from the /home directory using HTCondor's file transfer mechanism, which can easily handle smaller files (<1GB). By default, files created by your job will automatically be returned to your /home directory. See our Transfer Files To and From /home guide for complete details on managing your files this way.","title":"Use /home for Smaller Files"},{"location":"htc_workloads/managing_data/overview/#use-the-osdf-for-larger-files-and-containers","text":"You should use the OSDF ( Open Science Data Federation ) to stage job files where: individual input files per job are greater than 1GB per file an input file (of any size) is used by many jobs output files per job are greater than 1GB per file a Singularity/Apptainer container image ( .sif ) is used Important Note Files in OSDF are cached , so it is important to use a descriptive file name (i.e. version, dates) or a directory structure with unique names to ensure you know what version of the file you are using within your job. See our guide on where to place your files in the OSDF and how to use OSDF URLs in transfer_input_files / transfer_output_files . OSDF guide","title":"Use the OSDF for Larger Files and Containers"},{"location":"htc_workloads/managing_data/overview/#quotas","text":"The /home directory and OSDF origins all have quota limits. The /home directory is limited to 50 GBs, while OSDF limits vary. You can view your current usage on the access point with the command quota or quota -vs . Jobs will go on hold if quotas are exceeded. To request an increase in your quota, please send a request with justification to the ticket system at support@osg-htc.org .","title":"Quotas"},{"location":"htc_workloads/managing_data/overview/#external-data-transfer-tofrom-access-point","text":"Most common Unix tools or file transfer programs such as rsync , scp , Putty, WinSCP, gFTP , etc. can be used to upload data from your computer to access point, or to download files from the access point. See our Data Transfer Guide for more details.","title":"External Data Transfer to/from Access Point"},{"location":"htc_workloads/managing_data/overview/#faq","text":"For additional data information, see also the \"Data Storage and Transfer\" section of our FAQ .","title":"FAQ"},{"location":"htc_workloads/managing_data/overview/#data-policies","text":"Please see the OSPool Polices for important usage polices.","title":"Data Policies"},{"location":"htc_workloads/managing_data/scp/","text":"Use scp To Transfer Files To and From Access Point \u00b6 Overview \u00b6 This tutorial assumes that you will be using a command line application for performing file transfers instead of a GUI-based application such as WinSCP. We can transfer files to and from the access point using the scp command. Note scp is a counterpart to the secure shell command, ssh , that allows for secure, encrypted file transfers between systems using your ssh credentials. When using scp , you will always need to specify both the source of the content that you wish to copy and the destination of where you would like the copy to end up. For example: $ scp <source> <destination> Files on remote systems (like an OSG Access Point) are indicated using username@machine:/path/to/file . Transfer Files To Access Point \u00b6 Let's say you have a file you wish to transfer named my_file.txt . Using the terminal application on your computer, navigate to the location of my_file.txt . Then use the following scp command to tranfer my_file.txt to your /home on the access point. Note that you will not be logged into the access point when you perform this step. $ scp my_file.txt username@apXX.xx.osg-htc.org:/home/username/ Where NN is the specific number of your assigned login node (i.e. 04 or 05 ). Large files (>100MB in size) can be uploaded to your /public directory also using scp : $ scp my_large_file.gz username@apXX.xx.osg-htc.org:/public/username/ Transfer Directories To Access Point \u00b6 To copy directories using scp , add the (recursive) -r option to your scp command. For example: $ scp -r my_Dir username@apXX.xx.osg-htc.org:/home/username/ Transfer Files to Another Directory on the Access Point \u00b6 If you are using the OSDF to stage some of your files, you can upload files directly to that path by replacing /home/username in the commands above. If I wanted to upload files to the OSDF location on ap20 , which is /ospool/ap20/data/username , I would use the following command: $ scp my_file.txt username@ap20.uc.osg-htc.org:/ospool/ap20/data/username Transfer Files From Access Point \u00b6 To transfer files from the access point back to your laptop or desktop you can use the scp command as shown above, but with the source being the copy that is located on the access point: $ scp username@apXX.xx.osg-htc.org:/home/username/my_file.txt ./ where ./ sets the destination of the copy to your current location on your computer. Again, you will not be logged into the access point when you perform this step. You can download files from a different directory in the same way as described above when uploading files. Transfer Files Directly Between Access Point and Another Server \u00b6 scp can be used to transfer files between the OSG access point and another server that you have ssh access to. This means that files don't have to first be transferred to your personal computer which can save a lot of time and effort! For example, to transfer a file from another server to your access point login node /home directory: $ scp username@serverhostname:/path/to/my_file.txt username@lapXX.xx.osg-htc.org:/home/username Be sure to use the username assigned to you on the other server and to provide the full path on the other server to your file. To transfer files from the OSG Access Point to the other server, just reverse the order of the two server statements. Other Graphical User Interface (GUI) Tools for transferring files and folders \u00b6 Apart from scp, other GUI software such as- WinSCP , FileZilla , Cyberduck can be used for transferring files and folders from and to the Access Point. Please remember to add your private key for the authentication method.","title":"Use scp To Transfer Files To and From OSG Managed Access Points"},{"location":"htc_workloads/managing_data/scp/#use-scp-to-transfer-files-to-and-from-access-point","text":"","title":"Use scp To Transfer Files To and From Access Point"},{"location":"htc_workloads/managing_data/scp/#overview","text":"This tutorial assumes that you will be using a command line application for performing file transfers instead of a GUI-based application such as WinSCP. We can transfer files to and from the access point using the scp command. Note scp is a counterpart to the secure shell command, ssh , that allows for secure, encrypted file transfers between systems using your ssh credentials. When using scp , you will always need to specify both the source of the content that you wish to copy and the destination of where you would like the copy to end up. For example: $ scp <source> <destination> Files on remote systems (like an OSG Access Point) are indicated using username@machine:/path/to/file .","title":"Overview"},{"location":"htc_workloads/managing_data/scp/#transfer-files-to-access-point","text":"Let's say you have a file you wish to transfer named my_file.txt . Using the terminal application on your computer, navigate to the location of my_file.txt . Then use the following scp command to tranfer my_file.txt to your /home on the access point. Note that you will not be logged into the access point when you perform this step. $ scp my_file.txt username@apXX.xx.osg-htc.org:/home/username/ Where NN is the specific number of your assigned login node (i.e. 04 or 05 ). Large files (>100MB in size) can be uploaded to your /public directory also using scp : $ scp my_large_file.gz username@apXX.xx.osg-htc.org:/public/username/","title":"Transfer Files To Access Point"},{"location":"htc_workloads/managing_data/scp/#transfer-directories-to-access-point","text":"To copy directories using scp , add the (recursive) -r option to your scp command. For example: $ scp -r my_Dir username@apXX.xx.osg-htc.org:/home/username/","title":"Transfer Directories To Access Point"},{"location":"htc_workloads/managing_data/scp/#transfer-files-to-another-directory-on-the-access-point","text":"If you are using the OSDF to stage some of your files, you can upload files directly to that path by replacing /home/username in the commands above. If I wanted to upload files to the OSDF location on ap20 , which is /ospool/ap20/data/username , I would use the following command: $ scp my_file.txt username@ap20.uc.osg-htc.org:/ospool/ap20/data/username","title":"Transfer Files to Another Directory on the Access Point"},{"location":"htc_workloads/managing_data/scp/#transfer-files-from-access-point","text":"To transfer files from the access point back to your laptop or desktop you can use the scp command as shown above, but with the source being the copy that is located on the access point: $ scp username@apXX.xx.osg-htc.org:/home/username/my_file.txt ./ where ./ sets the destination of the copy to your current location on your computer. Again, you will not be logged into the access point when you perform this step. You can download files from a different directory in the same way as described above when uploading files.","title":"Transfer Files From Access Point"},{"location":"htc_workloads/managing_data/scp/#transfer-files-directly-between-access-point-and-another-server","text":"scp can be used to transfer files between the OSG access point and another server that you have ssh access to. This means that files don't have to first be transferred to your personal computer which can save a lot of time and effort! For example, to transfer a file from another server to your access point login node /home directory: $ scp username@serverhostname:/path/to/my_file.txt username@lapXX.xx.osg-htc.org:/home/username Be sure to use the username assigned to you on the other server and to provide the full path on the other server to your file. To transfer files from the OSG Access Point to the other server, just reverse the order of the two server statements.","title":"Transfer Files Directly Between Access Point and Another Server"},{"location":"htc_workloads/managing_data/scp/#other-graphical-user-interface-gui-tools-for-transferring-files-and-folders","text":"Apart from scp, other GUI software such as- WinSCP , FileZilla , Cyberduck can be used for transferring files and folders from and to the Access Point. Please remember to add your private key for the authentication method.","title":"Other Graphical User Interface (GUI) Tools for transferring files and folders"},{"location":"htc_workloads/specific_resource/arm64/","text":"ARM64 \u00b6 ARM64 (AArch64) and x86_64 are both 64-bit architectures, but they differ in design and application. ARM64 is renowned for its energy efficiency, making it ideal for mobile devices and other low-power environments. In contrast, x86_64, predominantly used in Intel and AMD processors, emphasizes raw performance and compatibility with legacy software, establishing it as the standard for desktops, laptops, and servers. However, ARM64's energy efficiency has increasingly driven its adoption in high-throughput and high-performance computing environments. A small number of sites within the OSPool now offer ARM64 resources, though these resources currently see limited demand. The availability of these underutilized cycles provides a strong incentive for users to incorporate ARM64 resources when running their jobs. Listing Available Resources \u00b6 To see the ARM64 resources in the OSPool, use condor_status with a constraint for the archtechture (note that on Linux and HTCondor, the offical label for ARM64 is aarch64 ): condor_status -constraint 'Arch == \"aarch64\"' Requesting ARM64 \u00b6 By default, HTCondor will automatically send your job to the same architechture as the access point you are submitting from, which currently is the x86_64 architechture. If you also want to target ARM64, add the following to your requirements . requirements = (Arch == \"X86_64\" || Arch == \"aarch64\") Software Considerations \u00b6 Since ARM64 is a different architecture, x86_64 binaries and containers are incompatible. Additionally, OSPool's container synchronization is not yet ARM64-compatible. Therefore, the options for software on ARM64 resources are limited to the following: Simple Python codes. If you have a simple Python script which runs on the OSPool default images, it will probably work fine on ARM64 as well. All you need to this in this case, is update your requirements as described in the previous section. Pre-built binaries. If you have built binaries for multiple architechtures, you can use HTCondor's machine add substitution mechanism to switch between the binaries depending on what machine the job lands on. Please the the HTCondor documentation for more details. Multiarch containers. If you are able to build multiarch containers (for example, with docker buildx build --platform linux/amd64,linux/arm64 ), you can specify which container to use similar to the pre-built binaries case. However, the image synchronization is still a manual process, so please contact support@osg-htc.org for help with this setup.","title":"ARM64"},{"location":"htc_workloads/specific_resource/arm64/#arm64","text":"ARM64 (AArch64) and x86_64 are both 64-bit architectures, but they differ in design and application. ARM64 is renowned for its energy efficiency, making it ideal for mobile devices and other low-power environments. In contrast, x86_64, predominantly used in Intel and AMD processors, emphasizes raw performance and compatibility with legacy software, establishing it as the standard for desktops, laptops, and servers. However, ARM64's energy efficiency has increasingly driven its adoption in high-throughput and high-performance computing environments. A small number of sites within the OSPool now offer ARM64 resources, though these resources currently see limited demand. The availability of these underutilized cycles provides a strong incentive for users to incorporate ARM64 resources when running their jobs.","title":"ARM64"},{"location":"htc_workloads/specific_resource/arm64/#listing-available-resources","text":"To see the ARM64 resources in the OSPool, use condor_status with a constraint for the archtechture (note that on Linux and HTCondor, the offical label for ARM64 is aarch64 ): condor_status -constraint 'Arch == \"aarch64\"'","title":"Listing Available Resources"},{"location":"htc_workloads/specific_resource/arm64/#requesting-arm64","text":"By default, HTCondor will automatically send your job to the same architechture as the access point you are submitting from, which currently is the x86_64 architechture. If you also want to target ARM64, add the following to your requirements . requirements = (Arch == \"X86_64\" || Arch == \"aarch64\")","title":"Requesting ARM64"},{"location":"htc_workloads/specific_resource/arm64/#software-considerations","text":"Since ARM64 is a different architecture, x86_64 binaries and containers are incompatible. Additionally, OSPool's container synchronization is not yet ARM64-compatible. Therefore, the options for software on ARM64 resources are limited to the following: Simple Python codes. If you have a simple Python script which runs on the OSPool default images, it will probably work fine on ARM64 as well. All you need to this in this case, is update your requirements as described in the previous section. Pre-built binaries. If you have built binaries for multiple architechtures, you can use HTCondor's machine add substitution mechanism to switch between the binaries depending on what machine the job lands on. Please the the HTCondor documentation for more details. Multiarch containers. If you are able to build multiarch containers (for example, with docker buildx build --platform linux/amd64,linux/arm64 ), you can specify which container to use similar to the pre-built binaries case. However, the image synchronization is still a manual process, so please contact support@osg-htc.org for help with this setup.","title":"Software Considerations"},{"location":"htc_workloads/specific_resource/el9-transition/","text":"Operating System Transition to EL9 \u00b6 During May 2024, the OSPool will transition to be mostly EL9 based. The access points will be upgraded, and the execution points will mostly shift to EL9. Note that EL9 in this context refers to Enterprise Linux 9, and is an umbrella term for CentOS Stream 9 and derived distributions such as AlmaLinux 9 and RockyLinux 9. What You Need to Do \u00b6 The access point transitions will be mostly transparent. You will get an email about when the switchover will happen, and the access point will be offline for about 8 hours. Data and jobs will be retained, so no action is required. If your jobs use containers (Apptainer/Singularity, Docker) \u00b6 No action is needed for researchers already using a Apptainer/Singularity or Docker software containers in their jobs. Becuase software containers have a small operating system installed inside of them, these jobs carry everything they need with them and do not rely signifcantly on the host operating system. By default, your jobs will match to any operating system in the HTC pool, including the new EL9 hosts. All other jobs (not using containers) \u00b6 Researchers not already using a Docker or Apptainer software container will need to either: Test their software/code on an EL9 machine to see their software needs to be rebuilt, and then update the job requirements line to refer to RHEL 9 . See Requirements or Switch to using a software container (recommended). See the below for additional information. If you would like to access as much computing capacity as possible, consider using an Apptainer or Docker software container for your jobs so that your jobs can match to a variety of operating systems. Options For Transitioning Your Jobs \u00b6 Option 1: Use a Software Container (Recommended) \u00b6 Using a software container to provide a base version of Linux will allow you to run on any nodes in the OSPool regardless of the operating system it is running, and not limit you to a subset of nodes. Apptainer/Singularity Docker Option 2: Transition to a New Operating System \u00b6 At any time, you can require a specific operating system version (or versions) for your jobs. Instructions for requesting a specific operating system(s) are outlined here: Requirements This option is more limiting because you are restricted to operating systems used by OSPool, and the number of nodes running that operating system. Alternativly, you can make your job run in a provided base OS container. For example, if you want your job to always run in RHEL 8, remove the requirements and add +SingularityImage in your submit file. Example: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" requirements = True","title":"EL9 Transition"},{"location":"htc_workloads/specific_resource/el9-transition/#operating-system-transition-to-el9","text":"During May 2024, the OSPool will transition to be mostly EL9 based. The access points will be upgraded, and the execution points will mostly shift to EL9. Note that EL9 in this context refers to Enterprise Linux 9, and is an umbrella term for CentOS Stream 9 and derived distributions such as AlmaLinux 9 and RockyLinux 9.","title":"Operating System Transition to EL9"},{"location":"htc_workloads/specific_resource/el9-transition/#what-you-need-to-do","text":"The access point transitions will be mostly transparent. You will get an email about when the switchover will happen, and the access point will be offline for about 8 hours. Data and jobs will be retained, so no action is required.","title":"What You Need to Do"},{"location":"htc_workloads/specific_resource/el9-transition/#if-your-jobs-use-containers-apptainersingularity-docker","text":"No action is needed for researchers already using a Apptainer/Singularity or Docker software containers in their jobs. Becuase software containers have a small operating system installed inside of them, these jobs carry everything they need with them and do not rely signifcantly on the host operating system. By default, your jobs will match to any operating system in the HTC pool, including the new EL9 hosts.","title":"If your jobs use containers (Apptainer/Singularity, Docker)"},{"location":"htc_workloads/specific_resource/el9-transition/#all-other-jobs-not-using-containers","text":"Researchers not already using a Docker or Apptainer software container will need to either: Test their software/code on an EL9 machine to see their software needs to be rebuilt, and then update the job requirements line to refer to RHEL 9 . See Requirements or Switch to using a software container (recommended). See the below for additional information. If you would like to access as much computing capacity as possible, consider using an Apptainer or Docker software container for your jobs so that your jobs can match to a variety of operating systems.","title":"All other jobs (not using containers)"},{"location":"htc_workloads/specific_resource/el9-transition/#options-for-transitioning-your-jobs","text":"","title":"Options For Transitioning Your Jobs"},{"location":"htc_workloads/specific_resource/el9-transition/#option-1-use-a-software-container-recommended","text":"Using a software container to provide a base version of Linux will allow you to run on any nodes in the OSPool regardless of the operating system it is running, and not limit you to a subset of nodes. Apptainer/Singularity Docker","title":"Option 1: Use a Software Container (Recommended)"},{"location":"htc_workloads/specific_resource/el9-transition/#option-2-transition-to-a-new-operating-system","text":"At any time, you can require a specific operating system version (or versions) for your jobs. Instructions for requesting a specific operating system(s) are outlined here: Requirements This option is more limiting because you are restricted to operating systems used by OSPool, and the number of nodes running that operating system. Alternativly, you can make your job run in a provided base OS container. For example, if you want your job to always run in RHEL 8, remove the requirements and add +SingularityImage in your submit file. Example: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" requirements = True","title":"Option 2: Transition to a New Operating System"},{"location":"htc_workloads/specific_resource/gpu-jobs/","text":"GPU Jobs \u00b6 GPUs (Graphical Processing Units) are a special kind of computer processor that are optimized for running very large numbers of simple calculations in parallel, which often can be applied to problems related to image processing or machine learning. Well-crafted GPU programs for suitable applications can outperform implementations running on CPUs by a factor of ten or more, but only when the program is written and designed explicitly to run on GPUs using special libraries like CUDA. Requesting GPUs \u00b6 To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB Users can request one or multiple GPU cores on a single GPU machine. Specific GPU Requests \u00b6 If your software or code requires a certain type of GPU, or has some other special requirement, there are options to specify this. gpus_minimum_capability = <version> gpus_maximum_capability = <version> gpus_minimum_memory = <quantity in MB> The first two options relate to GPU capability; the last to GPU memory 1) Capability If you want a certain type or family of GPUs, we usually recommend using the GPU's \"Capability\". This value is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability,\" which is related to hardware generation. See the table below for examples of GPU capability values. For example, an NVIDIA A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: gpus_minimum_capability = 8.0 If you wanted only to use that type of GPU (not anything newer), you could also set the maximum capability: gpus_maximum_capability = 8.0 2) GPU Memory GPU memory (also sometimes called \"vram\") is the amount of memory available on the GPU device. In HTCondor, GPU Memory is in units of megabytes. If you didn't care about the age of the GPU, but just the amount of available memory (at least, say 20GB or 20,000MB), you would use something like: gpus_minimum_memory = 20000 There are additional attributes that can be used to select GPU types; email the facilitation team if the above options do not satisfy your use case. Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity. Sample Submit File \u00b6 universe = container container_image = /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:1.3 log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out executable = run_gpu_job.py #arguments = +JobDurationCategory = \"Medium\" # specify both general requirements and gpu requirements if there are any # in this example, we are running on GPUs with at least 12GB of GPU memory # requirements = # gpus_minimum_capability = <version> # gpus_maximum_capability = <version> gpus_minimum_memory = 12000 request_gpus = 1 request_cpus = 1 request_memory = 4GB request_disk = 4GB queue 1 Available GPUs \u00b6 Capacity \u00b6 There are multiple OSPool contributors providing GPUs on a regular basis to the OSPool. Some of these contributors will make their GPUs available only when there is demand in the job queue, so after initial small-scale job testing, we strongly recommend submitting a signficant batch of test jobs to explore how much throughput you can get in the system as a whole. As a reminder, because the OSPool is dynamic, the more jobs submitted requesting GPUs, the more GPU machines will be pulled into the OSPool as execution points. GPU Types \u00b6 Because the composition of the OSPool can change from day to day, we do not know exactly what specific GPUs are available at any given time. Based on previous GPU job executions, you might land on one of the following types of GPUs: GPU Type Capability Memory V100 7.0 16 GB GeForce GTX 2080 Ti 7.5 10GB A100 8.0 40GB or 80GB A10 8.6 22GB GeForce RTX 3090 8.6 24GB GeForce RTX 4090 8.9 24GB Software and Data Considerations \u00b6 Software for GPUs \u00b6 For GPU-enabled machine learning libraries, we recommend using software containers to set up your software for jobs: Containers - Apptainer/Singularity Sample TensorFlow GPU Container Image Definition TensorFlow Example Job See our Data Staging and Transfer guide for details and contact the Research Computing Facilitation team with questions.","title":"GPU Jobs"},{"location":"htc_workloads/specific_resource/gpu-jobs/#gpu-jobs","text":"GPUs (Graphical Processing Units) are a special kind of computer processor that are optimized for running very large numbers of simple calculations in parallel, which often can be applied to problems related to image processing or machine learning. Well-crafted GPU programs for suitable applications can outperform implementations running on CPUs by a factor of ten or more, but only when the program is written and designed explicitly to run on GPUs using special libraries like CUDA.","title":"GPU Jobs"},{"location":"htc_workloads/specific_resource/gpu-jobs/#requesting-gpus","text":"To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB Users can request one or multiple GPU cores on a single GPU machine.","title":"Requesting GPUs"},{"location":"htc_workloads/specific_resource/gpu-jobs/#specific-gpu-requests","text":"If your software or code requires a certain type of GPU, or has some other special requirement, there are options to specify this. gpus_minimum_capability = <version> gpus_maximum_capability = <version> gpus_minimum_memory = <quantity in MB> The first two options relate to GPU capability; the last to GPU memory 1) Capability If you want a certain type or family of GPUs, we usually recommend using the GPU's \"Capability\". This value is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability,\" which is related to hardware generation. See the table below for examples of GPU capability values. For example, an NVIDIA A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: gpus_minimum_capability = 8.0 If you wanted only to use that type of GPU (not anything newer), you could also set the maximum capability: gpus_maximum_capability = 8.0 2) GPU Memory GPU memory (also sometimes called \"vram\") is the amount of memory available on the GPU device. In HTCondor, GPU Memory is in units of megabytes. If you didn't care about the age of the GPU, but just the amount of available memory (at least, say 20GB or 20,000MB), you would use something like: gpus_minimum_memory = 20000 There are additional attributes that can be used to select GPU types; email the facilitation team if the above options do not satisfy your use case. Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity.","title":"Specific GPU Requests"},{"location":"htc_workloads/specific_resource/gpu-jobs/#sample-submit-file","text":"universe = container container_image = /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:1.3 log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out executable = run_gpu_job.py #arguments = +JobDurationCategory = \"Medium\" # specify both general requirements and gpu requirements if there are any # in this example, we are running on GPUs with at least 12GB of GPU memory # requirements = # gpus_minimum_capability = <version> # gpus_maximum_capability = <version> gpus_minimum_memory = 12000 request_gpus = 1 request_cpus = 1 request_memory = 4GB request_disk = 4GB queue 1","title":"Sample Submit File"},{"location":"htc_workloads/specific_resource/gpu-jobs/#available-gpus","text":"","title":"Available GPUs"},{"location":"htc_workloads/specific_resource/gpu-jobs/#capacity","text":"There are multiple OSPool contributors providing GPUs on a regular basis to the OSPool. Some of these contributors will make their GPUs available only when there is demand in the job queue, so after initial small-scale job testing, we strongly recommend submitting a signficant batch of test jobs to explore how much throughput you can get in the system as a whole. As a reminder, because the OSPool is dynamic, the more jobs submitted requesting GPUs, the more GPU machines will be pulled into the OSPool as execution points.","title":"Capacity"},{"location":"htc_workloads/specific_resource/gpu-jobs/#gpu-types","text":"Because the composition of the OSPool can change from day to day, we do not know exactly what specific GPUs are available at any given time. Based on previous GPU job executions, you might land on one of the following types of GPUs: GPU Type Capability Memory V100 7.0 16 GB GeForce GTX 2080 Ti 7.5 10GB A100 8.0 40GB or 80GB A10 8.6 22GB GeForce RTX 3090 8.6 24GB GeForce RTX 4090 8.9 24GB","title":"GPU Types"},{"location":"htc_workloads/specific_resource/gpu-jobs/#software-and-data-considerations","text":"","title":"Software and Data Considerations"},{"location":"htc_workloads/specific_resource/gpu-jobs/#software-for-gpus","text":"For GPU-enabled machine learning libraries, we recommend using software containers to set up your software for jobs: Containers - Apptainer/Singularity Sample TensorFlow GPU Container Image Definition TensorFlow Example Job See our Data Staging and Transfer guide for details and contact the Research Computing Facilitation team with questions.","title":"Software for GPUs"},{"location":"htc_workloads/specific_resource/large-memory-jobs/","text":"Large Memory Jobs \u00b6 By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement. Please note that the OSG has limited resources available for large memory jobs. Requesting jobs with higher memory needs will results in longer than average queue times for these jobs.","title":"Large Memory Jobs"},{"location":"htc_workloads/specific_resource/large-memory-jobs/#large-memory-jobs","text":"By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement. Please note that the OSG has limited resources available for large memory jobs. Requesting jobs with higher memory needs will results in longer than average queue times for these jobs.","title":"Large Memory Jobs"},{"location":"htc_workloads/specific_resource/multicore-jobs/","text":"Multicore Jobs \u00b6 Please note, the OSG has limited support for multicore jobs. Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job. MPI Jobs For jobs that require MPI, see our OpenMPI Jobs guide.","title":"Multicore Jobs"},{"location":"htc_workloads/specific_resource/multicore-jobs/#multicore-jobs","text":"Please note, the OSG has limited support for multicore jobs. Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job. MPI Jobs For jobs that require MPI, see our OpenMPI Jobs guide.","title":"Multicore Jobs"},{"location":"htc_workloads/specific_resource/openmpi-jobs/","text":"OpenMPI Jobs \u00b6 Even though the Open Science Pool is a high throughput computing system, sometimes there is a need to run small OpenMPI based jobs. OSG has limited support for this, as long as the core count is small (4 is known to work well, 8 and 16 becomes more difficult due to the limited number of resources). Find an MPI-based Container \u00b6 To get started, first compile your code using an OpenMPI container. You can create your own OpenMPI container or use the one that is available on DockerHub. OSG also has an openmpi container that can be used for compiling. Please note that the OSG provided container openmpi.sif image is available only on the ap20.uc.osg-htc.org and ap21.uc.osg-htc.org access points. For the ap40 access point, please use your desired docker image and do apptainer pull . More information about using apptainer pull can be found here . Compile the Code \u00b6 To compile your code using the OSG provided image, start running the container first. Then run mpicc to compile the code: $ apptainer shell /ospool/uc-shared/public/OSG-Staff/openmpi.sif Apptainer> mpicc -o hello hello.c The hello.c is an example hello world code that can be executed using multiple processors. The code is given below: #include <mpi.h> #include <stdio.h> int main(int argc, char** argv) { MPI_Init(NULL, NULL); int world_size; MPI_Comm_size(MPI_COMM_WORLD, &world_size); int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, &name_len); printf(\"Hello world from processor %s, rank %d out of %d processors\\n\", processor_name, world_rank, world_size); MPI_Finalize(); } After compiling the code, you can test the executable locally using mpiexec : Apptainer> mpiexec -n 4 hello Hello world from processor ap21.uc.osg-htc.org, rank 0 out of 4 processors Hello world from processor ap21.uc.osg-htc.org, rank 1 out of 4 processors Hello world from processor ap21.uc.osg-htc.org, rank 2 out of 4 processors Hello world from processor ap21.uc.osg-htc.org, rank 3 out of 4 processors When testing is done be sure to exit from the apptainer shell using exit . Run a Job Using the MPI Container and Compiled Code \u00b6 The next step is to run your code as a job on the Open Science Pool. For this, first create a wrapper.sh . Example: #!/bin/sh set -e mpiexec -n 4 hello Then, a job submit file: +SingularityImage = \"osdf:///ospool/uc-shared/public/OSG-Staff/openmpi.sif\" executable = wrapper.sh transfer_input_files = hello +JobDurationCategory = \"Medium\" request_cpus = 4 request_memory = 1 GB output = job.out.$(Cluster).$(Process) error = job.error.$(Cluster).$(Process) log = job.log.$(Cluster).$(Process) queue 1 Note how the executable is the wrapper.sh script, and that the real executable hello is transferred using the transfer_input_files mechanism. Please make sure that the number of cores specified in the submit file via request_cpus match the -n argument in the wrapper.sh file.","title":"OpenMPI Jobs"},{"location":"htc_workloads/specific_resource/openmpi-jobs/#openmpi-jobs","text":"Even though the Open Science Pool is a high throughput computing system, sometimes there is a need to run small OpenMPI based jobs. OSG has limited support for this, as long as the core count is small (4 is known to work well, 8 and 16 becomes more difficult due to the limited number of resources).","title":"OpenMPI Jobs"},{"location":"htc_workloads/specific_resource/openmpi-jobs/#find-an-mpi-based-container","text":"To get started, first compile your code using an OpenMPI container. You can create your own OpenMPI container or use the one that is available on DockerHub. OSG also has an openmpi container that can be used for compiling. Please note that the OSG provided container openmpi.sif image is available only on the ap20.uc.osg-htc.org and ap21.uc.osg-htc.org access points. For the ap40 access point, please use your desired docker image and do apptainer pull . More information about using apptainer pull can be found here .","title":"Find an MPI-based Container"},{"location":"htc_workloads/specific_resource/openmpi-jobs/#compile-the-code","text":"To compile your code using the OSG provided image, start running the container first. Then run mpicc to compile the code: $ apptainer shell /ospool/uc-shared/public/OSG-Staff/openmpi.sif Apptainer> mpicc -o hello hello.c The hello.c is an example hello world code that can be executed using multiple processors. The code is given below: #include <mpi.h> #include <stdio.h> int main(int argc, char** argv) { MPI_Init(NULL, NULL); int world_size; MPI_Comm_size(MPI_COMM_WORLD, &world_size); int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, &name_len); printf(\"Hello world from processor %s, rank %d out of %d processors\\n\", processor_name, world_rank, world_size); MPI_Finalize(); } After compiling the code, you can test the executable locally using mpiexec : Apptainer> mpiexec -n 4 hello Hello world from processor ap21.uc.osg-htc.org, rank 0 out of 4 processors Hello world from processor ap21.uc.osg-htc.org, rank 1 out of 4 processors Hello world from processor ap21.uc.osg-htc.org, rank 2 out of 4 processors Hello world from processor ap21.uc.osg-htc.org, rank 3 out of 4 processors When testing is done be sure to exit from the apptainer shell using exit .","title":"Compile the Code"},{"location":"htc_workloads/specific_resource/openmpi-jobs/#run-a-job-using-the-mpi-container-and-compiled-code","text":"The next step is to run your code as a job on the Open Science Pool. For this, first create a wrapper.sh . Example: #!/bin/sh set -e mpiexec -n 4 hello Then, a job submit file: +SingularityImage = \"osdf:///ospool/uc-shared/public/OSG-Staff/openmpi.sif\" executable = wrapper.sh transfer_input_files = hello +JobDurationCategory = \"Medium\" request_cpus = 4 request_memory = 1 GB output = job.out.$(Cluster).$(Process) error = job.error.$(Cluster).$(Process) log = job.log.$(Cluster).$(Process) queue 1 Note how the executable is the wrapper.sh script, and that the real executable hello is transferred using the transfer_input_files mechanism. Please make sure that the number of cores specified in the submit file via request_cpus match the -n argument in the wrapper.sh file.","title":"Run a Job Using the MPI Container and Compiled Code"},{"location":"htc_workloads/specific_resource/requirements/","text":"Control Where Your Jobs Run / Job Requirements \u00b6 By default, your jobs will match any available slot in the OSG. This is fine for very generic jobs. However, in some cases a job may have one or more system requirements in order to complete successfully. For instance, your job may need to run on a node with a specific operating system. HTCondor provides several options for \"steering\" your jobs to appropriate nodes and system environments. The request_cpus , request_gpus , request_memory , and request_disk submit file attributes should be used to specify the hardware needs of your jobs. Please see our guides Multicore Jobs and Large Memory Jobs for more details. HTCondor also provides a requirements attribute and feature-specific attributes that can be added to your submit files to target specific environments in which to run your jobs. Lastly, there are some custom attributes you can add to your submit file to either focus on, or avoid, certain execution sites. Requirements \u00b6 The requirements attribute is formatted as an expression, so you can use logical operators to combine multiple requirements where && is used for AND and || used for OR. For example, the following requirements statement will direct jobs only to 64 bit RHEL (Red Hat Enterprise Linux) 9 nodes. requirements = OSGVO_OS_STRING == \"RHEL 9\" && Arch == \"X86_64\" Alternatively, if you have code which can run on either RHEL 8 or 9, you can use OR: requirements = (OSGVO_OS_STRING == \"RHEL 8\" || OSGVO_OS_STRING == \"RHEL 9\") && Arch == \"X86_64\" Note that parentheses placement is important for controling how the logical operations are interpreted by HTCondor. If you are interested in seeing a list of currently available operating systems (these are just the default ones, you can create a custom container image if you want something else): $ condor_status -autoformat OSGVO_OS_STRING | sort | uniq -c Another common requirement is to land on a node which has CVMFS. Then the requirements would be: requirements = HAS_oasis_opensciencegrid_org == True x86_64 Micro Architecture Levels \u00b6 The x86_64 set of CPUs contains a large number of different CPUs with different capabilities. Instead of trying to match on on individual attributes like the AVX/AVX2 ones in the previous section, it can be useful to match against a family of CPUs. There are currently 4 levels to chose from: x86_64-v1, x86_64-v2, x86_64-v3, and x86_64-v4. A description of the levels is available on Wikipedia . HTCondor advertises an attribute named Microarch . An example on how make jobs running on the two highest levels is: requirements = (Microarch >= \"x86_64-v3\") Note that in the past, it was recommended to use the HAS_AVX and HAS_AVX2 attributes to target CPUs with those capabilities. This is no longer recommended, with the replacement being Microarch >= \"x86_64-v3\" . Additional Feature-Specific Attributes \u00b6 There are many attributes that you can use with requirements . To see what values you can specify for a given attribute you can run the following command while connected to your login node: $ condor_status -af {ATTR_NAME} | sort -u For example, to see what values you can specify for the Microarch attribute run: $ condor_status -af Microarch | sort -u x86_64-v1 x86_64-v2 x86_64-v3 x86_64-v4 You will find many attributes will take the boolean values true or false . Below is a list of common attributes that you can include in your submit file requirements statement. Microarch - See above. x86_64-v1, x86_64-v2, x86_64-v3, and x86_64-v4 OSGVO_OS_NAME - The name of the operating system of the compute node. The most common name is RHEL OSGVO_OS_VERSION - Version of the operating system OSGVO_OS_STRING - Combined OS name and version. Please see the requirements string above on the recommended setup. OSGVO_CPU_MODEL - The CPU model identifier string as presented in /proc/cpuinfo HAS_CVMFS_oasis_opensciencegrid_org - Attribute specifying the need to access specific oasis /cvmfs file system repositories. Other common CVMFS repositories are HAS_CVMFS_singularity_opensciencegrid_org and project ones like HAS_CVMFS_xenon_opensciencegrid_org . For GPU attribtues, such as GPUs' compute capability, see our GPU guide . Non-x86 Based Architectures \u00b6 Within the computing community, there's a growing interest in exploring non-x86 architectures, such as ARM and PowerPC. As of now, the OSPool does not host resources based on these architectures; however, it is designed to accommodate them once available. The OSPool operates under a system where all tasks are configured to execute on the same architecture as the host from which they were submitted. This compatibility is ensured by HTCondor, which automatically adds the appropriate architecture to the job's requirements. By inspecting the classad of any given job, one would notice the inclusion of (TARGET.Arch == \"X86_64\") among its requirements, indicating the system's current architectural preference. If you do wish to specify a different architechure, just add it to your job requirements: requirements = Arch == \"PPC\" You can get a list of current architechures by running: $ condor_status -af Arch | sort | uniq X86_64 Specifying Sites / Avoiding Sites \u00b6 To run your jobs on a list of specific execution sites, or avoid a set of sites, use the +DESIRED_Sites / +UNDESIRED_Sites attributes in your job submit file. These attributes should only be used as a last resort. For example, it is much better to use feature attributes (see above) to make your job go to nodes matching what you really require, than to broadly allow/block whole sites. We encourage you to contact the facilitation team before taking this action, to make sure it is right for you. To avoid certain sites, first find the site names. You can find a current list by querying the pool: condor_status -af GLIDEIN_Site | sort -u In your submit file, add a comma separated list of sites like: +UNDESIRED_Sites = \"ISI,SU-ITS\" Those sites will now be exluded from the set of sites your job can run at. Similarly, you can use +DESIRED_Sites to list a subset of sites you want to target. For example, to run your jobs at the SU-ITS site, and only at that site, use: +DESIRED_Sites = \"ISI,SU-ITS\" Note that you should only specify one of +DESIRED_Sites / +UNDESIRED_Sites in the submit file. Using both at the same time will prevent the job from running.","title":"Control Where Your Jobs Run/Job Requirements"},{"location":"htc_workloads/specific_resource/requirements/#control-where-your-jobs-run-job-requirements","text":"By default, your jobs will match any available slot in the OSG. This is fine for very generic jobs. However, in some cases a job may have one or more system requirements in order to complete successfully. For instance, your job may need to run on a node with a specific operating system. HTCondor provides several options for \"steering\" your jobs to appropriate nodes and system environments. The request_cpus , request_gpus , request_memory , and request_disk submit file attributes should be used to specify the hardware needs of your jobs. Please see our guides Multicore Jobs and Large Memory Jobs for more details. HTCondor also provides a requirements attribute and feature-specific attributes that can be added to your submit files to target specific environments in which to run your jobs. Lastly, there are some custom attributes you can add to your submit file to either focus on, or avoid, certain execution sites.","title":"Control Where Your Jobs Run / Job Requirements"},{"location":"htc_workloads/specific_resource/requirements/#requirements","text":"The requirements attribute is formatted as an expression, so you can use logical operators to combine multiple requirements where && is used for AND and || used for OR. For example, the following requirements statement will direct jobs only to 64 bit RHEL (Red Hat Enterprise Linux) 9 nodes. requirements = OSGVO_OS_STRING == \"RHEL 9\" && Arch == \"X86_64\" Alternatively, if you have code which can run on either RHEL 8 or 9, you can use OR: requirements = (OSGVO_OS_STRING == \"RHEL 8\" || OSGVO_OS_STRING == \"RHEL 9\") && Arch == \"X86_64\" Note that parentheses placement is important for controling how the logical operations are interpreted by HTCondor. If you are interested in seeing a list of currently available operating systems (these are just the default ones, you can create a custom container image if you want something else): $ condor_status -autoformat OSGVO_OS_STRING | sort | uniq -c Another common requirement is to land on a node which has CVMFS. Then the requirements would be: requirements = HAS_oasis_opensciencegrid_org == True","title":"Requirements"},{"location":"htc_workloads/specific_resource/requirements/#x86_64-micro-architecture-levels","text":"The x86_64 set of CPUs contains a large number of different CPUs with different capabilities. Instead of trying to match on on individual attributes like the AVX/AVX2 ones in the previous section, it can be useful to match against a family of CPUs. There are currently 4 levels to chose from: x86_64-v1, x86_64-v2, x86_64-v3, and x86_64-v4. A description of the levels is available on Wikipedia . HTCondor advertises an attribute named Microarch . An example on how make jobs running on the two highest levels is: requirements = (Microarch >= \"x86_64-v3\") Note that in the past, it was recommended to use the HAS_AVX and HAS_AVX2 attributes to target CPUs with those capabilities. This is no longer recommended, with the replacement being Microarch >= \"x86_64-v3\" .","title":"x86_64 Micro Architecture Levels"},{"location":"htc_workloads/specific_resource/requirements/#additional-feature-specific-attributes","text":"There are many attributes that you can use with requirements . To see what values you can specify for a given attribute you can run the following command while connected to your login node: $ condor_status -af {ATTR_NAME} | sort -u For example, to see what values you can specify for the Microarch attribute run: $ condor_status -af Microarch | sort -u x86_64-v1 x86_64-v2 x86_64-v3 x86_64-v4 You will find many attributes will take the boolean values true or false . Below is a list of common attributes that you can include in your submit file requirements statement. Microarch - See above. x86_64-v1, x86_64-v2, x86_64-v3, and x86_64-v4 OSGVO_OS_NAME - The name of the operating system of the compute node. The most common name is RHEL OSGVO_OS_VERSION - Version of the operating system OSGVO_OS_STRING - Combined OS name and version. Please see the requirements string above on the recommended setup. OSGVO_CPU_MODEL - The CPU model identifier string as presented in /proc/cpuinfo HAS_CVMFS_oasis_opensciencegrid_org - Attribute specifying the need to access specific oasis /cvmfs file system repositories. Other common CVMFS repositories are HAS_CVMFS_singularity_opensciencegrid_org and project ones like HAS_CVMFS_xenon_opensciencegrid_org . For GPU attribtues, such as GPUs' compute capability, see our GPU guide .","title":"Additional Feature-Specific Attributes"},{"location":"htc_workloads/specific_resource/requirements/#non-x86-based-architectures","text":"Within the computing community, there's a growing interest in exploring non-x86 architectures, such as ARM and PowerPC. As of now, the OSPool does not host resources based on these architectures; however, it is designed to accommodate them once available. The OSPool operates under a system where all tasks are configured to execute on the same architecture as the host from which they were submitted. This compatibility is ensured by HTCondor, which automatically adds the appropriate architecture to the job's requirements. By inspecting the classad of any given job, one would notice the inclusion of (TARGET.Arch == \"X86_64\") among its requirements, indicating the system's current architectural preference. If you do wish to specify a different architechure, just add it to your job requirements: requirements = Arch == \"PPC\" You can get a list of current architechures by running: $ condor_status -af Arch | sort | uniq X86_64","title":"Non-x86 Based Architectures"},{"location":"htc_workloads/specific_resource/requirements/#specifying-sites-avoiding-sites","text":"To run your jobs on a list of specific execution sites, or avoid a set of sites, use the +DESIRED_Sites / +UNDESIRED_Sites attributes in your job submit file. These attributes should only be used as a last resort. For example, it is much better to use feature attributes (see above) to make your job go to nodes matching what you really require, than to broadly allow/block whole sites. We encourage you to contact the facilitation team before taking this action, to make sure it is right for you. To avoid certain sites, first find the site names. You can find a current list by querying the pool: condor_status -af GLIDEIN_Site | sort -u In your submit file, add a comma separated list of sites like: +UNDESIRED_Sites = \"ISI,SU-ITS\" Those sites will now be exluded from the set of sites your job can run at. Similarly, you can use +DESIRED_Sites to list a subset of sites you want to target. For example, to run your jobs at the SU-ITS site, and only at that site, use: +DESIRED_Sites = \"ISI,SU-ITS\" Note that you should only specify one of +DESIRED_Sites / +UNDESIRED_Sites in the submit file. Using both at the same time will prevent the job from running.","title":"Specifying Sites / Avoiding Sites"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/","text":"Convert Your Workflow From Slurm to HTCondor \u00b6 Introduction \u00b6 Slurm is a common workload manager for high performance computing (HPC) systems while HTCondor is a scheduler program developed for a high throughput computing (HTC) environment. As they are both implementations of scheduler/workload managers, they have some similarities, like needing to specify the computing resources required for a job. Some differences include the syntax for describing a job, and some of the system assumptions made by the scheduling program. In this guide, we will go through some general similarities and differences and provide an example of \"translating\" an existing Slurm submit file into HTCondor. Skip to this example . General Diffences Between Slurm and HTCondor \u00b6 HTCondor is good at managing a large quantity of single-node jobs; Slurm is suitable for scheduling multi-node and multi-core jobs, and can struggle when managing a large quantity of jobs Slurm requires a shared file system to operate, HTCondor does not. Slurm script has a certain order - all the requirements on the top then the code execution step; HTCondor script does not have any order. The only requirement is that it ends with the queue statement. Every requirement line in the Slurm script starts with #SBATCH . In HTCondor only the system requirements (RAM, Cores, Disk space) line starts with request_ The queue statement in HTCondor can be modified (include variables) to make it behave like an array job in Slurm. Basic job submission and queue checking command starts with a condor_ prefix in HTCondor; Slurm commands generally start with the letter s . To know more about Slurm please visit their website and for HTCondor take a look at the HTCondor manual page Special Considerations for the OSPool \u00b6 HTCondor on OSPool does not use modules and a shared file system . A user needs to identify every component of their jobs and transfer them from their access point to the execute node. The slides of the new user training contians more detils about it. Instead of relying on modules, please use the different conatiners available on the OSPool or make your own container . Please remember the faciliation team is here to support you . By default the wall time limit on the OSPool is 10 hours. Comparing Slurm and HTCondor Files \u00b6 A sample Slurm script is presented below with the equivalent HTCondor transformation. Submitting One Job \u00b6 The scenario here is submitting one Matlab job, requesting 8 cores, 16GB of memory (or RAM), planning to run for 20 hours, specifying where to save standard output and error Slurm Example \u00b6 #!/bin/bash #SBATCH --job-name=sample_slurm # Optional in HTCondor #SBATCH --error=job.%J.error #SBATCH --output=job.%J.out #SBATCH --time=20:00:00 #SBATCH --nodes=1 # HTCondor equivalent does not exist #SBATCH --ntasks-per-node=8 #SBATCH --mem-per-cpu=2gb #SBATCH --partition=batch # HTCondor equivalent does not exist module load matlab/r2020a matlab -nodisplay -r \"matlab_program(input_arguments),quit\" HTCondor Example \u00b6 +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a\" executable = matlab_program arguments = input_arguments # optional batch_name = sample_htcondor error = job.$(ClusterID).$(ProcID).error output = job.$(ClusterID).$(ProcID).out log = job.$(ProcID).log # transfer_input_files = +JobDurationCategory = \"Long\" request_cpus = 8 request_memory = 16 GB request_disk = 2 GB queue 1 Notice that: - Using a Singularity image replaces module loading - The Matlab command becomes executable and arguments in the submit file - HTCondor has its own custom \"log\" format in addition to saving standard output and standard error. - If there are additional input files, they would need to be added in the \"transfer_input_files\" line. - Note that memory is total not per-core. We also need to request disk space for the job's working directory, as it is not running on a shared file system. Submit Multiple Jobs \u00b6 Using the same base example, what options are needed if you wanted to run multiple copies of the same basic job? Slurm Example \u00b6 In Slurm, multiple tasks are expressed as an array job: %%%%%%%%%%%%%%%%%highlights for submitting an array jobs %%%%%%%%%%%%%%%%%%%%%%%%%%% #SBATCH --array=0-9 module load matlab/r2020a matlab -nodisplay -r \"matlab_program(input_arguments,$SLURM_ARRAY_TASK_ID),quit\" HTCondor Example \u00b6 In HTCondor, multiple tasks are submitted as many independent jobs. The $(ProcID) variable takes the place of $SLURM_ARRAY_TASK_ID above. %%%%%%%%%%%%%%% equivalent changes to HTCondor for array jobs%%%%%%%%%%%%%%%%%%%%%%%%%% executable = matlab_program arguments = input_arguments, $(ProcID) queue 10 HTCondor has many more ways to submit multiple jobs, behind this simple numerical approach. See our other HTCondor guides for more details.","title":"Convert your workflow from Slurm to HTCondor"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#convert-your-workflow-from-slurm-to-htcondor","text":"","title":"Convert Your Workflow From Slurm to HTCondor"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#introduction","text":"Slurm is a common workload manager for high performance computing (HPC) systems while HTCondor is a scheduler program developed for a high throughput computing (HTC) environment. As they are both implementations of scheduler/workload managers, they have some similarities, like needing to specify the computing resources required for a job. Some differences include the syntax for describing a job, and some of the system assumptions made by the scheduling program. In this guide, we will go through some general similarities and differences and provide an example of \"translating\" an existing Slurm submit file into HTCondor. Skip to this example .","title":"Introduction"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#general-diffences-between-slurm-and-htcondor","text":"HTCondor is good at managing a large quantity of single-node jobs; Slurm is suitable for scheduling multi-node and multi-core jobs, and can struggle when managing a large quantity of jobs Slurm requires a shared file system to operate, HTCondor does not. Slurm script has a certain order - all the requirements on the top then the code execution step; HTCondor script does not have any order. The only requirement is that it ends with the queue statement. Every requirement line in the Slurm script starts with #SBATCH . In HTCondor only the system requirements (RAM, Cores, Disk space) line starts with request_ The queue statement in HTCondor can be modified (include variables) to make it behave like an array job in Slurm. Basic job submission and queue checking command starts with a condor_ prefix in HTCondor; Slurm commands generally start with the letter s . To know more about Slurm please visit their website and for HTCondor take a look at the HTCondor manual page","title":"General Diffences Between Slurm and HTCondor"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#special-considerations-for-the-ospool","text":"HTCondor on OSPool does not use modules and a shared file system . A user needs to identify every component of their jobs and transfer them from their access point to the execute node. The slides of the new user training contians more detils about it. Instead of relying on modules, please use the different conatiners available on the OSPool or make your own container . Please remember the faciliation team is here to support you . By default the wall time limit on the OSPool is 10 hours.","title":"Special Considerations for the OSPool"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#comparing-slurm-and-htcondor-files","text":"A sample Slurm script is presented below with the equivalent HTCondor transformation.","title":"Comparing Slurm and HTCondor Files"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#submitting-one-job","text":"The scenario here is submitting one Matlab job, requesting 8 cores, 16GB of memory (or RAM), planning to run for 20 hours, specifying where to save standard output and error","title":"Submitting One Job"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#slurm-example","text":"#!/bin/bash #SBATCH --job-name=sample_slurm # Optional in HTCondor #SBATCH --error=job.%J.error #SBATCH --output=job.%J.out #SBATCH --time=20:00:00 #SBATCH --nodes=1 # HTCondor equivalent does not exist #SBATCH --ntasks-per-node=8 #SBATCH --mem-per-cpu=2gb #SBATCH --partition=batch # HTCondor equivalent does not exist module load matlab/r2020a matlab -nodisplay -r \"matlab_program(input_arguments),quit\"","title":"Slurm Example"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#htcondor-example","text":"+SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a\" executable = matlab_program arguments = input_arguments # optional batch_name = sample_htcondor error = job.$(ClusterID).$(ProcID).error output = job.$(ClusterID).$(ProcID).out log = job.$(ProcID).log # transfer_input_files = +JobDurationCategory = \"Long\" request_cpus = 8 request_memory = 16 GB request_disk = 2 GB queue 1 Notice that: - Using a Singularity image replaces module loading - The Matlab command becomes executable and arguments in the submit file - HTCondor has its own custom \"log\" format in addition to saving standard output and standard error. - If there are additional input files, they would need to be added in the \"transfer_input_files\" line. - Note that memory is total not per-core. We also need to request disk space for the job's working directory, as it is not running on a shared file system.","title":"HTCondor Example"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#submit-multiple-jobs","text":"Using the same base example, what options are needed if you wanted to run multiple copies of the same basic job?","title":"Submit Multiple Jobs"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#slurm-example_1","text":"In Slurm, multiple tasks are expressed as an array job: %%%%%%%%%%%%%%%%%highlights for submitting an array jobs %%%%%%%%%%%%%%%%%%%%%%%%%%% #SBATCH --array=0-9 module load matlab/r2020a matlab -nodisplay -r \"matlab_program(input_arguments,$SLURM_ARRAY_TASK_ID),quit\"","title":"Slurm Example"},{"location":"htc_workloads/submitting_workloads/Slurm_to_HTCondor/#htcondor-example_1","text":"In HTCondor, multiple tasks are submitted as many independent jobs. The $(ProcID) variable takes the place of $SLURM_ARRAY_TASK_ID above. %%%%%%%%%%%%%%% equivalent changes to HTCondor for array jobs%%%%%%%%%%%%%%%%%%%%%%%%%% executable = matlab_program arguments = input_arguments, $(ProcID) queue 10 HTCondor has many more ways to submit multiple jobs, behind this simple numerical approach. See our other HTCondor guides for more details.","title":"HTCondor Example"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/","text":"Checkpointing Jobs \u00b6 What is Checkpointing? \u00b6 Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for jobs that will exceed the 10 hour maximum suggested runtime on the OSPool. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint. Why Checkpoint? \u00b6 Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit. Process of Exit Driven Checkpointing \u00b6 Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs. Requirements for Exit Driven Checkpointing \u00b6 Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing. Changes to the Submit File \u00b6 Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log +JobDurationCategory = \"Medium\" cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1 Example Wrapper Script for Checkpointing Job \u00b6 As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status=$? if [ $timeout_exit_status -eq 124 ]; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status=$? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ]; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher. Checking the Progress of Checkpointing Jobs \u00b6 It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSPool Access Point. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 . More Information \u00b6 More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"Checkpointing Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#checkpointing-jobs","text":"","title":"Checkpointing Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#what-is-checkpointing","text":"Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for jobs that will exceed the 10 hour maximum suggested runtime on the OSPool. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint.","title":"What is Checkpointing?"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#why-checkpoint","text":"Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit.","title":"Why Checkpoint?"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#process-of-exit-driven-checkpointing","text":"Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs.","title":"Process of Exit Driven Checkpointing"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#requirements-for-exit-driven-checkpointing","text":"Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing.","title":"Requirements for Exit Driven Checkpointing"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#changes-to-the-submit-file","text":"Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log +JobDurationCategory = \"Medium\" cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1","title":"Changes to the Submit File"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#example-wrapper-script-for-checkpointing-job","text":"As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status=$? if [ $timeout_exit_status -eq 124 ]; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status=$? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ]; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher.","title":"Example Wrapper Script for Checkpointing Job"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#checking-the-progress-of-checkpointing-jobs","text":"It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSPool Access Point. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 .","title":"Checking the Progress of Checkpointing Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing-on-OSPool/#more-information","text":"More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"More Information"},{"location":"htc_workloads/submitting_workloads/jupyter/","text":"OSPool Notebooks: Access the OSPool via JupyterLab \u00b6 The OSG team supports an OSPool Notebooks service, a JupyterLab interface that connects with an OSPool Access Point. An OSPool Notebook instance can be used to manage files, submit jobs, summarize results, and run tutorials. Quick Start \u00b6 Go to this link to start an OSPool Notebooks instance: Launch an OSPool Notebook You will be prompted to \"Sign in\" using your institution credentials. Once logged in, you will be automatically redirected to the \"Server Options\" page. Several server options are listed, supporting a variety of programming environment and scientific workflows. Select your desired server option and click \"Start\" to launch your instance. This process can take several minutes to complete. You will be redirected automatically when your instance is ready. If you have an existing account on the ap40.uw.osg-htc.org Access Point, the started Jupyter instance will connect to your account on that Access Point. If you don't have an existing OSPool account, your Jupyter instance will be running on a temporary Access Point as the \"joyvan\" user. For more details on the differences between these instances, see Working with your OSPool Notebooks Instance . To log out of your session, go to the top left corner of the JupyterLab interface and click the \"File\" tab. Under this tab, click \"Log Out\". Why use OSPool Notebooks? \u00b6 There are many benefits to using this service: Ease of access : All you need to access OSPool Notebooks is an internet connection and web browser! You don't need an account, ssh keys, or anything else installed on your computer. User-friendly environment : The JupyterLab environment provides access to notebooks, terminals, and text editors in a visual environment, making it easier to use for researchers with newer command line skills. Learn yourself, train others : We have self-serve tutorials that anyone can use by starting up an OSPool Notebook and then going through the materials. This can be used by individuals (with or without an OSPool account!) or by anyone who wants to run a training on using the OSPool. Integration with Access Point : If you have an existing OSPool account, on ap40.uw.osg-htc.org , the OSPool Notebook service allows you to have the above benefits as part of your full OSPool account. If you start with a guest account, and then apply for a full account, you can keep using the same interface to work with the full OSPool. Working with your OSPool Notebooks Instance \u00b6 Needed Submit File Options \u00b6 When submitting jobs from the terminal in the OSPool Notebooks interface, make sure to always include this option in your submit file: should_transfer_input = YES This option is needed for jobs to start and run successfully. OSPool Notebook Experience \u00b6 There will be slight differences in your OSPool Notebook instance, depending on whether you have an existing OSPool account and what Access Point it is on. Click on the section below that applies to you to learn more. For all users, notebooks will time out after an hour an inactivity and may run for a maximum of four hours. Timing out will not impact jobs submitted to the OSPool. For researchers with accounts on a uw.osg-htc.org access point Working in OSPool Notebooks, your account will be tied to your account on your uw.osg-htc.org access point. This means you will be able to interact with files in your /home directory, execute code, and save files, similar to like you would if you were logged into your access point via a terminal. If you submit jobs to HTCondor, by default, your jobs will run on the Open Science Pool. As of right now, these HTCondor jobs will not be able to access any data you have stored in `/protected`. Unlike logging into your access point through a terminal, when you log in through an OSPool Notebooks instance, you can run computionally intensive tasks in your /home directory. This is because each researcher has a total of 8 CPUs and 16 GB memory available to their OSPool Notebook instance's /home directory. If you would like your HTCondor jobs to run inside your Jupyter container and not on the OSPool, you can copy/paste these lines to your submit file: requirements = Machine == \"CHTC-Jupyter-User-EP-$ENV(HOSTNAME)\" +FromJupyterLab = true The requirements = and +FromJupyterLab lines tell HTCondor to assign all jobs to run on the dedicated execute point server assigned to your instance upon launch. For researchers with accounts on ap2*.uc.osg-htc.org access point Working in OSPool Notebooks, your account will not be tied to your account on your ap2*.uc.osg-htc.org access point. OSPool Notebooks are run on only our uw.osg-htc.org access points . This means your OSPool account will not be recognized. Therefore, while you are welcome to upload data to your OSPool Notebooks instance and to use the 8 CPUs and 16 GB memory available to your instance to submit HTCondor jobs and analyze data, we recommend you request an account on a uw.osg-htc.org access points access point to be able to run full OSPool workflows and to avoid having data deleted upon logging out. For researchers with guest access on an OSPool access point Our OSPool Notebooks instance is a great way to see if you would like to request an account on an OSPool access point or to practice small High Throughput Computing workflows without needing an OSPool account. Your instance has HTCondor pre-installed, which allows you to practice the job submission process required to use OSG resources. Your instance will have 8 CPUs and 16 GB of memory available to your computations. We encourage you to also attend our twice-a-month trainings (where you can use your OSPool Notebooks instance to follow along). At any time, you are welcome to request a full account that will allow you to submit jobs to the OSPool using a Jupyter-based interface. Read More \u00b6 For more information about the JupyterLab interface in general, see the JupyterLab manual .","title":"Launch a JupyterLab Instance"},{"location":"htc_workloads/submitting_workloads/jupyter/#ospool-notebooks-access-the-ospool-via-jupyterlab","text":"The OSG team supports an OSPool Notebooks service, a JupyterLab interface that connects with an OSPool Access Point. An OSPool Notebook instance can be used to manage files, submit jobs, summarize results, and run tutorials.","title":"OSPool Notebooks: Access the OSPool via JupyterLab"},{"location":"htc_workloads/submitting_workloads/jupyter/#quick-start","text":"Go to this link to start an OSPool Notebooks instance: Launch an OSPool Notebook You will be prompted to \"Sign in\" using your institution credentials. Once logged in, you will be automatically redirected to the \"Server Options\" page. Several server options are listed, supporting a variety of programming environment and scientific workflows. Select your desired server option and click \"Start\" to launch your instance. This process can take several minutes to complete. You will be redirected automatically when your instance is ready. If you have an existing account on the ap40.uw.osg-htc.org Access Point, the started Jupyter instance will connect to your account on that Access Point. If you don't have an existing OSPool account, your Jupyter instance will be running on a temporary Access Point as the \"joyvan\" user. For more details on the differences between these instances, see Working with your OSPool Notebooks Instance . To log out of your session, go to the top left corner of the JupyterLab interface and click the \"File\" tab. Under this tab, click \"Log Out\".","title":"Quick Start"},{"location":"htc_workloads/submitting_workloads/jupyter/#why-use-ospool-notebooks","text":"There are many benefits to using this service: Ease of access : All you need to access OSPool Notebooks is an internet connection and web browser! You don't need an account, ssh keys, or anything else installed on your computer. User-friendly environment : The JupyterLab environment provides access to notebooks, terminals, and text editors in a visual environment, making it easier to use for researchers with newer command line skills. Learn yourself, train others : We have self-serve tutorials that anyone can use by starting up an OSPool Notebook and then going through the materials. This can be used by individuals (with or without an OSPool account!) or by anyone who wants to run a training on using the OSPool. Integration with Access Point : If you have an existing OSPool account, on ap40.uw.osg-htc.org , the OSPool Notebook service allows you to have the above benefits as part of your full OSPool account. If you start with a guest account, and then apply for a full account, you can keep using the same interface to work with the full OSPool.","title":"Why use OSPool Notebooks?"},{"location":"htc_workloads/submitting_workloads/jupyter/#working-with-your-ospool-notebooks-instance","text":"","title":"Working with your OSPool Notebooks Instance"},{"location":"htc_workloads/submitting_workloads/jupyter/#needed-submit-file-options","text":"When submitting jobs from the terminal in the OSPool Notebooks interface, make sure to always include this option in your submit file: should_transfer_input = YES This option is needed for jobs to start and run successfully.","title":"Needed Submit File Options"},{"location":"htc_workloads/submitting_workloads/jupyter/#ospool-notebook-experience","text":"There will be slight differences in your OSPool Notebook instance, depending on whether you have an existing OSPool account and what Access Point it is on. Click on the section below that applies to you to learn more. For all users, notebooks will time out after an hour an inactivity and may run for a maximum of four hours. Timing out will not impact jobs submitted to the OSPool. For researchers with accounts on a uw.osg-htc.org access point Working in OSPool Notebooks, your account will be tied to your account on your uw.osg-htc.org access point. This means you will be able to interact with files in your /home directory, execute code, and save files, similar to like you would if you were logged into your access point via a terminal. If you submit jobs to HTCondor, by default, your jobs will run on the Open Science Pool. As of right now, these HTCondor jobs will not be able to access any data you have stored in `/protected`. Unlike logging into your access point through a terminal, when you log in through an OSPool Notebooks instance, you can run computionally intensive tasks in your /home directory. This is because each researcher has a total of 8 CPUs and 16 GB memory available to their OSPool Notebook instance's /home directory. If you would like your HTCondor jobs to run inside your Jupyter container and not on the OSPool, you can copy/paste these lines to your submit file: requirements = Machine == \"CHTC-Jupyter-User-EP-$ENV(HOSTNAME)\" +FromJupyterLab = true The requirements = and +FromJupyterLab lines tell HTCondor to assign all jobs to run on the dedicated execute point server assigned to your instance upon launch. For researchers with accounts on ap2*.uc.osg-htc.org access point Working in OSPool Notebooks, your account will not be tied to your account on your ap2*.uc.osg-htc.org access point. OSPool Notebooks are run on only our uw.osg-htc.org access points . This means your OSPool account will not be recognized. Therefore, while you are welcome to upload data to your OSPool Notebooks instance and to use the 8 CPUs and 16 GB memory available to your instance to submit HTCondor jobs and analyze data, we recommend you request an account on a uw.osg-htc.org access points access point to be able to run full OSPool workflows and to avoid having data deleted upon logging out. For researchers with guest access on an OSPool access point Our OSPool Notebooks instance is a great way to see if you would like to request an account on an OSPool access point or to practice small High Throughput Computing workflows without needing an OSPool account. Your instance has HTCondor pre-installed, which allows you to practice the job submission process required to use OSG resources. Your instance will have 8 CPUs and 16 GB of memory available to your computations. We encourage you to also attend our twice-a-month trainings (where you can use your OSPool Notebooks instance to follow along). At any time, you are welcome to request a full account that will allow you to submit jobs to the OSPool using a Jupyter-based interface.","title":"OSPool Notebook Experience"},{"location":"htc_workloads/submitting_workloads/jupyter/#read-more","text":"For more information about the JupyterLab interface in general, see the JupyterLab manual .","title":"Read More"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/","text":"Monitor and Review Jobs With condor_q and condor_history\" Objectives Monitor Queued Jobs with condor_q Default condor_q Constraints for condor_q View All Job Attributes Constraints for Job Attributes View Specific Job Attributes Across More Than One Job View Jobs that are Held View Machine Matches for a Job Review Job History with condor_history Default condor_history Constrain Your condor_history Query Viewing and Constraining Job Attributes Special Considerations More Information on Options for condor_q and condor_history Monitor and Review Jobs With condor_q and condor_history\" \u00b6 Objectives \u00b6 This guide discusses how to monitor jobs in the queue with condor_q and to review jobs that have recently left the queue with condor_history . Monitor Queued Jobs with condor_q \u00b6 Default condor_q \u00b6 The default behavior of condor_q is to list all of a user's jobs currently in HTCondor's queue grouped into batches. A batch consists of all jobs submitted using a single submit file. For example: $ condor_q -- Schedd: ap40.uw.osg-htc.org : <192.170.227.146:9618?... @ 03/04/22 12:31:45 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 21562536 3/4 12:31 _ _ 5 5 21562536.0-4 Total for query: 5 jobs; 0 completed, 0 removed, 5 idle, 0 running, 0 held, 0 suspended Total for alice: 5 jobs; 0 completed, 0 removed, 5 idle, 0 running, 0 held, 0 suspended Total for all users: 4112 jobs; 0 completed, 0 removed, 76 idle, 904 running, 3132 held, 0 suspended Constraints for condor_q \u00b6 condor_q can be used to list individual jobs associated with a username <U> , cluster ID <C> , or job ID <J> as indicated by <U/C/J> . Additionally, the flag -nobatch can be used to list individual jobs instead of batches of jobs using the format condor_q <U/C/J> -nobatch . $ condor_q alice -nobatch -- Schedd: ap40.uw.osg-htc.org : <192.170.227.146:9618?... @ 03/04/22 12:52:22 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 21562638.0 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter1.csv 21562638.1 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter2.csv 21562638.2 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter3.csv 21562638.3 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter4.csv 21562638.4 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter5.csv 21562639.0 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Alice_in_Wonderland.tx 21562639.1 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Dracula.txt 21562639.2 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Huckleberry_Finn.txt 21562639.3 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Pride_and_Prejudice.tx 21562639.4 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Ulysses.txt View All Job Attributes \u00b6 Information about HTCondor jobs are saved as \"job attributes\". Job attributes can be viewed using the -l flag, a shorthand for -long . The output of condor_q <U/C/J> -l can be used to learn more about a job and to diagnose errors. Examples of job attributes listed when using condor_q <U/C/J> -l are as follows: Attribute Description MemoryUsage Maximum memory that a job used in MB DiskUsage Maximum disk space that a job used in KB BatchName Job batch label MATCH_EXP_JOBGLIDEIN_ResourceName Location of site at which a job is running RemoteHost Location of ite and slot number where a job is running ExitCode Exit code of a job upon its completion HoldReason Human-readable message as to why a job was held. It can be used to determine if a job should be released or not. HoldReasonCode Integer value that represents why a job was put on hold JobNotification Integer indicating when the user should be emailed regarding a change of status for their job RemotePool Name of the pool in which a job is running NumRestarts Number of restarts carried out by a job Many additional attributes are provided by HTCondor to learn about your jobs, including attributes dedicated to workflows that utilize DAGman and containers. For more information about these and other attributes, please see the HTCondor Manual . Constraints for Job Attributes \u00b6 To display only the output of specified attributes, it is possible to use the \"auto format\" flag denoted as -af with condor_q <U/C/J> . An example use case is to view the owner and location of the site where a given job, such as job ID 15244592.127 , is running by using: $ condor_q 15244592.127 -af Owner MATCH_EXP_JOBGLIDEIN_ResourceName alice BNL-ATLAS In the above example, the Owner is the user alice and the job is running on resources owned by the Brookhaven National Laboratory as indicated by BNL_ATLAS . View Specific Job Attributes Across More Than One Job \u00b6 It is possible to sort and filter the output for one or more job attributes across a batch of jobs. When investigating more than one job, it is advantageous to limit the print out to a certain number of jobs to avoid flooding your screen. To limit the output to a specified number of jobs, use -limit N and replace N with the number of jobs you would like to view. For example, to view the site location where 100 jobs belonging to batch 12245532 ran, you can use: $ condor_q 12245532 -limit 100 -af MATCH_EXP_JOBGLIDEIN_ResourceName | sort | uniq -c 9 Crane 4 LSU-DB-CE1 4 ND-CAML_gpu 71 Rice-RAPID-Backfill 2 SDSC-PRP-CE1 6 TCNJ-ELSA 1 Tufts-Cluster 3 WSU-GRID In this example, 71 jobs ran at Rice University (Rice-RAPID-Backfill) while only one job ran at Tufts University (Tufts-Cluster). If you would like to know which abbreviations correspond to which compute resource provider in the OSPool, contact a Research Computing Facilitator . View Jobs that are Held \u00b6 To isolate and print out held jobs, use condor_q <U/C/J> -held . The this command will print jobs currently in the \"Held\" state and will not print jobs that are in the \"Run\", \"Done\", or \"Idle\" states. Using the job ads and constraints described above, it is possible to print out the reasons why a subset of a user's jobs are being held. $ condor_q alice -held -af HoldReason | sort | uniq -c 4 Error from glidein_3439920_345771664@c6-6-39-2.aglt2.org: SHADOW at 192.170.227.166 failed to send file(s) to <192.41.230.81:44309>: error reading from /home/alice/InputData.txt: (errno 2) No such file or directory; STARTER failed to receive file(s) from <192.170.227.166:9618> 1 Job in status 2 put on hold by SYSTEM_PERIODIC_HOLD due to memory usage 10572684. In the output above, four jobs were place on hold due to a \"missing file or directory\" in the path of /home/alice/InputData.txt that was specified in the transfer_input_files line of the submit file. Because HTCondor could not locate this input (possibly due to an incorrect file path), the job was placed on hold. Additionally, one job was placed on hold due to exceeding the requested memory specified in the submit file. An in-depth guide on troubleshooting issues with held jobs on the OSPool is available on our website. View Machine Matches for a Job \u00b6 The -analyze and -better-analyze options can be used to view the number of machines that match to a job. These flags are often used to diagnose many problems, including understanding why a job has not started running. A portion of the output from these options shows the number of machines in the pool and how many of these are able to run your job: 21607747.000: Run analysis summary ignoring user priority. Of 2189 machines, 1605 are rejected by your job's requirements 53 reject your job because of their own requirements 1 match and are already running your jobs 0 match but are serving other users 530 are able to run your job Additional output of these options include the requirements line of the job's submit file, last successful match date, hold reason messages, and other useful information. The -analyze and -better-analyze options deliver similar output, however, -better-analyze is a newer feature that provides additional information including the number of slots matched by your job given the different requirements specified in the submit file. Additional information on using -analyze and -better-analyze for troubleshooting will be available in our troubleshooting guide in the near future. Review Job History with condor_history \u00b6 Default condor_history \u00b6 Somewhat similar to condor_q , which shows jobs currently in the queue, condor_history is used to show information about jobs that have recently left the queue. By default, condor_history will show every user's job that HTCondor still has a record of in its history. Because HTCondor jobs are constantly being sent to the queue on OSG-managed Access Points, HTCondor cleans its history of jobs every few days to free up space for new jobs that have recently left the queue. Once a job is cleaned from HTCondor's history, it is removed permanently from the queue. Before a job is cleaned from HTCondor's queue, condor_history can be valuable for learning about recently completed jobs. As previously stated, condor_history without any additional flags will list every user's job, which can be thousands of lines long. To exit this behavior, use control + C . In most cases, it is recommended to combine condor_history with one or more of the options below to help limit the output of this command to only the desired information. Constrain Your condor_history Query \u00b6 Like condor_q , it is possible to limit the output of your condor_history query by user <U> , cluster ID <C> , and job ID <J> as indicated by ( <U/C/J> ). By default, HTCondor will continue to search through its history of jobs by the option it is constrained by. Since HTCondor's history is extensive, this means your command line prompt will not be returned to you until HTCondor has finished its search and analysis of its entire history. To prevent this time-consuming behavior from occurring, we recommend using the -limit N flag with condor_history . This will tell HTCondor to limit its search to the first N items that appear matching its constraint. For example, condor_history alice -limit 20 will return the condor_history output of the user alice's 20 most recently submitted jobs. Viewing and Constraining Job Attributes \u00b6 Displaying the list of job attributes using -l and -af can also be used with condor_history . It is important to note that some attributes are renamed when a job exits the queue and enters HTCondor's history. For example, RemoteHost is renamed to LastRemoteHost and HoldReason will become LastHoldReason . Special Considerations \u00b6 Although many options that exist for condor_q also exist for condor_history , some do not. For example, -analyze and -better-analyze cannot be used with condor_history . Additionally, -hold cannot be used with condor_history as no job in HTCondor's history can be in the held state. More Information on Options for condor_q and condor_history \u00b6 A full list of the options for condor_q and condor_history may be listed by using combining them with the \u2013-help flag or by viewing the HTCondor manual .","title":"Monitor and Review Jobs With condor_q and condor_history"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#monitor-and-review-jobs-with-condor_q-and-condor_history","text":"","title":"Monitor and Review Jobs With condor_q and condor_history\""},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#objectives","text":"This guide discusses how to monitor jobs in the queue with condor_q and to review jobs that have recently left the queue with condor_history .","title":"Objectives"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#monitor-queued-jobs-with-condor_q","text":"","title":"Monitor Queued Jobs with condor_q"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#default-condor_q","text":"The default behavior of condor_q is to list all of a user's jobs currently in HTCondor's queue grouped into batches. A batch consists of all jobs submitted using a single submit file. For example: $ condor_q -- Schedd: ap40.uw.osg-htc.org : <192.170.227.146:9618?... @ 03/04/22 12:31:45 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 21562536 3/4 12:31 _ _ 5 5 21562536.0-4 Total for query: 5 jobs; 0 completed, 0 removed, 5 idle, 0 running, 0 held, 0 suspended Total for alice: 5 jobs; 0 completed, 0 removed, 5 idle, 0 running, 0 held, 0 suspended Total for all users: 4112 jobs; 0 completed, 0 removed, 76 idle, 904 running, 3132 held, 0 suspended","title":"Default condor_q"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#constraints-for-condor_q","text":"condor_q can be used to list individual jobs associated with a username <U> , cluster ID <C> , or job ID <J> as indicated by <U/C/J> . Additionally, the flag -nobatch can be used to list individual jobs instead of batches of jobs using the format condor_q <U/C/J> -nobatch . $ condor_q alice -nobatch -- Schedd: ap40.uw.osg-htc.org : <192.170.227.146:9618?... @ 03/04/22 12:52:22 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 21562638.0 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter1.csv 21562638.1 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter2.csv 21562638.2 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter3.csv 21562638.3 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter4.csv 21562638.4 alice 3/4 12:52 0+00:00:00 I 0 0.0 soilModel.py parameter5.csv 21562639.0 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Alice_in_Wonderland.tx 21562639.1 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Dracula.txt 21562639.2 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Huckleberry_Finn.txt 21562639.3 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Pride_and_Prejudice.tx 21562639.4 alice 3/4 12:52 0+00:00:00 I 0 0.0 wordcount.py Ulysses.txt","title":"Constraints for condor_q"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#view-all-job-attributes","text":"Information about HTCondor jobs are saved as \"job attributes\". Job attributes can be viewed using the -l flag, a shorthand for -long . The output of condor_q <U/C/J> -l can be used to learn more about a job and to diagnose errors. Examples of job attributes listed when using condor_q <U/C/J> -l are as follows: Attribute Description MemoryUsage Maximum memory that a job used in MB DiskUsage Maximum disk space that a job used in KB BatchName Job batch label MATCH_EXP_JOBGLIDEIN_ResourceName Location of site at which a job is running RemoteHost Location of ite and slot number where a job is running ExitCode Exit code of a job upon its completion HoldReason Human-readable message as to why a job was held. It can be used to determine if a job should be released or not. HoldReasonCode Integer value that represents why a job was put on hold JobNotification Integer indicating when the user should be emailed regarding a change of status for their job RemotePool Name of the pool in which a job is running NumRestarts Number of restarts carried out by a job Many additional attributes are provided by HTCondor to learn about your jobs, including attributes dedicated to workflows that utilize DAGman and containers. For more information about these and other attributes, please see the HTCondor Manual .","title":"View All Job Attributes"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#constraints-for-job-attributes","text":"To display only the output of specified attributes, it is possible to use the \"auto format\" flag denoted as -af with condor_q <U/C/J> . An example use case is to view the owner and location of the site where a given job, such as job ID 15244592.127 , is running by using: $ condor_q 15244592.127 -af Owner MATCH_EXP_JOBGLIDEIN_ResourceName alice BNL-ATLAS In the above example, the Owner is the user alice and the job is running on resources owned by the Brookhaven National Laboratory as indicated by BNL_ATLAS .","title":"Constraints for Job Attributes"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#view-specific-job-attributes-across-more-than-one-job","text":"It is possible to sort and filter the output for one or more job attributes across a batch of jobs. When investigating more than one job, it is advantageous to limit the print out to a certain number of jobs to avoid flooding your screen. To limit the output to a specified number of jobs, use -limit N and replace N with the number of jobs you would like to view. For example, to view the site location where 100 jobs belonging to batch 12245532 ran, you can use: $ condor_q 12245532 -limit 100 -af MATCH_EXP_JOBGLIDEIN_ResourceName | sort | uniq -c 9 Crane 4 LSU-DB-CE1 4 ND-CAML_gpu 71 Rice-RAPID-Backfill 2 SDSC-PRP-CE1 6 TCNJ-ELSA 1 Tufts-Cluster 3 WSU-GRID In this example, 71 jobs ran at Rice University (Rice-RAPID-Backfill) while only one job ran at Tufts University (Tufts-Cluster). If you would like to know which abbreviations correspond to which compute resource provider in the OSPool, contact a Research Computing Facilitator .","title":"View Specific Job Attributes Across More Than One Job"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#view-jobs-that-are-held","text":"To isolate and print out held jobs, use condor_q <U/C/J> -held . The this command will print jobs currently in the \"Held\" state and will not print jobs that are in the \"Run\", \"Done\", or \"Idle\" states. Using the job ads and constraints described above, it is possible to print out the reasons why a subset of a user's jobs are being held. $ condor_q alice -held -af HoldReason | sort | uniq -c 4 Error from glidein_3439920_345771664@c6-6-39-2.aglt2.org: SHADOW at 192.170.227.166 failed to send file(s) to <192.41.230.81:44309>: error reading from /home/alice/InputData.txt: (errno 2) No such file or directory; STARTER failed to receive file(s) from <192.170.227.166:9618> 1 Job in status 2 put on hold by SYSTEM_PERIODIC_HOLD due to memory usage 10572684. In the output above, four jobs were place on hold due to a \"missing file or directory\" in the path of /home/alice/InputData.txt that was specified in the transfer_input_files line of the submit file. Because HTCondor could not locate this input (possibly due to an incorrect file path), the job was placed on hold. Additionally, one job was placed on hold due to exceeding the requested memory specified in the submit file. An in-depth guide on troubleshooting issues with held jobs on the OSPool is available on our website.","title":"View Jobs that are Held"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#view-machine-matches-for-a-job","text":"The -analyze and -better-analyze options can be used to view the number of machines that match to a job. These flags are often used to diagnose many problems, including understanding why a job has not started running. A portion of the output from these options shows the number of machines in the pool and how many of these are able to run your job: 21607747.000: Run analysis summary ignoring user priority. Of 2189 machines, 1605 are rejected by your job's requirements 53 reject your job because of their own requirements 1 match and are already running your jobs 0 match but are serving other users 530 are able to run your job Additional output of these options include the requirements line of the job's submit file, last successful match date, hold reason messages, and other useful information. The -analyze and -better-analyze options deliver similar output, however, -better-analyze is a newer feature that provides additional information including the number of slots matched by your job given the different requirements specified in the submit file. Additional information on using -analyze and -better-analyze for troubleshooting will be available in our troubleshooting guide in the near future.","title":"View Machine Matches for a Job"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#review-job-history-with-condor_history","text":"","title":"Review Job History with condor_history"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#default-condor_history","text":"Somewhat similar to condor_q , which shows jobs currently in the queue, condor_history is used to show information about jobs that have recently left the queue. By default, condor_history will show every user's job that HTCondor still has a record of in its history. Because HTCondor jobs are constantly being sent to the queue on OSG-managed Access Points, HTCondor cleans its history of jobs every few days to free up space for new jobs that have recently left the queue. Once a job is cleaned from HTCondor's history, it is removed permanently from the queue. Before a job is cleaned from HTCondor's queue, condor_history can be valuable for learning about recently completed jobs. As previously stated, condor_history without any additional flags will list every user's job, which can be thousands of lines long. To exit this behavior, use control + C . In most cases, it is recommended to combine condor_history with one or more of the options below to help limit the output of this command to only the desired information.","title":"Default condor_history"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#constrain-your-condor_history-query","text":"Like condor_q , it is possible to limit the output of your condor_history query by user <U> , cluster ID <C> , and job ID <J> as indicated by ( <U/C/J> ). By default, HTCondor will continue to search through its history of jobs by the option it is constrained by. Since HTCondor's history is extensive, this means your command line prompt will not be returned to you until HTCondor has finished its search and analysis of its entire history. To prevent this time-consuming behavior from occurring, we recommend using the -limit N flag with condor_history . This will tell HTCondor to limit its search to the first N items that appear matching its constraint. For example, condor_history alice -limit 20 will return the condor_history output of the user alice's 20 most recently submitted jobs.","title":"Constrain Your condor_history Query"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#viewing-and-constraining-job-attributes","text":"Displaying the list of job attributes using -l and -af can also be used with condor_history . It is important to note that some attributes are renamed when a job exits the queue and enters HTCondor's history. For example, RemoteHost is renamed to LastRemoteHost and HoldReason will become LastHoldReason .","title":"Viewing and Constraining Job Attributes"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#special-considerations","text":"Although many options that exist for condor_q also exist for condor_history , some do not. For example, -analyze and -better-analyze cannot be used with condor_history . Additionally, -hold cannot be used with condor_history as no job in HTCondor's history can be in the held state.","title":"Special Considerations"},{"location":"htc_workloads/submitting_workloads/monitor_review_jobs/#more-information-on-options-for-condor_q-and-condor_history","text":"A full list of the options for condor_q and condor_history may be listed by using combining them with the \u2013-help flag or by viewing the HTCondor manual .","title":"More Information on Options for condor_q and condor_history"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/","text":"Submit and Resubmit Multiple Jobs using Bash Scripting \u00b6 This guide is based on the original scripts from Jarryd Ramborger at the George Lab at UC San Diego, presented at HTC'24 ( watch the presentation ). You can reach Jarryd at jkramborger@health.ucsd.edu Overview \u00b6 HTCondor allows for submitting multiple jobs from a single submit file . Many users find the queue <var> from <list> option particularly helpful, as it allows users to submit multiple jobs with one or more distinct variables per job. However, creating this list for hundreds of jobs becomes inefficient to do manually. How can you automate creation of this list? Furthermore, what if you want to rerun only some runs and not all? This guide will walk through the basics of submitting and rerunning jobs with multiple variables by leveraging bash scripting. To fully utilize this guide, users should be familiar with using queue <var> from <list> in submitting multiple jobs . Before you start: Consider your own workflow \u00b6 Every user's workflow is different and has varying needs. You will need to evaluate your own workflow and come up with a strategy for writing scripts for submitting and resubmitting jobs. Here are some questions to consider when strategizing how to automate your workflow: What are common elements within the list of jobs that I need to submit? How do correctly loop through the list of input files and/or arguments in my submit file? How can I uniquely name my outputs so that they don't overwrite each other? If my some of my jobs fail, what parts of their output files (including standard error, output, and log files) distinguishes them from successful jobs? How can I utilize this difference in rerunning my jobs? General strategy Write a bash script to create a list of variables. (e.g. ls *.mp4 > input_list.txt ). Use queue <var> from <list> to submit multiple jobs based on the variables. To resubmit multiple jobs, write a bash script to update your list of variables. In this guide, we will walk through one detailed example on how to leverage bash scripting and the queue statement to submit multiple jobs. However, your needs may differ from what's presented. We encourage you to consider how to integrate or modify these methods into your own workflow as you read this guide. Submit multiple jobs matching a filename or file extension \u00b6 Consider a pipeline in which a researcher needs to analyze hundreds of .mp4 video files saved in multiple directories under different names. Below is a snippet of a tree diagram of what their file directory structure might look like. Their data is stored in subdirectories in videos/ , and they have created separate directories for their standard error ( err/ ), standard output ( out/ ), and final analysis ( analysis/ ). /home/username/ |-- analysis/ |-- analyze_videos.sh |-- err/ |-- log/ |-- out/ |-- videos/ | |-- dataset1/ | | |-- 2024-09-18-mouse1.mp4 | | |-- 2024-09-18-mouse2.mp4 | | |-- 2024-09-18-mouse3.mp4 | | `-- notes.txt | |-- dataset2/ | | |-- 2024-10-31-mouse1.mp4 | | |-- 2024-10-31-mouse2.mp4 | | `-- notes.txt | ... The researcher needs to run analyze_videos.sh on each .mp4 file. They plan to use the queue <var> from <list> syntax to submit all their jobs from a single submit file. To create their list, they can manually copy and paste the path of each .mp4 file into a text document, but this can soon becomes a tedious task. How can they automate this process? Create a list of files using Bash scripting \u00b6 Instead of copy and pasting the path to each video manually, we can leverage Bash scripting to do this for us. There are multiple ways to achieve this: if all the files were in one directory, we could use simple ls *.mp4 > job_list.txt command, similar to the example in the Easily Submit Multiple Jobs guide . However, the files are listed in multiple sub-directories, so in this case, we will use the find command to recursively find the .mp4 files. find can be a powerful tool - read more about the find command here . Our plan is to print the path for each file, as well as the basename of the file itself. These two elements will then be utilized as variables for our HTCondor submit file. Below is the script create_list.sh , which accomplishes these tasks: #!/bin/bash # Set name of the file containing the list of jobs job_list=job_list.txt # Specify directory with videos videos_directory=videos # Remove the job list file, if it exists, for a clean start if [ -f \"${job_list}\" ] ; then rm \"${job_list}\" fi # Append list of videos with .mp4 extension to the job list file for video in $(find ${videos_directory} -type f -name \"*.mp4\"); do echo \"${video}, $(basename ${video} .mp4)\" >> ${job_list} echo \"${video} added to ${job_list}\" done What the script doing in the for loop? For users unfamiliar with Bash scripting, the above code can be daunting. Let's break it down step-by-step. Recall that the goal is to grab a list of paths to the .mp4 files, as well as the basenames for the files themselves. The .mp4 files are listed in multiple sub-directories. To find those .mp4 files, we can use the find command: find ${videos_directory} -type f -name \"*.mp4\" The first argument after the find command indicates which directory to search in. In this case, we pass the value assigned to ${videos_directory} , which should be videos . Next, -type f indicates that we want to find a file (in contrast to finding a directory). Lastly, -name \"*.mp4\" indicates we want to find files that match a certain naming pattern. The wildcard *.mp4 indicates a name with any length of characters as long as it ends with .mp4 . The full find command above would generate a list of full paths to files: videos/dataset1/2024-09-18-mouse2.mp4 videos/dataset1/2024-09-18-mouse1.mp4 videos/dataset2/2024-10-31-mouse1.mp4 videos/dataset2/2024-10-31-mouse2.mp4 We can then use for to loop through this list and assign each line to the variable ${video} . The $() captures the output of the find command, which is then used in the for loop. for video in $(find ${videos_directory} -type f -name \"*.mp4\"); do Each time the for loop runs, we want to collect information about the path and the basename in job_list.txt . The path is already assigned to the variable ${video} . To get the basename, we use the basename command. When we pass the path into basename and use .mp4 as the second argument, it returns the basename without the .mp4 extension. basename ${video} .mp4 We can then print both the path and the basename to our ${job_list} file, separated by a comma. echo \"${video}, $(basename ${video} .mp4)\" >> ${job_list} The next line prints a message to the terminal to let us know that the corresponding file is added to the rerun list. This line could be omitted. echo \"${video} added to ${job_list}\" Once the script is created, we change the script to be executable and run the script. When this is complete, job_list.txt now contains a list of absolute paths to the .mp4 files in the videos directory and the basenames of each .mp4 file. [username@ap40 ~ ]$ chmod +x create_list.sh [username@ap40 ~ ]$ ./create_list.sh videos/dataset1/2024-09-18-mouse2.mp4 added to job_list.txt videos/dataset1/2024-09-18-mouse1.mp4 added to job_list.txt videos/dataset2/2024-10-31-mouse1.mp4 added to job_list.txt videos/dataset2/2024-10-31-mouse2.mp4 added to job_list.txt [username@ap40 ~ ]$ ls analysis/ create_list.sh job_list.txt out/ analyze_videos.sh err/ log/ videos/ [username@ap40 ~ ]$ cat job_list.txt videos/dataset1/2024-09-18-mouse2.mp4, 2024-09-18-mouse2 videos/dataset1/2024-09-18-mouse1.mp4, 2024-09-18-mouse1 videos/dataset2/2024-10-31-mouse1.mp4, 2024-10-31-mouse1 videos/dataset2/2024-10-31-mouse2.mp4, 2024-10-31-mouse2 Notice that in our list, we have the name of the file without the .mp4 extension. In our script, we used the basename program within the loop to remove the .mp4 file extension. Why might we want to do this? We can use the basename of our file (e.g. 2024-09-18-mouse1 ) in the filename of our outputs to keep our files organized. Leaving out the .mp4 extension in these names will make naming these outputs easier. Submit multiple jobs using queue <var> from <list> \u00b6 Now, that we have a list, we can use job_list.txt in our HTCondor submit file using the queue <var> from <list> syntax. Below is our submit file, job.sub . # job.sub executable = analyze_videos.sh arguments = $(video) log = log/job_$(Cluster).log error = err/$(video)_$(Cluster).err output = out/$(video)_$(Cluster).out transfer_input_files = $(path) transfer_output_files = $(video).txt transfer_output_remaps = \"$(video).txt = analysis/$(video).txt\" +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 10GB request_disk = 10GB queue path, video from job_list.txt Notice the queue statement on the last line. As HTCondor loops through each line in job_list.txt , the values of $(path) and $(video) will change correspondingly. We set transfer_input_files to reference each .mp4 file using the $(path) value from the list. We also pass $(video) as an argument to our executable, so our executable knows the basename of the video file and can proceed with its analysis. Lastly, we remap the output file so that HTCondor transfers it to the analysis folder. The researcher can now submit the job. When the job is finished, the output files of the analysis, log, and other files are in their respective folders, as seen in this tree diagram. /home/username/ |-- analysis | |-- 2024-09-18-mouse1.txt | |-- 2024-10-31-mouse1.txt |-- analyze_videos.sh |-- create_list.sh |-- err | |-- 2024-09-18-mouse1_800457.err | |-- 2024-09-18-mouse2_800457.err | |-- 2024-10-31-mouse1_800457.err | `-- 2024-10-31-mouse2_800457.err |-- job_list.txt |-- job.sub |-- log | `-- job_800457.log |-- out | |-- 2024-09-18-mouse1_800457.out | |-- 2024-09-18-mouse2_800457.out | |-- 2024-10-31-mouse1_800457.out | `-- 2024-10-31-mouse2_800457.out ... Notice that 2024-09-18-mouse2.txt and 2024-10-31-mouse2.txt does not exist in the analysis folder. In our hypothetical scenario, these particular jobs failed, and our researcher needs to rerun the jobs after identifying what caused the failure. The researcher could manually create another job_list.txt file with the right information to resubmit the job, but if there are many failures, this could become a tedious task. In the next section, we will see how to automate resubmission of jobs from a batch submission. Resubmit selected jobs from a batch submission \u00b6 Sometimes, one or more jobs in a batch of jobs will fail. Failure can be caused by various reasons: going over requested memory, inconsistent files \u2014 the potential reasons are too numerous to list! How can we resubmit a few jobs out of the entire batch of jobs in an automated fashion? First, we need to identify the differences between a failed job and a successful job . In the previous example, if our researcher's calculation was successful, it would transfer the results of the analysis, a .txt file, to the analysis folder. If the analysis was unsuccessful, the corresponding .txt file would not exist. After rectifying the point of failure (such as increasing requested memory), the researcher needs to resubmit the job for videos with the missing analyses. Because the missing analyses have no .txt files in the analysis folder, the researcher realized that they need a script that checks for which analyses are missing and create a new list of videos for submission. Create an updated list of files using Bash scripting \u00b6 One strategy we can use in this scenario is to compare job_list.txt with the files currently in the analysis directory. If the basename of our .mp4 video matches with a .txt file in the analysis directory, we can skip the corresponding job. If the basename does not have a match, then we should add that line of the job list to the rerun list. Below is the script create_rerun_list.sh : #!/bin/bash # Define input and output files for job list input_list=job_list.txt rerun_list=rerun_list.txt # Define analysis directory analysis_dir=analysis # Remove the rerun list file, if it exists, for a clean start if [ -f \"${rerun_list}\" ] ; then rm \"${rerun_list}\" fi # Loop through input_list and finds the basename: # If there is no match, add it to rerun_list # If there is a match in the analysis folder, skip while read line; do match=$( awk '{print $2}' <<< \"${line}\") if [ $(ls ${analysis_dir}/*.txt | grep -c ${match}) -eq 0 ]; then echo \"${line}\" >> ${rerun_list} echo \"${match} added to ${rerun_list}.\" else echo \"${match} has already been run. Skipping.\" fi done < ${input_list} What the script doing in the while loop? For users unfamiliar with Bash scripting, the above code can be daunting. Let's break it down step-by-step. The while block The beginning of the while block reads a file and assigns each line to the variable $line : while read line; do The end of the block uses ${input_list} as the file to be read: done < ${input_list} Assigning the basename to ${match} The first line in the while block uses multiple bash commands. match=$( awk '{print $2}' <<< \"${line}\") Recall the format of each line in job_list.txt : videos/dataset1/2024-09-18-mouse2.mp4, 2024-09-18-mouse2 The basename ( 2024-09-18-mouse2 ) can be used to match to files in the analysis directory. The basename is the second item in the line, so we use awk '{print2}' to grab the second item. However, we still need to refer to the corresponding line in our list. To achieve that, <<< feeds ${line} into the awk command. Lastly, the variable ${match} is assigned to the value of the entire command, which is encased in $() . The if conditional statement Next, we use an if block to make decisions about which lines to add to our list. if [ $(ls ${analysis_dir}/*.txt | grep -c ${match}) -eq 0 ]; How did we come to writing this condition? Our goal is to match the basename ( 2024-09-18-mouse2 ) to a .txt file in the analysis directory. We can get a list of the .txt files in the analysis directory using the ls command with the wildcard * : ls ${analysis_dir}/*.txt We can then pipe ( | ) the output of the ls command to the grep command to see if the basename ${match} exists in the output of the above ls command. Additionally, the -c flag for grep counts the number of matches. The combination of those pieces then becomes: ls ${analysis_dir}/*.txt | grep -c ${match} Lastly, we need to evaluate whether there is a match. If there is a match, the above command will return a number greater than 0. If there is no match, the command will return zero. Thus, we set the condition to see if the command (encased in $() ) evaluates to zero with -eq 0 . The then and else blocks Lastly, we print or skip lines depending on whether there is or isn't a match. The then block: If the conditional statement evaluates to 0, then that means there is no match, and we should add the line to the rerun list. echo \"${line}\" >> ${rerun_list} The next line prints a message to the terminal to let us know that the corresponding file is added to the rerun list. This line could be omitted. echo \"${match} added to ${rerun_list}.\" The else block: If the conditional statement returns a value other than 0, then that means there is a match, and the line should not be added to the rerun list. This line could be omitted. echo \"${match} has already been run. Skipping.\" We can execute create_rerun_list.sh and observe which video files will be added to the rerun list, and which will be skipped. [username@ap40 ~ ]$ ./create_rerun_list.sh 2024-09-18-mouse2 added to rerun_list.txt. 2024-09-18-mouse1 has already been run. Skipping. 2024-10-31-mouse1 has already been run. Skipping. 2024-10-31-mouse2 added to rerun_list.txt. Resubmit selected jobs \u00b6 Our researcher is now ready to resubmit the selected jobs. Instead of editing the job.sub file, it will be useful to create separate submit file, job_rerun.sub , where the only difference is the queue statement. queue path, video from rerun_list.txt Our researcher can now submit the new jobs, and two jobs are submitted accordingly. [username@ap40 ~ ]$ condor_submit job_rerun.sub Submitting job(s).. 2 job(s) submitted to cluster 800458. Other tips and techniques \u00b6 Useful commands \u00b6 Often your bash scripts will use commands like find , grep , ls , and awk . Exploring these commands will widen your toolkit for writing automated scripts. Arguments \u00b6 You can generalize your scripts further by using arguments in your Bash scripts. For example, we could change the first few lines of create_list.sh to accept arguments: #!/bin/bash # Set name of the file containing the list of jobs job_list=$1 # Specify directory with videos videos_directory=$2 ... This allows us to specify the name of our job list file and directory containing our videos without having to constantly edit the script. When we run the new script, the arguments we put in are then assigned to the corresponding variables in the script. [username@ap40 ~] ./create_list.sh new_list.txt updated_videos updated_videos/dataset1/2024-09-18-mouse2.mp4 added to new_list.txt updated_videos/dataset1/2024-09-18-mouse1.mp4 added to new_list.txt updated_videos/dataset2/2024-10-31-mouse1.mp4 added to new_list.txt updated_videos/dataset2/2024-10-31-mouse2.mp4 added to new_list.txt Using variables in the condor_submit command \u00b6 In the above example, we copied job.sub to job_rerun.sub and needed to edit the queue statement to refer to rerun_list.txt . Instead of doing this, we can use a variable in the queue statement. queue path, video from $(job_list) We can assign the variable and submit the job in one step. [username@ap40 ~] condor_submit job.sub job_list=rerun_list.txt Submitting job(s).. 2 job(s) submitted to cluster 800469. Warning Double-check your files and do a small-scale test before using variables in the condor_submit command. Any mistakes in a submitting a large number of jobs could potentially mean a large mess to clean up! Get Help \u00b6 For assistance, questions, or discussing strategy, please email the OSG Research Facilitation team at support@osg-htc.org or attend our Virtual Office Hours .","title":"Submit and Resubmit Multiple Jobs using Bash Scripting"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#submit-and-resubmit-multiple-jobs-using-bash-scripting","text":"This guide is based on the original scripts from Jarryd Ramborger at the George Lab at UC San Diego, presented at HTC'24 ( watch the presentation ). You can reach Jarryd at jkramborger@health.ucsd.edu","title":"Submit and Resubmit Multiple Jobs using Bash Scripting"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#overview","text":"HTCondor allows for submitting multiple jobs from a single submit file . Many users find the queue <var> from <list> option particularly helpful, as it allows users to submit multiple jobs with one or more distinct variables per job. However, creating this list for hundreds of jobs becomes inefficient to do manually. How can you automate creation of this list? Furthermore, what if you want to rerun only some runs and not all? This guide will walk through the basics of submitting and rerunning jobs with multiple variables by leveraging bash scripting. To fully utilize this guide, users should be familiar with using queue <var> from <list> in submitting multiple jobs .","title":"Overview"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#before-you-start-consider-your-own-workflow","text":"Every user's workflow is different and has varying needs. You will need to evaluate your own workflow and come up with a strategy for writing scripts for submitting and resubmitting jobs. Here are some questions to consider when strategizing how to automate your workflow: What are common elements within the list of jobs that I need to submit? How do correctly loop through the list of input files and/or arguments in my submit file? How can I uniquely name my outputs so that they don't overwrite each other? If my some of my jobs fail, what parts of their output files (including standard error, output, and log files) distinguishes them from successful jobs? How can I utilize this difference in rerunning my jobs? General strategy Write a bash script to create a list of variables. (e.g. ls *.mp4 > input_list.txt ). Use queue <var> from <list> to submit multiple jobs based on the variables. To resubmit multiple jobs, write a bash script to update your list of variables. In this guide, we will walk through one detailed example on how to leverage bash scripting and the queue statement to submit multiple jobs. However, your needs may differ from what's presented. We encourage you to consider how to integrate or modify these methods into your own workflow as you read this guide.","title":"Before you start: Consider your own workflow"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#submit-multiple-jobs-matching-a-filename-or-file-extension","text":"Consider a pipeline in which a researcher needs to analyze hundreds of .mp4 video files saved in multiple directories under different names. Below is a snippet of a tree diagram of what their file directory structure might look like. Their data is stored in subdirectories in videos/ , and they have created separate directories for their standard error ( err/ ), standard output ( out/ ), and final analysis ( analysis/ ). /home/username/ |-- analysis/ |-- analyze_videos.sh |-- err/ |-- log/ |-- out/ |-- videos/ | |-- dataset1/ | | |-- 2024-09-18-mouse1.mp4 | | |-- 2024-09-18-mouse2.mp4 | | |-- 2024-09-18-mouse3.mp4 | | `-- notes.txt | |-- dataset2/ | | |-- 2024-10-31-mouse1.mp4 | | |-- 2024-10-31-mouse2.mp4 | | `-- notes.txt | ... The researcher needs to run analyze_videos.sh on each .mp4 file. They plan to use the queue <var> from <list> syntax to submit all their jobs from a single submit file. To create their list, they can manually copy and paste the path of each .mp4 file into a text document, but this can soon becomes a tedious task. How can they automate this process?","title":"Submit multiple jobs matching a filename or file extension"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#create-a-list-of-files-using-bash-scripting","text":"Instead of copy and pasting the path to each video manually, we can leverage Bash scripting to do this for us. There are multiple ways to achieve this: if all the files were in one directory, we could use simple ls *.mp4 > job_list.txt command, similar to the example in the Easily Submit Multiple Jobs guide . However, the files are listed in multiple sub-directories, so in this case, we will use the find command to recursively find the .mp4 files. find can be a powerful tool - read more about the find command here . Our plan is to print the path for each file, as well as the basename of the file itself. These two elements will then be utilized as variables for our HTCondor submit file. Below is the script create_list.sh , which accomplishes these tasks: #!/bin/bash # Set name of the file containing the list of jobs job_list=job_list.txt # Specify directory with videos videos_directory=videos # Remove the job list file, if it exists, for a clean start if [ -f \"${job_list}\" ] ; then rm \"${job_list}\" fi # Append list of videos with .mp4 extension to the job list file for video in $(find ${videos_directory} -type f -name \"*.mp4\"); do echo \"${video}, $(basename ${video} .mp4)\" >> ${job_list} echo \"${video} added to ${job_list}\" done What the script doing in the for loop? For users unfamiliar with Bash scripting, the above code can be daunting. Let's break it down step-by-step. Recall that the goal is to grab a list of paths to the .mp4 files, as well as the basenames for the files themselves. The .mp4 files are listed in multiple sub-directories. To find those .mp4 files, we can use the find command: find ${videos_directory} -type f -name \"*.mp4\" The first argument after the find command indicates which directory to search in. In this case, we pass the value assigned to ${videos_directory} , which should be videos . Next, -type f indicates that we want to find a file (in contrast to finding a directory). Lastly, -name \"*.mp4\" indicates we want to find files that match a certain naming pattern. The wildcard *.mp4 indicates a name with any length of characters as long as it ends with .mp4 . The full find command above would generate a list of full paths to files: videos/dataset1/2024-09-18-mouse2.mp4 videos/dataset1/2024-09-18-mouse1.mp4 videos/dataset2/2024-10-31-mouse1.mp4 videos/dataset2/2024-10-31-mouse2.mp4 We can then use for to loop through this list and assign each line to the variable ${video} . The $() captures the output of the find command, which is then used in the for loop. for video in $(find ${videos_directory} -type f -name \"*.mp4\"); do Each time the for loop runs, we want to collect information about the path and the basename in job_list.txt . The path is already assigned to the variable ${video} . To get the basename, we use the basename command. When we pass the path into basename and use .mp4 as the second argument, it returns the basename without the .mp4 extension. basename ${video} .mp4 We can then print both the path and the basename to our ${job_list} file, separated by a comma. echo \"${video}, $(basename ${video} .mp4)\" >> ${job_list} The next line prints a message to the terminal to let us know that the corresponding file is added to the rerun list. This line could be omitted. echo \"${video} added to ${job_list}\" Once the script is created, we change the script to be executable and run the script. When this is complete, job_list.txt now contains a list of absolute paths to the .mp4 files in the videos directory and the basenames of each .mp4 file. [username@ap40 ~ ]$ chmod +x create_list.sh [username@ap40 ~ ]$ ./create_list.sh videos/dataset1/2024-09-18-mouse2.mp4 added to job_list.txt videos/dataset1/2024-09-18-mouse1.mp4 added to job_list.txt videos/dataset2/2024-10-31-mouse1.mp4 added to job_list.txt videos/dataset2/2024-10-31-mouse2.mp4 added to job_list.txt [username@ap40 ~ ]$ ls analysis/ create_list.sh job_list.txt out/ analyze_videos.sh err/ log/ videos/ [username@ap40 ~ ]$ cat job_list.txt videos/dataset1/2024-09-18-mouse2.mp4, 2024-09-18-mouse2 videos/dataset1/2024-09-18-mouse1.mp4, 2024-09-18-mouse1 videos/dataset2/2024-10-31-mouse1.mp4, 2024-10-31-mouse1 videos/dataset2/2024-10-31-mouse2.mp4, 2024-10-31-mouse2 Notice that in our list, we have the name of the file without the .mp4 extension. In our script, we used the basename program within the loop to remove the .mp4 file extension. Why might we want to do this? We can use the basename of our file (e.g. 2024-09-18-mouse1 ) in the filename of our outputs to keep our files organized. Leaving out the .mp4 extension in these names will make naming these outputs easier.","title":"Create a list of files using Bash scripting"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#submit-multiple-jobs-using-queue-var-from-list","text":"Now, that we have a list, we can use job_list.txt in our HTCondor submit file using the queue <var> from <list> syntax. Below is our submit file, job.sub . # job.sub executable = analyze_videos.sh arguments = $(video) log = log/job_$(Cluster).log error = err/$(video)_$(Cluster).err output = out/$(video)_$(Cluster).out transfer_input_files = $(path) transfer_output_files = $(video).txt transfer_output_remaps = \"$(video).txt = analysis/$(video).txt\" +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 10GB request_disk = 10GB queue path, video from job_list.txt Notice the queue statement on the last line. As HTCondor loops through each line in job_list.txt , the values of $(path) and $(video) will change correspondingly. We set transfer_input_files to reference each .mp4 file using the $(path) value from the list. We also pass $(video) as an argument to our executable, so our executable knows the basename of the video file and can proceed with its analysis. Lastly, we remap the output file so that HTCondor transfers it to the analysis folder. The researcher can now submit the job. When the job is finished, the output files of the analysis, log, and other files are in their respective folders, as seen in this tree diagram. /home/username/ |-- analysis | |-- 2024-09-18-mouse1.txt | |-- 2024-10-31-mouse1.txt |-- analyze_videos.sh |-- create_list.sh |-- err | |-- 2024-09-18-mouse1_800457.err | |-- 2024-09-18-mouse2_800457.err | |-- 2024-10-31-mouse1_800457.err | `-- 2024-10-31-mouse2_800457.err |-- job_list.txt |-- job.sub |-- log | `-- job_800457.log |-- out | |-- 2024-09-18-mouse1_800457.out | |-- 2024-09-18-mouse2_800457.out | |-- 2024-10-31-mouse1_800457.out | `-- 2024-10-31-mouse2_800457.out ... Notice that 2024-09-18-mouse2.txt and 2024-10-31-mouse2.txt does not exist in the analysis folder. In our hypothetical scenario, these particular jobs failed, and our researcher needs to rerun the jobs after identifying what caused the failure. The researcher could manually create another job_list.txt file with the right information to resubmit the job, but if there are many failures, this could become a tedious task. In the next section, we will see how to automate resubmission of jobs from a batch submission.","title":"Submit multiple jobs using queue &lt;var&gt; from &lt;list&gt;"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#resubmit-selected-jobs-from-a-batch-submission","text":"Sometimes, one or more jobs in a batch of jobs will fail. Failure can be caused by various reasons: going over requested memory, inconsistent files \u2014 the potential reasons are too numerous to list! How can we resubmit a few jobs out of the entire batch of jobs in an automated fashion? First, we need to identify the differences between a failed job and a successful job . In the previous example, if our researcher's calculation was successful, it would transfer the results of the analysis, a .txt file, to the analysis folder. If the analysis was unsuccessful, the corresponding .txt file would not exist. After rectifying the point of failure (such as increasing requested memory), the researcher needs to resubmit the job for videos with the missing analyses. Because the missing analyses have no .txt files in the analysis folder, the researcher realized that they need a script that checks for which analyses are missing and create a new list of videos for submission.","title":"Resubmit selected jobs from a batch submission"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#create-an-updated-list-of-files-using-bash-scripting","text":"One strategy we can use in this scenario is to compare job_list.txt with the files currently in the analysis directory. If the basename of our .mp4 video matches with a .txt file in the analysis directory, we can skip the corresponding job. If the basename does not have a match, then we should add that line of the job list to the rerun list. Below is the script create_rerun_list.sh : #!/bin/bash # Define input and output files for job list input_list=job_list.txt rerun_list=rerun_list.txt # Define analysis directory analysis_dir=analysis # Remove the rerun list file, if it exists, for a clean start if [ -f \"${rerun_list}\" ] ; then rm \"${rerun_list}\" fi # Loop through input_list and finds the basename: # If there is no match, add it to rerun_list # If there is a match in the analysis folder, skip while read line; do match=$( awk '{print $2}' <<< \"${line}\") if [ $(ls ${analysis_dir}/*.txt | grep -c ${match}) -eq 0 ]; then echo \"${line}\" >> ${rerun_list} echo \"${match} added to ${rerun_list}.\" else echo \"${match} has already been run. Skipping.\" fi done < ${input_list} What the script doing in the while loop? For users unfamiliar with Bash scripting, the above code can be daunting. Let's break it down step-by-step. The while block The beginning of the while block reads a file and assigns each line to the variable $line : while read line; do The end of the block uses ${input_list} as the file to be read: done < ${input_list} Assigning the basename to ${match} The first line in the while block uses multiple bash commands. match=$( awk '{print $2}' <<< \"${line}\") Recall the format of each line in job_list.txt : videos/dataset1/2024-09-18-mouse2.mp4, 2024-09-18-mouse2 The basename ( 2024-09-18-mouse2 ) can be used to match to files in the analysis directory. The basename is the second item in the line, so we use awk '{print2}' to grab the second item. However, we still need to refer to the corresponding line in our list. To achieve that, <<< feeds ${line} into the awk command. Lastly, the variable ${match} is assigned to the value of the entire command, which is encased in $() . The if conditional statement Next, we use an if block to make decisions about which lines to add to our list. if [ $(ls ${analysis_dir}/*.txt | grep -c ${match}) -eq 0 ]; How did we come to writing this condition? Our goal is to match the basename ( 2024-09-18-mouse2 ) to a .txt file in the analysis directory. We can get a list of the .txt files in the analysis directory using the ls command with the wildcard * : ls ${analysis_dir}/*.txt We can then pipe ( | ) the output of the ls command to the grep command to see if the basename ${match} exists in the output of the above ls command. Additionally, the -c flag for grep counts the number of matches. The combination of those pieces then becomes: ls ${analysis_dir}/*.txt | grep -c ${match} Lastly, we need to evaluate whether there is a match. If there is a match, the above command will return a number greater than 0. If there is no match, the command will return zero. Thus, we set the condition to see if the command (encased in $() ) evaluates to zero with -eq 0 . The then and else blocks Lastly, we print or skip lines depending on whether there is or isn't a match. The then block: If the conditional statement evaluates to 0, then that means there is no match, and we should add the line to the rerun list. echo \"${line}\" >> ${rerun_list} The next line prints a message to the terminal to let us know that the corresponding file is added to the rerun list. This line could be omitted. echo \"${match} added to ${rerun_list}.\" The else block: If the conditional statement returns a value other than 0, then that means there is a match, and the line should not be added to the rerun list. This line could be omitted. echo \"${match} has already been run. Skipping.\" We can execute create_rerun_list.sh and observe which video files will be added to the rerun list, and which will be skipped. [username@ap40 ~ ]$ ./create_rerun_list.sh 2024-09-18-mouse2 added to rerun_list.txt. 2024-09-18-mouse1 has already been run. Skipping. 2024-10-31-mouse1 has already been run. Skipping. 2024-10-31-mouse2 added to rerun_list.txt.","title":"Create an updated list of files using Bash scripting"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#resubmit-selected-jobs","text":"Our researcher is now ready to resubmit the selected jobs. Instead of editing the job.sub file, it will be useful to create separate submit file, job_rerun.sub , where the only difference is the queue statement. queue path, video from rerun_list.txt Our researcher can now submit the new jobs, and two jobs are submitted accordingly. [username@ap40 ~ ]$ condor_submit job_rerun.sub Submitting job(s).. 2 job(s) submitted to cluster 800458.","title":"Resubmit selected jobs"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#other-tips-and-techniques","text":"","title":"Other tips and techniques"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#useful-commands","text":"Often your bash scripts will use commands like find , grep , ls , and awk . Exploring these commands will widen your toolkit for writing automated scripts.","title":"Useful commands"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#arguments","text":"You can generalize your scripts further by using arguments in your Bash scripts. For example, we could change the first few lines of create_list.sh to accept arguments: #!/bin/bash # Set name of the file containing the list of jobs job_list=$1 # Specify directory with videos videos_directory=$2 ... This allows us to specify the name of our job list file and directory containing our videos without having to constantly edit the script. When we run the new script, the arguments we put in are then assigned to the corresponding variables in the script. [username@ap40 ~] ./create_list.sh new_list.txt updated_videos updated_videos/dataset1/2024-09-18-mouse2.mp4 added to new_list.txt updated_videos/dataset1/2024-09-18-mouse1.mp4 added to new_list.txt updated_videos/dataset2/2024-10-31-mouse1.mp4 added to new_list.txt updated_videos/dataset2/2024-10-31-mouse2.mp4 added to new_list.txt","title":"Arguments"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#using-variables-in-the-condor_submit-command","text":"In the above example, we copied job.sub to job_rerun.sub and needed to edit the queue statement to refer to rerun_list.txt . Instead of doing this, we can use a variable in the queue statement. queue path, video from $(job_list) We can assign the variable and submit the job in one step. [username@ap40 ~] condor_submit job.sub job_list=rerun_list.txt Submitting job(s).. 2 job(s) submitted to cluster 800469. Warning Double-check your files and do a small-scale test before using variables in the condor_submit command. Any mistakes in a submitting a large number of jobs could potentially mean a large mess to clean up!","title":"Using variables in the condor_submit command"},{"location":"htc_workloads/submitting_workloads/re-submit-with-bash/#get-help","text":"For assistance, questions, or discussing strategy, please email the OSG Research Facilitation team at support@osg-htc.org or attend our Virtual Office Hours .","title":"Get Help"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/","text":"Easily Submit Multiple Jobs \u00b6 Overview \u00b6 HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? As described in our Policies for using an OSPool Access Point , users should submit multiple jobs using a single submit file, or where applicable, as few separate submit files as needed. Using HTCondor multi-job submission features is more efficient for users and will help ensure reliable operation of the the login nodes. Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. If you are interested in a particular approach that isn't described here, please contact OSG Facilitators and we will work with you to identify options to meet the needs of your work. Submit Multiple Jobs Using queue \u00b6 All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc. 1. Use queue N in you HTCondor submit files \u00b6 When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include: A. Use integer numbered input files \u00b6 [user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100 B. Specify a row or column number for each job \u00b6 $(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software. C. Need N to start at 1 \u00b6 If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables. 2. Submit multiple jobs with one or more distinct variables per job \u00b6 Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial . Use multiple variables for each job \u00b6 Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt 3. Organizing Jobs Into Individual Directories \u00b6 One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Easily Submit Multiple Jobs"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#easily-submit-multiple-jobs","text":"","title":"Easily Submit Multiple Jobs"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#overview","text":"HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? As described in our Policies for using an OSPool Access Point , users should submit multiple jobs using a single submit file, or where applicable, as few separate submit files as needed. Using HTCondor multi-job submission features is more efficient for users and will help ensure reliable operation of the the login nodes. Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. If you are interested in a particular approach that isn't described here, please contact OSG Facilitators and we will work with you to identify options to meet the needs of your work.","title":"Overview"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#submit-multiple-jobs-using-queue","text":"All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc.","title":"Submit Multiple Jobs Using queue"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#1-use-queue-n-in-you-htcondor-submit-files","text":"When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include:","title":"1. Use queue N in you HTCondor submit files"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#a-use-integer-numbered-input-files","text":"[user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100","title":"A. Use integer numbered input files"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#b-specify-a-row-or-column-number-for-each-job","text":"$(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software.","title":"B. Specify a row or column number for each job"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#c-need-n-to-start-at-1","text":"If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables.","title":"C. Need N to start at 1"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#2-submit-multiple-jobs-with-one-or-more-distinct-variables-per-job","text":"Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial .","title":"2. Submit multiple jobs with one or more distinct variables per job"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#use-multiple-variables-for-each-job","text":"Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt","title":"Use multiple variables for each job"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#3-organizing-jobs-into-individual-directories","text":"One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location.","title":"3. Organizing Jobs Into Individual Directories"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Get Help"},{"location":"htc_workloads/submitting_workloads/tutorial-command/","text":"Workflow Tutorials \u00b6 OSPool workflow tutorials on Github \u00b6 All of the OSG provided tutorials are available as repositories on Github . These tutorials are tested regularly and should work as is, but if you experience any issues please contact us. Available tutorials \u00b6 The following tutorials are available and are compatible with OSG-provided Access Points: Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points blast-split ............ How to run BLAST on the OSPool by splitting a large input file fastqc ................. How to run FastQC on the OSPool dagman-wordfreq ........ DAGMan based wordfreq example error101 ............... Use condor_q -better-analyze to analyze stuck jobs matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World osg-locations .......... Tutorial based on OSPool location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager quickstart ............. How to run your first OSPool job scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSPool - matrix multiplication Install and setup a tutorial \u00b6 On an OSPool Access Point, type the following to download a tutorial's materials: $ git clone https://github.com/OSGConnect/<tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"List of Available Tutorials"},{"location":"htc_workloads/submitting_workloads/tutorial-command/#workflow-tutorials","text":"","title":"Workflow Tutorials"},{"location":"htc_workloads/submitting_workloads/tutorial-command/#ospool-workflow-tutorials-on-github","text":"All of the OSG provided tutorials are available as repositories on Github . These tutorials are tested regularly and should work as is, but if you experience any issues please contact us.","title":"OSPool workflow tutorials on Github"},{"location":"htc_workloads/submitting_workloads/tutorial-command/#available-tutorials","text":"The following tutorials are available and are compatible with OSG-provided Access Points: Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points blast-split ............ How to run BLAST on the OSPool by splitting a large input file fastqc ................. How to run FastQC on the OSPool dagman-wordfreq ........ DAGMan based wordfreq example error101 ............... Use condor_q -better-analyze to analyze stuck jobs matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World osg-locations .......... Tutorial based on OSPool location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager quickstart ............. How to run your first OSPool job scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSPool - matrix multiplication","title":"Available tutorials"},{"location":"htc_workloads/submitting_workloads/tutorial-command/#install-and-setup-a-tutorial","text":"On an OSPool Access Point, type the following to download a tutorial's materials: $ git clone https://github.com/OSGConnect/<tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Install and setup a tutorial"},{"location":"htc_workloads/submitting_workloads/tutorial-error101/","text":"Troubleshooting Job Errors \u00b6 In this lesson, we'll learn how to troubleshoot jobs that never start or fail in unexpected ways. Troubleshooting techniques \u00b6 Diagnostics with condor_q \u00b6 The condor_q command shows the status of the jobs and it can be used to diagnose why jobs are not running. Using the -better-analyze flag with condor_q can show you detailed information about why a job isn't starting on a specific pool. Since OSG Connect sends jobs to many places, we also need to specify a pool name with the -pool flag. Unless you know a specific pool you would like to query, checking the flock.opensciencegrid.org pool is usually a good place to start. $ condor_q -better-analyze JOB-ID -pool POOL-NAME Let's do an example. First we'll need to login as usual, and then load the tutorial error101 . $ ssh username@login.osgconnect.net $ tutorial error101 $ cd tutorial-error101 $ condor_submit error101_job.submit We'll check the job status the normal way: condor_q username For some reason, our job is still idle. Why? Try using condor_q -better-analyze to find out. Remember that you will also need to specify a pool name. In this case we'll use flock.opensciencegrid.org : $ condor_q -better-analyze JOB-ID -pool flock.opensciencegrid.org # Produces a long ouput. # The following lines are part of the output regarding the job requirements. The Requirements expression for your job reduces to these conditions: Slots Step Matched Condition ----- -------- --------- [0] 10674 TARGET.Arch == \"X86_64\" [1] 10674 TARGET.OpSys == \"LINUX\" [3] 10674 TARGET.Disk >= RequestDisk [5] 0 TARGET.Memory >= RequestMemory [8] 10674 TARGET.HasFileTransfer By looking through the match conditions, we see that many nodes match our requests for the Linux operating system and the x86_64 architecture, but none of them match our requirement for 51200 MB of memory. Let's look at our submit script and see if we can find the source of this error: $ cat error101_job.submit Universe = vanilla Executable = error101.sh # to sleep an hour Arguments = 3600 request_memory = 2 TB Error = job.err Output = job.out Log = job.log Queue 1 See the request_memory line? We are asking for 2 Terabytes of memory, when we meant to only ask for 2 Gigabytes of memory. Our job is not matching any available job slots because none of the slots offer 2 TB of memory. Let's fix that by changing that line to read request_memory = 2 GB . $ nano error101_job.submit Let's cancel our idle job with the condor_rm command and then resubmit our edited job: $ condor_rm JOB-ID $ condor_submit error101_job.submit Alternatively, you can edit the resource requirements of the idle job in queue: condor_qedit JOB_ID RequestMemory 2048 Held jobs and condor_release \u00b6 Occasionally, a job can fail in various ways and go into \"Held\" state. Held state means that the job has encountered some error, and cannot run. This doesn't necessarily mean that your job has failed, but, for whatever reason, Condor cannot fulfill your request(s). In this particular case, a user had this in his or her Condor submit file: transfer_output_files = outputfile However, when the job executed, it went into Held state: $ condor_q -analyze 372993.0 -- Submitter: login01.osgconnect.net : <192.170.227.195:56174> : login01.osgconnect.net --- 372993.000: Request is held. Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485>: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory; SHADOW failed to receive file(s) from <192.84.86.100:50805> Let's break down this error message piece by piece: Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485> This part is quite cryptic, but it simply means that the worker node where your job executed (glidein_9371@compute-6-28.tier2 or 10.3.11.39) tried to transfer a file to the OSG Connect login node (192.170.227.195) but did not succeed. The next part explains why: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory This bit has the full path of the file that Condor tried to transfer back to login.osgconnect.net . The reason why the file transfer failed is because outputfile was never created on the worker node. Remember that at the beginning we said that the user specifically requested transfer_outputfiles = outputfile ! Condor could not complete this request, and so the job went into Held state instead of finishing normally. It's quite possible that the error was simply transient, and if we retry, the job will succeed. We can re-queue a job that is in Held state by using condor_release : condor_release JOB-ID","title":"Troubleshooting Job Errors"},{"location":"htc_workloads/submitting_workloads/tutorial-error101/#troubleshooting-job-errors","text":"In this lesson, we'll learn how to troubleshoot jobs that never start or fail in unexpected ways.","title":"Troubleshooting Job Errors"},{"location":"htc_workloads/submitting_workloads/tutorial-error101/#troubleshooting-techniques","text":"","title":"Troubleshooting techniques"},{"location":"htc_workloads/submitting_workloads/tutorial-error101/#diagnostics-with-condor_q","text":"The condor_q command shows the status of the jobs and it can be used to diagnose why jobs are not running. Using the -better-analyze flag with condor_q can show you detailed information about why a job isn't starting on a specific pool. Since OSG Connect sends jobs to many places, we also need to specify a pool name with the -pool flag. Unless you know a specific pool you would like to query, checking the flock.opensciencegrid.org pool is usually a good place to start. $ condor_q -better-analyze JOB-ID -pool POOL-NAME Let's do an example. First we'll need to login as usual, and then load the tutorial error101 . $ ssh username@login.osgconnect.net $ tutorial error101 $ cd tutorial-error101 $ condor_submit error101_job.submit We'll check the job status the normal way: condor_q username For some reason, our job is still idle. Why? Try using condor_q -better-analyze to find out. Remember that you will also need to specify a pool name. In this case we'll use flock.opensciencegrid.org : $ condor_q -better-analyze JOB-ID -pool flock.opensciencegrid.org # Produces a long ouput. # The following lines are part of the output regarding the job requirements. The Requirements expression for your job reduces to these conditions: Slots Step Matched Condition ----- -------- --------- [0] 10674 TARGET.Arch == \"X86_64\" [1] 10674 TARGET.OpSys == \"LINUX\" [3] 10674 TARGET.Disk >= RequestDisk [5] 0 TARGET.Memory >= RequestMemory [8] 10674 TARGET.HasFileTransfer By looking through the match conditions, we see that many nodes match our requests for the Linux operating system and the x86_64 architecture, but none of them match our requirement for 51200 MB of memory. Let's look at our submit script and see if we can find the source of this error: $ cat error101_job.submit Universe = vanilla Executable = error101.sh # to sleep an hour Arguments = 3600 request_memory = 2 TB Error = job.err Output = job.out Log = job.log Queue 1 See the request_memory line? We are asking for 2 Terabytes of memory, when we meant to only ask for 2 Gigabytes of memory. Our job is not matching any available job slots because none of the slots offer 2 TB of memory. Let's fix that by changing that line to read request_memory = 2 GB . $ nano error101_job.submit Let's cancel our idle job with the condor_rm command and then resubmit our edited job: $ condor_rm JOB-ID $ condor_submit error101_job.submit Alternatively, you can edit the resource requirements of the idle job in queue: condor_qedit JOB_ID RequestMemory 2048","title":"Diagnostics with condor_q"},{"location":"htc_workloads/submitting_workloads/tutorial-error101/#held-jobs-and-condor_release","text":"Occasionally, a job can fail in various ways and go into \"Held\" state. Held state means that the job has encountered some error, and cannot run. This doesn't necessarily mean that your job has failed, but, for whatever reason, Condor cannot fulfill your request(s). In this particular case, a user had this in his or her Condor submit file: transfer_output_files = outputfile However, when the job executed, it went into Held state: $ condor_q -analyze 372993.0 -- Submitter: login01.osgconnect.net : <192.170.227.195:56174> : login01.osgconnect.net --- 372993.000: Request is held. Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485>: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory; SHADOW failed to receive file(s) from <192.84.86.100:50805> Let's break down this error message piece by piece: Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485> This part is quite cryptic, but it simply means that the worker node where your job executed (glidein_9371@compute-6-28.tier2 or 10.3.11.39) tried to transfer a file to the OSG Connect login node (192.170.227.195) but did not succeed. The next part explains why: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory This bit has the full path of the file that Condor tried to transfer back to login.osgconnect.net . The reason why the file transfer failed is because outputfile was never created on the worker node. Remember that at the beginning we said that the user specifically requested transfer_outputfiles = outputfile ! Condor could not complete this request, and so the job went into Held state instead of finishing normally. It's quite possible that the error was simply transient, and if we retry, the job will succeed. We can re-queue a job that is in Held state by using condor_release : condor_release JOB-ID","title":"Held jobs and condor_release"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/","text":"Organizing and Submitting HTC Workloads \u00b6 Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. This tutorial starts with the same set up as our Wordcount Tutorial for Submitting Multiple Jobs , but focuses on how to organize that example more effectively on the Access Point, with an eye to scaling up to a larger HTC workload in the future. Our Workload \u00b6 We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt Try running the command to see what the output is for the script. Once you have done that delete the output file created ( rm counts.Alice_in_Wonderland.txt ). We want to run this script on all the books we have copies of. What is the input set for this HTC workload? What is the output set? Make an Organization Plan \u00b6 Based on what you know about the script, inputs, and outputs, how would you organize this HTC workload in directories (folders) on the access point? There will also be system and HTCondor files produced when we submit a job -- how would you organize the log, standard error and standard output files? Try making those changes before moving on to the next section of the tutorial. Organize Files \u00b6 There are many different ways to organize files; a simple example that works for most workloads is having a directory for your input files and a directory for your output files. We can set up this structure on the command line by running: $ mkdir input $ mv *.txt input/ $ mkdir output/ We can view our current directory and its subdirectories by using the recursive flag with the ls command: $ ls -R README.md books.submit input output wordcount.py ./input: Alice_in_Wonderland.txt Huckleberry_Finn.txt Ulysses.txt Dracula.txt Pride_and_Prejudice.txt ./output: We are also going to create directories for the HTCondor log files and the standard error and standard output files (in one directory): $ mkdir logs $ mkdir errout Submit One Job \u00b6 Now we want to submit a test job that uses this organizing scheme, using just one item in our input set -- in this example, we'll use the Alice_in_Wonderland.txt file from our input/ directory. The lines that need to be filled in are shown below and can be edited using the nano text editor: $ nano books.submit executable = wordcount.py arguments = Alice_in_Wonderland.txt transfer_input_files = input/Alice_in_Wonderland.txt transfer_output_files = counts.Alice_in_Wonderland.txt transfer_output_remaps = \"counts.Alice_in_Wonderland.txt=output/counts.Alice_in_Wonderland.txt\" Note that to tell HTCondor the location of the input file, we need to include the input directory. We're also using a submit file option called transfer_output_remaps that will essentially move the output file to our output/ directory by renaming or remapping it. We also want to edit the submit file lines that tell the log, error and output files where to go: $ nano books.submit output = logs/job.$(ClusterID).$(ProcID).out error = errout/job.$(ClusterID).$(ProcID).err log = errout/job.$(ClusterID).$(ProcID).log Once you've made the above changes to the books.submit file, you can submit it, and monitor its progress: $ condor_submit books.submit $ condor_watch_q (Type CTRL - C to stop the condor_watch_q command.) Submit Multiple Jobs \u00b6 We are now sufficiently organized to submit our whole workload. First, we need to create a file with our input set -- in this case, it will be a list of the book files we want to analyze. We can do this by using the shell's listing command ls and redirecting the output to a file: $ cd input $ ls > booklist.txt $ cat booklist.txt $ mv booklist.txt .. $ cd .. Then, we modify our submit file to reference this input list and replace the static values from our test job ( Alice_in_Wonderland.txt ) with a variable -- we've chosen $(book) below: $ nano books.submit executable = wordcount.py arguments = $(book) transfer_input_files = input/$(book) transfer_output_files = counts.$(book) transfer_output_remaps = \"counts.$(book)=output/counts.$(book)\" # other options queue book from booklist.txt Once this is done, you can submit the jobs as usual: $ condor_submit books.submit","title":"Organizing and Submitting HTC Workloads"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/#organizing-and-submitting-htc-workloads","text":"Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. This tutorial starts with the same set up as our Wordcount Tutorial for Submitting Multiple Jobs , but focuses on how to organize that example more effectively on the Access Point, with an eye to scaling up to a larger HTC workload in the future.","title":"Organizing and Submitting HTC Workloads"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/#our-workload","text":"We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt Try running the command to see what the output is for the script. Once you have done that delete the output file created ( rm counts.Alice_in_Wonderland.txt ). We want to run this script on all the books we have copies of. What is the input set for this HTC workload? What is the output set?","title":"Our Workload"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/#make-an-organization-plan","text":"Based on what you know about the script, inputs, and outputs, how would you organize this HTC workload in directories (folders) on the access point? There will also be system and HTCondor files produced when we submit a job -- how would you organize the log, standard error and standard output files? Try making those changes before moving on to the next section of the tutorial.","title":"Make an Organization Plan"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/#organize-files","text":"There are many different ways to organize files; a simple example that works for most workloads is having a directory for your input files and a directory for your output files. We can set up this structure on the command line by running: $ mkdir input $ mv *.txt input/ $ mkdir output/ We can view our current directory and its subdirectories by using the recursive flag with the ls command: $ ls -R README.md books.submit input output wordcount.py ./input: Alice_in_Wonderland.txt Huckleberry_Finn.txt Ulysses.txt Dracula.txt Pride_and_Prejudice.txt ./output: We are also going to create directories for the HTCondor log files and the standard error and standard output files (in one directory): $ mkdir logs $ mkdir errout","title":"Organize Files"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/#submit-one-job","text":"Now we want to submit a test job that uses this organizing scheme, using just one item in our input set -- in this example, we'll use the Alice_in_Wonderland.txt file from our input/ directory. The lines that need to be filled in are shown below and can be edited using the nano text editor: $ nano books.submit executable = wordcount.py arguments = Alice_in_Wonderland.txt transfer_input_files = input/Alice_in_Wonderland.txt transfer_output_files = counts.Alice_in_Wonderland.txt transfer_output_remaps = \"counts.Alice_in_Wonderland.txt=output/counts.Alice_in_Wonderland.txt\" Note that to tell HTCondor the location of the input file, we need to include the input directory. We're also using a submit file option called transfer_output_remaps that will essentially move the output file to our output/ directory by renaming or remapping it. We also want to edit the submit file lines that tell the log, error and output files where to go: $ nano books.submit output = logs/job.$(ClusterID).$(ProcID).out error = errout/job.$(ClusterID).$(ProcID).err log = errout/job.$(ClusterID).$(ProcID).log Once you've made the above changes to the books.submit file, you can submit it, and monitor its progress: $ condor_submit books.submit $ condor_watch_q (Type CTRL - C to stop the condor_watch_q command.)","title":"Submit One Job"},{"location":"htc_workloads/submitting_workloads/tutorial-organizing/#submit-multiple-jobs","text":"We are now sufficiently organized to submit our whole workload. First, we need to create a file with our input set -- in this case, it will be a list of the book files we want to analyze. We can do this by using the shell's listing command ls and redirecting the output to a file: $ cd input $ ls > booklist.txt $ cat booklist.txt $ mv booklist.txt .. $ cd .. Then, we modify our submit file to reference this input list and replace the static values from our test job ( Alice_in_Wonderland.txt ) with a variable -- we've chosen $(book) below: $ nano books.submit executable = wordcount.py arguments = $(book) transfer_input_files = input/$(book) transfer_output_files = counts.$(book) transfer_output_remaps = \"counts.$(book)=output/counts.$(book)\" # other options queue book from booklist.txt Once this is done, you can submit the jobs as usual: $ condor_submit books.submit","title":"Submit Multiple Jobs"},{"location":"htc_workloads/submitting_workloads/tutorial-osg-locations/","text":"Finding OSG Locations \u00b6 In this section, we will learn how to quickly submit multiple jobs simultaneously using HTCondor and we will visualize where these jobs run so we can get an idea of where and jobs are distributed on the Open Science Pool. Gathering network information from the OSG \u00b6 Now to create a submit file that will run in the OSG! Use the tutorial command to download the job submission files: tutorial osg-locations . Change into the tutorial-osg-locations directory with cd tutorial-osg-locations . Hostname fetching code \u00b6 The following Python script finds the ClassAd of the machine it's running on and finds a network identity that can be used to perform lookups: #!/bin/env python import re import os import socket machine_ad_file_name = os.getenv('_CONDOR_MACHINE_AD') try: machine_ad_file = open(machine_ad_file_name, 'r') machine_ad = machine_ad_file.read() machine_ad_file.close() except TypeError: print socket.getfqdn() exit(1) try: print re.search(r'GLIDEIN_Gatekeeper = \"(.*):\\d*/jobmanager-\\w*\"', machine_ad, re.MULTILINE).group(1) except AttributeError: try: print re.search(r'GLIDEIN_Gatekeeper = \"(\\S+) \\S+:9619\"', machine_ad, re.MULTILINE).group(1) except AttributeError: exit(1) This script ( wn-geoip.py ) is contained in the zipped archive ( wn-geoip.tar.gz ) that is transferred to the job and unpacked by the job wrapper script location-wrapper.sh . You will be using location-wrapper.sh as your executable and wn-geoip.tar.gz as an input file. The submit file for this job, scalingup.submit , is setup to specify these files and submit 100 jobs simultaneously. It also uses the job's process value to create unique output, error and log files for each of the job. $ cat scalingup.submit # The following requirments ensure we land on compute nodes # which have all the dependencies (modules, so we can # module load python2.7) and avoid some machines where # GeoIP does not work (such as Kubernetes containers) requirements = OSG_OS_STRING == \"RHEL 7\" && HAS_MODULES && GLIDEIN_Gatekeeper =!= UNDEFINED # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = location-wrapper.sh transfer_input_files = wn-geoip.tar.gz # We can specify unique filenames for each job by using # the job's 'process' value. error = job.$(Process).error output = job.$(Process).output log = job.$(Process).log # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue 100 jobs with the above specifications. queue 100 Submit this job using the condor_submit command: $ condor_submit scalingup.submit Wait for the results. Remember, you can use watch condor_q to monitor the status of your jobs. Collating your results \u00b6 Now that you have your results, it's time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format job.#.output (e.g., job.10.output ), your command will look something like this: $ cat job.*.output The * is a wildcard so the above cat command runs on all files that start with job- and end in .output . Additionally, you can use cat in combination with the sort and uniq commands to print only the unique results: $ cat job.*.output | sort | uniq Mapping your results \u00b6 To visualize the locations of the machines that your jobs ran on, you will be using http://www.mapcustomizer.com/. Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Where did your jobs run?","title":"Finding OSG Locations"},{"location":"htc_workloads/submitting_workloads/tutorial-osg-locations/#finding-osg-locations","text":"In this section, we will learn how to quickly submit multiple jobs simultaneously using HTCondor and we will visualize where these jobs run so we can get an idea of where and jobs are distributed on the Open Science Pool.","title":"Finding OSG Locations"},{"location":"htc_workloads/submitting_workloads/tutorial-osg-locations/#gathering-network-information-from-the-osg","text":"Now to create a submit file that will run in the OSG! Use the tutorial command to download the job submission files: tutorial osg-locations . Change into the tutorial-osg-locations directory with cd tutorial-osg-locations .","title":"Gathering network information from the OSG"},{"location":"htc_workloads/submitting_workloads/tutorial-osg-locations/#hostname-fetching-code","text":"The following Python script finds the ClassAd of the machine it's running on and finds a network identity that can be used to perform lookups: #!/bin/env python import re import os import socket machine_ad_file_name = os.getenv('_CONDOR_MACHINE_AD') try: machine_ad_file = open(machine_ad_file_name, 'r') machine_ad = machine_ad_file.read() machine_ad_file.close() except TypeError: print socket.getfqdn() exit(1) try: print re.search(r'GLIDEIN_Gatekeeper = \"(.*):\\d*/jobmanager-\\w*\"', machine_ad, re.MULTILINE).group(1) except AttributeError: try: print re.search(r'GLIDEIN_Gatekeeper = \"(\\S+) \\S+:9619\"', machine_ad, re.MULTILINE).group(1) except AttributeError: exit(1) This script ( wn-geoip.py ) is contained in the zipped archive ( wn-geoip.tar.gz ) that is transferred to the job and unpacked by the job wrapper script location-wrapper.sh . You will be using location-wrapper.sh as your executable and wn-geoip.tar.gz as an input file. The submit file for this job, scalingup.submit , is setup to specify these files and submit 100 jobs simultaneously. It also uses the job's process value to create unique output, error and log files for each of the job. $ cat scalingup.submit # The following requirments ensure we land on compute nodes # which have all the dependencies (modules, so we can # module load python2.7) and avoid some machines where # GeoIP does not work (such as Kubernetes containers) requirements = OSG_OS_STRING == \"RHEL 7\" && HAS_MODULES && GLIDEIN_Gatekeeper =!= UNDEFINED # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = location-wrapper.sh transfer_input_files = wn-geoip.tar.gz # We can specify unique filenames for each job by using # the job's 'process' value. error = job.$(Process).error output = job.$(Process).output log = job.$(Process).log # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue 100 jobs with the above specifications. queue 100 Submit this job using the condor_submit command: $ condor_submit scalingup.submit Wait for the results. Remember, you can use watch condor_q to monitor the status of your jobs.","title":"Hostname fetching code"},{"location":"htc_workloads/submitting_workloads/tutorial-osg-locations/#collating-your-results","text":"Now that you have your results, it's time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format job.#.output (e.g., job.10.output ), your command will look something like this: $ cat job.*.output The * is a wildcard so the above cat command runs on all files that start with job- and end in .output . Additionally, you can use cat in combination with the sort and uniq commands to print only the unique results: $ cat job.*.output | sort | uniq","title":"Collating your results"},{"location":"htc_workloads/submitting_workloads/tutorial-osg-locations/#mapping-your-results","text":"To visualize the locations of the machines that your jobs ran on, you will be using http://www.mapcustomizer.com/. Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Where did your jobs run?","title":"Mapping your results"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/","text":"Quickstart - Submit Example HTCondor Jobs \u00b6 Login to OSG Access Point \u00b6 To begin, login to your OSG Access Point. Pretyped Setup \u00b6 To save some typing, you can download the tutorial materials into your home directory on the access point. This is highly recommended to ensure that you don't encounter transcription errors during the tutorials $ git clone https://github.com/OSGConnect/tutorial-quickstart Now, let's start the quickstart tutorial: $ cd tutorial-quickstart Manual Setup \u00b6 Alternatively, if you want the full manual experience, create a new directory for the tutorial work: $ mkdir tutorial-quickstart $ cd tutorial-quickstart Example Jobs \u00b6 Job 1: A single discovery job \u00b6 Inside the tutorial directory that you created or installed previously, let's create a test script to execute as your job. For pretyped setup, this is the short.sh file: #!/bin/bash # short.sh: a short discovery job printf \"Start time: \"; /bin/date printf \"Job is running on node: \"; /bin/hostname printf \"Job running as user: \"; /usr/bin/id printf \"Job is running in directory: \"; /bin/pwd echo echo \"Working hard...\" sleep 20 echo \"Science complete!\" Now, make the script executable. $ chmod +x short.sh Run the script locally \u00b6 When setting up a new job submission, it's important to test your job outside of HTCondor before submitting into the Open Science Pool. $ ./short.sh Start time: Wed Aug 08 09:21:35 CDT 2023 Job is running on node: ap50.ux.osg-htc.org Job running as user: uid=54161(alice), gid=5782(osg) groups=5782(osg),5513(osg.login-nodes),7158(osg.OSG-Staff) Job is running in directory: /home/alice/tutorial-quickstart Working hard... Science complete! Create an HTCondor submit file \u00b6 So far, so good! Let's create a simple (if verbose) HTCondor submit file. This can be found in tutorial01.submit . # Our executable is the main program or script that we've created # to do the 'work' of a single job. executable = short.sh # We need to name the files that HTCondor should create to save the # terminal output (stdout) and error (stderr) created by our job. # Similarly, we need to name the log file where HTCondor will save # information about job execution steps. log = short.log error = short.error output = short.output # This is the default category for jobs +JobDurationCategory = \"Medium\" # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # The last line of a submit file indicates how many jobs of the above # description should be queued. We'll start with one job. queue 1 Submit the job \u00b6 Submit the job using condor_submit : $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 144121. Check the job status \u00b6 The condor_q command tells the status of currently running jobs. $ condor_q -- Schedd: ap50.ux.osg-htc.org : < 192.170.227.22:9618?... @ 08/10/23 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 1441271 08/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for alice: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended You can also get the status of a specific job cluster: $ condor_q 1441271 -- Schedd: ap50.ux.osg-htc.org : < 192.170.227.22:9618?... @ 08/10/23 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 1441271 08/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for alice: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended Note the DONE , RUN , and IDLE columns. Your job will be listed in the IDLE column if it hasn't started yet. If it's currently scheduled and running, it will appear in the RUN column. As it finishes up, it will then show in the DONE column. Once the job completes completely, it will not appear in condor_q . Let's wait for your job to finish \u2013 that is, for condor_q not to show the job in its output. A useful tool for this is condor_watch_q \u2013 it efficiently monitors the status of your jobs by monitoring their corresponding log files. Let's submit the job again, and use condor_watch_q to follow the progress of your job (the status will update at two-second intervals): $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 1441272 $ condor_watch_q ... Note : To close condor_watch_q, hold down Ctrl and press C. Check the job output \u00b6 Once your job has finished, you can look at the files that HTCondor has returned to the working directory. The names of these files were specified in our submit file. If everything was successful, it should have returned: a log file from HTCondor for the job cluster: short.log This file can tell you where a job ran, how long it ran, and what resources it used. If a job shows up as \"held\" in condor_q , this file will have a message that gives a reason why. an output file for each job's output: short.output This file can have useful messages that describe how the job progressed. an error file for each job's errors: short.error If the job encountered any errors, they will likely be in this file. In this case, we will read the output file, which should contain the information printed by our script. It should look something like this: $ cat short.output Start time: Mon Aug 10 20:18:56 UTC 2023 Job is running on node: osg-84086-0-cmswn2030.fnal.gov Job running as user: uid=12740(osg) gid=9652(osg) groups=9652(osg) Job is running in directory: /srv Working hard... Science complete! Job 2: Using Inputs and Arguments in a Job \u00b6 Sometimes it's useful to pass arguments to your executable from your submit file. For example, you might want to use the same job script for more than one run, varying only the parameters. You can do that by adding Arguments to your submission file. First, let's edit our existing short.sh script to accept arguments. To avoid losing our original script, we make a copy of the file under the name short_transfer.sh if you haven't already downloaded this entire tutorial . $ cp short.sh short_transfer.sh Now, edit the file to include the added lines below or use cat to view the existing short_transfer.sh file: #!/bin/bash # short_transfer.sh: a short discovery job printf \"Start time: \"; /bin/date printf \"Job is running on node: \"; /bin/hostname printf \"Job running as user: \"; /usr/bin/id printf \"Job is running in directory: \"; /bin/pwd printf \"The command line argument is: \"; echo $@ printf \"Job number is: \"; echo $2 printf \"Contents of $1 is \"; cat $1 cat $1 > output$2.txt echo echo \"Working hard...\" ls -l $PWD sleep 20 echo \"Science complete!\" We need to make our new script executable just as we did before: $ chmod +x short_transfer.sh Notice that with our changes, the new script will now print out the contents of whatever file we specify in our arguments, specified by the $1 . It will also copy the contents of that file into another file called output.txt . Make a simple text file called input.txt that we can pass to our script: \"Hello World\" Once again, before submitting our job we should test it locally to ensure it runs as we expect: $ ./short_transfer.sh input.txt Start time: Tue Dec 11 10:19:12 CST 2018 Job is running on node: ap50.ux.osg-htc.org Job running as user: uid=54161(alice), gid=5782(osg) groups=5782(osg),5513(osg.login-nodes),7158(osg.OSG-Staff) Job is running in directory: /home/alice/tutorial-quickstart The command line argument is: input.txt Job number is: Contents of input.txt is \"Hello World\" Working hard total 28 drwxrwxr-x 2 alice users 34 Aug 15 09:37 Images -rw-rw-r-- 1 alice users 13 Aug 15 09:37 input.txt drwxrwxr-x 2 alice users 114 Aug 11 09:50 log -rw-r--r-- 1 alice users 13 Aug 11 10:19 output.txt -rwxrwxr-x 1 alice users 291 Aug 15 09:37 short.sh -rwxrwxr-x 1 alice users 390 Aug 11 10:18 short_transfer.sh -rw-rw-r-- 1 alice users 806 Aug 15 09:37 tutorial01.submit -rw-rw-r-- 1 alice users 547 Aug 11 09:49 tutorial02.submit -rw-rw-r-- 1 alice users 1321 Aug 15 09:37 tutorial03.submit Science complete! Now, let's edit our submit file to properly handle these new arguments and output files and save this as tutorial02.submit # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input file. executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt # output files will be transferred back automatically log = job.log error = job.error output = job.output +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue one job with the above specifications. queue 1 Notice the added arguments = input.txt information. The arguments option specifies what arguments should be passed to the executable. The transfer_input_files options needs to be included as well. When jobs are executed on the Open Science Pool via HTCondor, they are sent only with files that are specified. Any new files generated by the job in the working directory will be returned to the Access Point. Submit the new submit file using condor_submit . Be sure to check your output files once the job completes. $ condor_submit tutorial02.submit Submitting job(s). 1 job(s) submitted to cluster 1444781. Run the commands from the previous section to check on the job in the queue, and view the outputs when the job completes. Job 3: Submitting Multiple Jobs at Once \u00b6 What do we need to do to submit several jobs simultaneously? In the first example, Condor returned three files: out, error, and log. If we want to submit several jobs, we need to track these three files for each job. An easy way to do this is to add the $(Cluster) and $(Process) macros to the HTCondor submit file. Since this can make our working directory really messy with a large number of jobs, let's tell HTCondor to put the files in a directory called log . We will also include the $(Process) value as a second argument to our script, which will cause it to give our output files unique names. If you want to try it out, you can do so like this: $ ./short_transfer.sh input.txt 12 Incorporating all these ideas, here's what the third submit file looks like, called tutorial03.submit : # For this example, we'll specify unique filenames for each job by using # the job's 'Process' value. executable = short_transfer.sh arguments = input.txt $(Process) transfer_input_files = input.txt log = log/job.$(Cluster).log error = log/job.$(Cluster).$(Process).error output = log/job.$(Cluster).$(Process).output +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Let's queue ten jobs with the above specifications queue 10 Before submitting, we also need to make sure the log directory exists. $ mkdir -p log You'll see something like the following upon submission: $ condor_submit tutorial03.submit Submitting job(s).......... 10 job(s) submitted to cluster 1444786. Look at the output files in the log directory and notice how each job received its own separate output file: $ ls log job.1444786.0.error job.1444786.3.error job.1444786.6.error job.1444786.9.error job.1444786.0.output job.1444786.3.output job.1444786.6.output job.1444786.9.output job.1444786.2.error job.1444786.4.error job.1444786.7.error job.1444786.log job.1444786.1.output job.1444786.4.output job.1444786.7.output job.1444786.2.error job.1444786.5.error job.1444786.8.error job.1444786.2.output job.1444786.5.output job.1444786.8.output Removing Jobs \u00b6 On occasion, jobs will need to be removed for a variety of reasons (incorrect parameters, errors in submission, etc.). In these instances, the condor_rm command can be used to remove an entire job submission or just particular jobs in a submission. The condor_rm command accepts a cluster id, a job id, or username and will remove an entire cluster of jobs, a single job, or all the jobs belonging to a given user respectively. E.g. if a job submission generates 100 jobs and is assigned a cluster id of 103, then condor_rm 103.0 will remove the first job in the cluster. Likewise, condor_rm 103 will remove all the jobs in the job submission and condor_rm [username] will remove all jobs belonging to the user. The condor_rm documenation has more details on using condor_rm including ways to remove jobs based on other constraints. Getting Your Work Running \u00b6 Now that you have some practice with running HTCondor jobs, consider reviewing our Getting Started Roadmap to see what next steps will get your own computational work running on the OSPool.","title":"Quickstart-Submit Example HTCondor Jobs"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#quickstart-submit-example-htcondor-jobs","text":"","title":"Quickstart - Submit Example HTCondor Jobs"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#login-to-osg-access-point","text":"To begin, login to your OSG Access Point.","title":"Login to OSG Access Point"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#pretyped-setup","text":"To save some typing, you can download the tutorial materials into your home directory on the access point. This is highly recommended to ensure that you don't encounter transcription errors during the tutorials $ git clone https://github.com/OSGConnect/tutorial-quickstart Now, let's start the quickstart tutorial: $ cd tutorial-quickstart","title":"Pretyped Setup"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#manual-setup","text":"Alternatively, if you want the full manual experience, create a new directory for the tutorial work: $ mkdir tutorial-quickstart $ cd tutorial-quickstart","title":"Manual Setup"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#example-jobs","text":"","title":"Example Jobs"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#job-1-a-single-discovery-job","text":"Inside the tutorial directory that you created or installed previously, let's create a test script to execute as your job. For pretyped setup, this is the short.sh file: #!/bin/bash # short.sh: a short discovery job printf \"Start time: \"; /bin/date printf \"Job is running on node: \"; /bin/hostname printf \"Job running as user: \"; /usr/bin/id printf \"Job is running in directory: \"; /bin/pwd echo echo \"Working hard...\" sleep 20 echo \"Science complete!\" Now, make the script executable. $ chmod +x short.sh","title":"Job 1: A single discovery job"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#run-the-script-locally","text":"When setting up a new job submission, it's important to test your job outside of HTCondor before submitting into the Open Science Pool. $ ./short.sh Start time: Wed Aug 08 09:21:35 CDT 2023 Job is running on node: ap50.ux.osg-htc.org Job running as user: uid=54161(alice), gid=5782(osg) groups=5782(osg),5513(osg.login-nodes),7158(osg.OSG-Staff) Job is running in directory: /home/alice/tutorial-quickstart Working hard... Science complete!","title":"Run the script locally"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#create-an-htcondor-submit-file","text":"So far, so good! Let's create a simple (if verbose) HTCondor submit file. This can be found in tutorial01.submit . # Our executable is the main program or script that we've created # to do the 'work' of a single job. executable = short.sh # We need to name the files that HTCondor should create to save the # terminal output (stdout) and error (stderr) created by our job. # Similarly, we need to name the log file where HTCondor will save # information about job execution steps. log = short.log error = short.error output = short.output # This is the default category for jobs +JobDurationCategory = \"Medium\" # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # The last line of a submit file indicates how many jobs of the above # description should be queued. We'll start with one job. queue 1","title":"Create an HTCondor submit file"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#submit-the-job","text":"Submit the job using condor_submit : $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 144121.","title":"Submit the job"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#check-the-job-status","text":"The condor_q command tells the status of currently running jobs. $ condor_q -- Schedd: ap50.ux.osg-htc.org : < 192.170.227.22:9618?... @ 08/10/23 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 1441271 08/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for alice: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended You can also get the status of a specific job cluster: $ condor_q 1441271 -- Schedd: ap50.ux.osg-htc.org : < 192.170.227.22:9618?... @ 08/10/23 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 1441271 08/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for alice: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended Note the DONE , RUN , and IDLE columns. Your job will be listed in the IDLE column if it hasn't started yet. If it's currently scheduled and running, it will appear in the RUN column. As it finishes up, it will then show in the DONE column. Once the job completes completely, it will not appear in condor_q . Let's wait for your job to finish \u2013 that is, for condor_q not to show the job in its output. A useful tool for this is condor_watch_q \u2013 it efficiently monitors the status of your jobs by monitoring their corresponding log files. Let's submit the job again, and use condor_watch_q to follow the progress of your job (the status will update at two-second intervals): $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 1441272 $ condor_watch_q ... Note : To close condor_watch_q, hold down Ctrl and press C.","title":"Check the job status"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#check-the-job-output","text":"Once your job has finished, you can look at the files that HTCondor has returned to the working directory. The names of these files were specified in our submit file. If everything was successful, it should have returned: a log file from HTCondor for the job cluster: short.log This file can tell you where a job ran, how long it ran, and what resources it used. If a job shows up as \"held\" in condor_q , this file will have a message that gives a reason why. an output file for each job's output: short.output This file can have useful messages that describe how the job progressed. an error file for each job's errors: short.error If the job encountered any errors, they will likely be in this file. In this case, we will read the output file, which should contain the information printed by our script. It should look something like this: $ cat short.output Start time: Mon Aug 10 20:18:56 UTC 2023 Job is running on node: osg-84086-0-cmswn2030.fnal.gov Job running as user: uid=12740(osg) gid=9652(osg) groups=9652(osg) Job is running in directory: /srv Working hard... Science complete!","title":"Check the job output"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#job-2-using-inputs-and-arguments-in-a-job","text":"Sometimes it's useful to pass arguments to your executable from your submit file. For example, you might want to use the same job script for more than one run, varying only the parameters. You can do that by adding Arguments to your submission file. First, let's edit our existing short.sh script to accept arguments. To avoid losing our original script, we make a copy of the file under the name short_transfer.sh if you haven't already downloaded this entire tutorial . $ cp short.sh short_transfer.sh Now, edit the file to include the added lines below or use cat to view the existing short_transfer.sh file: #!/bin/bash # short_transfer.sh: a short discovery job printf \"Start time: \"; /bin/date printf \"Job is running on node: \"; /bin/hostname printf \"Job running as user: \"; /usr/bin/id printf \"Job is running in directory: \"; /bin/pwd printf \"The command line argument is: \"; echo $@ printf \"Job number is: \"; echo $2 printf \"Contents of $1 is \"; cat $1 cat $1 > output$2.txt echo echo \"Working hard...\" ls -l $PWD sleep 20 echo \"Science complete!\" We need to make our new script executable just as we did before: $ chmod +x short_transfer.sh Notice that with our changes, the new script will now print out the contents of whatever file we specify in our arguments, specified by the $1 . It will also copy the contents of that file into another file called output.txt . Make a simple text file called input.txt that we can pass to our script: \"Hello World\" Once again, before submitting our job we should test it locally to ensure it runs as we expect: $ ./short_transfer.sh input.txt Start time: Tue Dec 11 10:19:12 CST 2018 Job is running on node: ap50.ux.osg-htc.org Job running as user: uid=54161(alice), gid=5782(osg) groups=5782(osg),5513(osg.login-nodes),7158(osg.OSG-Staff) Job is running in directory: /home/alice/tutorial-quickstart The command line argument is: input.txt Job number is: Contents of input.txt is \"Hello World\" Working hard total 28 drwxrwxr-x 2 alice users 34 Aug 15 09:37 Images -rw-rw-r-- 1 alice users 13 Aug 15 09:37 input.txt drwxrwxr-x 2 alice users 114 Aug 11 09:50 log -rw-r--r-- 1 alice users 13 Aug 11 10:19 output.txt -rwxrwxr-x 1 alice users 291 Aug 15 09:37 short.sh -rwxrwxr-x 1 alice users 390 Aug 11 10:18 short_transfer.sh -rw-rw-r-- 1 alice users 806 Aug 15 09:37 tutorial01.submit -rw-rw-r-- 1 alice users 547 Aug 11 09:49 tutorial02.submit -rw-rw-r-- 1 alice users 1321 Aug 15 09:37 tutorial03.submit Science complete! Now, let's edit our submit file to properly handle these new arguments and output files and save this as tutorial02.submit # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input file. executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt # output files will be transferred back automatically log = job.log error = job.error output = job.output +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue one job with the above specifications. queue 1 Notice the added arguments = input.txt information. The arguments option specifies what arguments should be passed to the executable. The transfer_input_files options needs to be included as well. When jobs are executed on the Open Science Pool via HTCondor, they are sent only with files that are specified. Any new files generated by the job in the working directory will be returned to the Access Point. Submit the new submit file using condor_submit . Be sure to check your output files once the job completes. $ condor_submit tutorial02.submit Submitting job(s). 1 job(s) submitted to cluster 1444781. Run the commands from the previous section to check on the job in the queue, and view the outputs when the job completes.","title":"Job 2: Using Inputs and Arguments in a Job"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#job-3-submitting-multiple-jobs-at-once","text":"What do we need to do to submit several jobs simultaneously? In the first example, Condor returned three files: out, error, and log. If we want to submit several jobs, we need to track these three files for each job. An easy way to do this is to add the $(Cluster) and $(Process) macros to the HTCondor submit file. Since this can make our working directory really messy with a large number of jobs, let's tell HTCondor to put the files in a directory called log . We will also include the $(Process) value as a second argument to our script, which will cause it to give our output files unique names. If you want to try it out, you can do so like this: $ ./short_transfer.sh input.txt 12 Incorporating all these ideas, here's what the third submit file looks like, called tutorial03.submit : # For this example, we'll specify unique filenames for each job by using # the job's 'Process' value. executable = short_transfer.sh arguments = input.txt $(Process) transfer_input_files = input.txt log = log/job.$(Cluster).log error = log/job.$(Cluster).$(Process).error output = log/job.$(Cluster).$(Process).output +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Let's queue ten jobs with the above specifications queue 10 Before submitting, we also need to make sure the log directory exists. $ mkdir -p log You'll see something like the following upon submission: $ condor_submit tutorial03.submit Submitting job(s).......... 10 job(s) submitted to cluster 1444786. Look at the output files in the log directory and notice how each job received its own separate output file: $ ls log job.1444786.0.error job.1444786.3.error job.1444786.6.error job.1444786.9.error job.1444786.0.output job.1444786.3.output job.1444786.6.output job.1444786.9.output job.1444786.2.error job.1444786.4.error job.1444786.7.error job.1444786.log job.1444786.1.output job.1444786.4.output job.1444786.7.output job.1444786.2.error job.1444786.5.error job.1444786.8.error job.1444786.2.output job.1444786.5.output job.1444786.8.output","title":"Job 3: Submitting Multiple Jobs at Once"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#removing-jobs","text":"On occasion, jobs will need to be removed for a variety of reasons (incorrect parameters, errors in submission, etc.). In these instances, the condor_rm command can be used to remove an entire job submission or just particular jobs in a submission. The condor_rm command accepts a cluster id, a job id, or username and will remove an entire cluster of jobs, a single job, or all the jobs belonging to a given user respectively. E.g. if a job submission generates 100 jobs and is assigned a cluster id of 103, then condor_rm 103.0 will remove the first job in the cluster. Likewise, condor_rm 103 will remove all the jobs in the job submission and condor_rm [username] will remove all jobs belonging to the user. The condor_rm documenation has more details on using condor_rm including ways to remove jobs based on other constraints.","title":"Removing Jobs"},{"location":"htc_workloads/submitting_workloads/tutorial-quickstart/#getting-your-work-running","text":"Now that you have some practice with running HTCondor jobs, consider reviewing our Getting Started Roadmap to see what next steps will get your own computational work running on the OSPool.","title":"Getting Your Work Running"},{"location":"htc_workloads/using_software/available-containers-list/","text":"Existing OSPool-Supported Containers \u00b6 This is list of commonly used containers in the Open Science Pool. These can be used directly in your jobs or as base images if you want to define your own. Please see the pages on Apptainer containers and Docker containers for detailed instructions on how to use containers. Base \u00b6 Debian 12 (htc/debian:12) Debian 12 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__debian__12.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/debian:12 Project Website Container Definition EL 7 (htc/centos:7) Enterprise Linux (CentOS) 7 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__centos__7.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/centos:7 Project Website Container Definition Rocky 8 (htc/rocky:8) Rocky Linux 8 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8 Project Website Container Definition Rocky 8 / CUDA 11.0.3 (htc/rocky:8-cuda-11.0.3) Rocky Linux 8 / CUDA 11.0.3 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8-cuda-11.0.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8-cuda-11.0.3 Project Website Container Definition Rocky 9 (htc/rocky:9) Rocky Linux 9 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9 Project Website Container Definition Rocky 9 / CUDA 2.6.0 (htc/rocky:9-cuda-12.6.0) Rocky Linux 9 / CUDA 12.6.0 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9-cuda-12.6.0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9-cuda-12.6.0 Project Website Container Definition Ubuntu 20.04 (htc/ubuntu:20.04) Ubuntu 20.04 (Focal) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__20.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:20.04 Project Website Container Definition Ubuntu 22.04 (htc/ubuntu:22.04) Ubuntu 22.04 (Jammy Jellyfish) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__22.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:22.04 Project Website Container Definition Ubuntu 24.04 (htc/ubuntu:24.04) Ubuntu 24.04 (Nobel Numbat) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__24.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:24.04 Project Website Container Definition AI \u00b6 Tensorflow 2.15 (htc/tensorflow:2.15) Tensorflow image from the Tensorflow project, with OSG additions OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__tensorflow__2.15.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:2.15 Project Website Container Definition scikit-learn:1.3.2 (htc/scikit-learn:1.3) scikit-learn, configured for execution on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__scikit-learn__1.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/scikit-learn:1.3 Project Website Container Definition Languages \u00b6 Julia (opensciencegrid/osgvo-julia) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.0.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.5.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.7.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.0.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.5.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.7.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Project Website Container Definition Julia (m8zeng/julia-packages) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/m8zeng__julia-packages__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/m8zeng/julia-packages:latest Project Website Container Definition Matlab Runtime (opensciencegrid/osgvo-matlab-runtime) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2018b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2021b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2022b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2021b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2022b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2023a Project Website Container Definition Matlab Runtime (htc/matlab-runtime:R2023a) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/matlab-runtime:R2023a Project Website Container Definition R (opensciencegrid/osgvo-r) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__3.5.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__4.0.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:latest Project Website Container Definition R (clkwisconsin/spacetimer) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/clkwisconsin__spacetimer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/clkwisconsin/spacetimer:latest Project Website Container Definition Project \u00b6 XENONnT (opensciencegrid/osgvo-xenon) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_0-13-1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.21 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.23 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.11 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.04.18 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.05.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.06.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.07.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.07.27 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.09.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:add_latex /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:gpu /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latex_test3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:stable /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_0-13-1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_v100 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:upgrade-boost Project Website Container Definition XENONnT (xenonnt/base-environment) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.24.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__testing.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.21 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.23 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.24 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.11 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.04.18 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.05.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.06.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.07.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.07.27 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.09.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:add_latex /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:gpu /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latex_test3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:stable /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:straxen_v100 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:testing /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:upgrade-boost Project Website Container Definition XENONnT (xenonnt/osg_dev) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__osg_dev__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Project Website Container Definition Tools \u00b6 DeepLabCut 3.0.0rc3 (htc/deeplabcut:3.0.0rc4) A software package for animal pose estimation OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__deeplabcut__3.0.0rc4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/deeplabcut:3.0.0rc4 Project Website Container Definition FreeSurfer (opensciencegrid/osgvo-freesurfer) A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.1.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.1.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest Project Website Container Definition GROMACS (opensciencegrid/osgvo-gromacs) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2018.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2020.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2018.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2020.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest Project Website Container Definition GROMACS GPU (opensciencegrid/osgvo-gromacs-gpu) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest Project Website Container Definition Gromacs 2023.4 (htc/gromacs:2023.4) Gromacs 2023.4 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2023.4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2023.4 Project Website Container Definition Gromacs 2024.2 (htc/gromacs:2024.2) Gromacs 2024.2 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2024.2.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2024.2 Project Website Container Definition Minimal (htc/minimal:0) Minimal image - used for testing OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__minimal__0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/minimal:0 Project Website Container Definition PyTorch 2.3.1 (htc/pytorch:2.3.1-cuda11.8) A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__pytorch__2.3.1-cuda11.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/pytorch:2.3.1-cuda11.8 Project Website Container Definition Quantum Espresso (opensciencegrid/osgvo-quantum-espresso) A suite for first-principles electronic-structure calculations and materials modeling OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.8 Project Website Container Definition RASPA2 (opensciencegrid/osgvo-raspa2) General purpose classical simulation package. It can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-raspa2__2.0.41.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-raspa2:2.0.41 Project Website Container Definition TensorFlow (opensciencegrid/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__2.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest Project Website Container Definition TensorFlow (rynge/tensorflow-cowsay) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/rynge__tensorflow-cowsay__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/rynge/tensorflow-cowsay:latest Project Website Container Definition TensorFlow (jiahe58/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/jiahe58__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/jiahe58/tensorflow:latest Project Website Container Definition TensorFlow GPU (opensciencegrid/tensorflow-gpu) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.2-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.3-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest Project Website Container Definition TensorFlow GPU (efajardo/astroflow) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/efajardo__astroflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/efajardo/astroflow:latest Project Website Container Definition TensorFlow GPU (ssrujanaa/catsanddogs) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/ssrujanaa__catsanddogs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/ssrujanaa/catsanddogs:latest Project Website Container Definition TensorFlow GPU (weiphy/skopt) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/weiphy__skopt__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/weiphy/skopt:latest Project Website Container Definition","title":"Containers - Predefined List"},{"location":"htc_workloads/using_software/available-containers-list/#existing-ospool-supported-containers","text":"This is list of commonly used containers in the Open Science Pool. These can be used directly in your jobs or as base images if you want to define your own. Please see the pages on Apptainer containers and Docker containers for detailed instructions on how to use containers.","title":"Existing OSPool-Supported Containers"},{"location":"htc_workloads/using_software/available-containers-list/#base","text":"Debian 12 (htc/debian:12) Debian 12 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__debian__12.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/debian:12 Project Website Container Definition EL 7 (htc/centos:7) Enterprise Linux (CentOS) 7 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__centos__7.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/centos:7 Project Website Container Definition Rocky 8 (htc/rocky:8) Rocky Linux 8 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8 Project Website Container Definition Rocky 8 / CUDA 11.0.3 (htc/rocky:8-cuda-11.0.3) Rocky Linux 8 / CUDA 11.0.3 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8-cuda-11.0.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8-cuda-11.0.3 Project Website Container Definition Rocky 9 (htc/rocky:9) Rocky Linux 9 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9 Project Website Container Definition Rocky 9 / CUDA 2.6.0 (htc/rocky:9-cuda-12.6.0) Rocky Linux 9 / CUDA 12.6.0 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9-cuda-12.6.0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9-cuda-12.6.0 Project Website Container Definition Ubuntu 20.04 (htc/ubuntu:20.04) Ubuntu 20.04 (Focal) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__20.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:20.04 Project Website Container Definition Ubuntu 22.04 (htc/ubuntu:22.04) Ubuntu 22.04 (Jammy Jellyfish) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__22.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:22.04 Project Website Container Definition Ubuntu 24.04 (htc/ubuntu:24.04) Ubuntu 24.04 (Nobel Numbat) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__24.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:24.04 Project Website Container Definition","title":"Base"},{"location":"htc_workloads/using_software/available-containers-list/#ai","text":"Tensorflow 2.15 (htc/tensorflow:2.15) Tensorflow image from the Tensorflow project, with OSG additions OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__tensorflow__2.15.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:2.15 Project Website Container Definition scikit-learn:1.3.2 (htc/scikit-learn:1.3) scikit-learn, configured for execution on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__scikit-learn__1.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/scikit-learn:1.3 Project Website Container Definition","title":"AI"},{"location":"htc_workloads/using_software/available-containers-list/#languages","text":"Julia (opensciencegrid/osgvo-julia) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.0.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.5.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.7.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.0.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.5.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.7.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Project Website Container Definition Julia (m8zeng/julia-packages) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/m8zeng__julia-packages__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/m8zeng/julia-packages:latest Project Website Container Definition Matlab Runtime (opensciencegrid/osgvo-matlab-runtime) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2018b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2021b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2022b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2021b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2022b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2023a Project Website Container Definition Matlab Runtime (htc/matlab-runtime:R2023a) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/matlab-runtime:R2023a Project Website Container Definition R (opensciencegrid/osgvo-r) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__3.5.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__4.0.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:latest Project Website Container Definition R (clkwisconsin/spacetimer) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/clkwisconsin__spacetimer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/clkwisconsin/spacetimer:latest Project Website Container Definition","title":"Languages"},{"location":"htc_workloads/using_software/available-containers-list/#project","text":"XENONnT (opensciencegrid/osgvo-xenon) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_0-13-1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.21 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.23 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.11 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.04.18 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.05.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.06.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.07.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.07.27 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.09.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:add_latex /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:gpu /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latex_test3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:stable /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_0-13-1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_v100 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:upgrade-boost Project Website Container Definition XENONnT (xenonnt/base-environment) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.24.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__testing.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.21 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.23 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.24 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.11 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.04.18 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.05.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.06.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.07.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.07.27 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.09.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:add_latex /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:gpu /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latex_test3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:stable /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:straxen_v100 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:testing /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:upgrade-boost Project Website Container Definition XENONnT (xenonnt/osg_dev) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__osg_dev__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Project Website Container Definition","title":"Project"},{"location":"htc_workloads/using_software/available-containers-list/#tools","text":"DeepLabCut 3.0.0rc3 (htc/deeplabcut:3.0.0rc4) A software package for animal pose estimation OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__deeplabcut__3.0.0rc4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/deeplabcut:3.0.0rc4 Project Website Container Definition FreeSurfer (opensciencegrid/osgvo-freesurfer) A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.1.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.1.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest Project Website Container Definition GROMACS (opensciencegrid/osgvo-gromacs) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2018.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2020.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2018.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2020.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest Project Website Container Definition GROMACS GPU (opensciencegrid/osgvo-gromacs-gpu) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest Project Website Container Definition Gromacs 2023.4 (htc/gromacs:2023.4) Gromacs 2023.4 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2023.4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2023.4 Project Website Container Definition Gromacs 2024.2 (htc/gromacs:2024.2) Gromacs 2024.2 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2024.2.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2024.2 Project Website Container Definition Minimal (htc/minimal:0) Minimal image - used for testing OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__minimal__0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/minimal:0 Project Website Container Definition PyTorch 2.3.1 (htc/pytorch:2.3.1-cuda11.8) A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__pytorch__2.3.1-cuda11.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/pytorch:2.3.1-cuda11.8 Project Website Container Definition Quantum Espresso (opensciencegrid/osgvo-quantum-espresso) A suite for first-principles electronic-structure calculations and materials modeling OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.8 Project Website Container Definition RASPA2 (opensciencegrid/osgvo-raspa2) General purpose classical simulation package. It can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-raspa2__2.0.41.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-raspa2:2.0.41 Project Website Container Definition TensorFlow (opensciencegrid/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__2.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest Project Website Container Definition TensorFlow (rynge/tensorflow-cowsay) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/rynge__tensorflow-cowsay__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/rynge/tensorflow-cowsay:latest Project Website Container Definition TensorFlow (jiahe58/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/jiahe58__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/jiahe58/tensorflow:latest Project Website Container Definition TensorFlow GPU (opensciencegrid/tensorflow-gpu) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.2-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.3-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest Project Website Container Definition TensorFlow GPU (efajardo/astroflow) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/efajardo__astroflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/efajardo/astroflow:latest Project Website Container Definition TensorFlow GPU (ssrujanaa/catsanddogs) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/ssrujanaa__catsanddogs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/ssrujanaa/catsanddogs:latest Project Website Container Definition TensorFlow GPU (weiphy/skopt) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/weiphy__skopt__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/weiphy/skopt:latest Project Website Container Definition","title":"Tools"},{"location":"htc_workloads/using_software/compiling-applications/","text":"Compiling Software \u00b6 Introduction \u00b6 Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. You have two options for using code on the OSG \u2013 transferring the code files by themselves, or putting the code files into a container. Sometimes code is already compiled and they offer a direct executable for the UNIX or Linux system. Those types of software can be directly used on the OSPool. If your software is dependent on different library functions and does not have a make or install command consider using containers. To learn more about containers please follow the instructions on our container guide. If your code is written in C or C++, and has instructions including make-this guide will help you. Moreover, this guide provides general information for compiling and using your software in the OSPool. A detailed example of a specific software compilation process is additionally available at Example Compilation Guide . What is compiling? The process of compiling converts human readable code into binary, machine readable code that will execute the steps of the program. Get software source code \u00b6 The first step to compiling your software is to locate and download the source code, being sure to select the version that you want. Source code will often be made available as a compressed tar archive which will need to be extracted for before compilation. You should also carefully review the installation instructions provided by the software developers. The installation instructions should include important information regarding various options for configuring and performing the compilation. Also carefully note any system dependencies (hardware, other software, and libraries) that are required for your software. Select the appropriate compiler and compilation options \u00b6 A compiler is a program that is used to peform source code compilation. The GNU Compiler Collection (GCC) is a common, open source collection of compilers with support for C, C++, fotran, and other languages, and includes important libraries for supporting your compilation and sometimes software execution. Your software compilation may require certain versions of a compiler which should be noted in the installation instructions or system dependencies documention. Currently the Access Points have GCC 8.5.0 as the default version, but newer versions of GCC may also be available - to learn more please contact support@osg-htc.org . Static versus dynamic linking during compilation \u00b6 Binary code often depends on additional information (i.e. instructions) from other software, known as libraries, for proper execution. The default behavior when compiling, is for the final binary to be \"dynamically linked\" to libraries that it depends on, such that when the binary is executed, it will look for these library files on the system that it is running on. Thus a copy of the appropriate library files will need to be available to your software wherever it runs. OSPool users can transfer a copy of the necessary libraries along with with their jobs to manage such dependencies if not supported by the execute node that your jobs run on. However, the option exists to \"statically link\" the library dependencies of your software. By statically linking libraries during compilation, the library code will be directly packaged with your software binary meaning the libraries will always be available to your software which your software to run on more execute nodes. To statically link libraries during compilation, use the -static flag when running gcc , use --enable-static when running a configure script, or set your LD_FLAGS environment variable to --enable-static (e.g. export LD_FLAGS=\"--enable-static\" ). Get access to libraries needed for your software \u00b6 As described above, your software may require additional software, known as libraries, for compilation and execution. For greatest portability of your software, we recommend installing the libraries needed for your software and transferring a copy of the libraries along with your subsequent jobs. When using libraries that you have installed yourself, you will likely need to add these libraries to your LIBRARY_PATH environment variable before compiling your software. There may also be additional environment variables that will need to be defined or modified for software compilation, this information should be provided in the installtion instructions of your software. For any libraries added to LIBRARY_PATH before software compilation, you'll also need to add these same libraries to your LD_LIBRARY_PATH as a step in your job's executable bash script before executing your software. Perform your compilation \u00b6 Software compilation is easiest to perform interactively, and OSPool users are welcome to compile software directly on their assigned Access Point. This will ensure that your application is built on an environment that is similar to the majority of the compute nodes on OSG. Because OSG Access Points currently use the Alma/CentOS Linux 8 operating system (which are similar to the more general Red Hat Enterprise Linux, or RHEL distribution), your software will, generally, only be compatible for execution on RHEL 9 or similar operating systems. You can use the requirements statement of your HTCondor submit file to direct your jobs to execute nodes with specific operating systems, for instance: requirements = (OSGVO_OS_STRING == \"RHEL 9\") Software installation typically includes three steps: 1.) configuration, 2.) compilation, and 3.) \"installation\" which places the compiled code in a specific location. In most cases, these steps will be achieved with the following commands: ./configure make make install Most software is written to install to a default location, however your OSG Access Point account is not authorized to write to these default system locations. Instead, you will want to create a folder for your software installation in your home directory and use an option in the configuration step that will install the software to this folder: ./configure --prefix=/home/username/path where username should be replaced with your OSG username and path replaced with the path to the directory you created for your software installation. Watch out for hardware feature detection \u00b6 Some software builds might try to optimize the software for the particular host you are building on. In general this is a good idea (optimized code will perform better), but be aware that not all execution endpoints on OSG are the same. If your software picks up hardware features such as AVX/AVX2, you might have to ensure the jobs are running on hardware with those features. For example, if your software requires AVX2: requirements = (OSGVO_OS_STRING == \"RHEL 9\") && (HAS_AVX2 == True) Please see Control Where Your Jobs Run / Job Requirements Use Your Software \u00b6 When submitting jobs, you will need to transfer a copy of your compiled software, and any dynamically-linked dependencies that you also installed. Our Introduction to Data Management on OSG guide is a good starting point for more information for selecting the appropriate methods for transferring you software. Depending on your job workflow, it may be possible to directly specify your executable binary as the executable in your HTCondor submit file. When using your software in subsequent job submissions, be sure to add additional commands to the executable bash script to define evironment variables, like for instance LD_LIBRARY_PATH , that may be needed to properly execute your software. Get Additional Assistance \u00b6 If you have questions or need assistance, please contact support@osg-htc.org .","title":"Compiling Software"},{"location":"htc_workloads/using_software/compiling-applications/#compiling-software","text":"","title":"Compiling Software"},{"location":"htc_workloads/using_software/compiling-applications/#introduction","text":"Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. You have two options for using code on the OSG \u2013 transferring the code files by themselves, or putting the code files into a container. Sometimes code is already compiled and they offer a direct executable for the UNIX or Linux system. Those types of software can be directly used on the OSPool. If your software is dependent on different library functions and does not have a make or install command consider using containers. To learn more about containers please follow the instructions on our container guide. If your code is written in C or C++, and has instructions including make-this guide will help you. Moreover, this guide provides general information for compiling and using your software in the OSPool. A detailed example of a specific software compilation process is additionally available at Example Compilation Guide . What is compiling? The process of compiling converts human readable code into binary, machine readable code that will execute the steps of the program.","title":"Introduction"},{"location":"htc_workloads/using_software/compiling-applications/#get-software-source-code","text":"The first step to compiling your software is to locate and download the source code, being sure to select the version that you want. Source code will often be made available as a compressed tar archive which will need to be extracted for before compilation. You should also carefully review the installation instructions provided by the software developers. The installation instructions should include important information regarding various options for configuring and performing the compilation. Also carefully note any system dependencies (hardware, other software, and libraries) that are required for your software.","title":"Get software source code"},{"location":"htc_workloads/using_software/compiling-applications/#select-the-appropriate-compiler-and-compilation-options","text":"A compiler is a program that is used to peform source code compilation. The GNU Compiler Collection (GCC) is a common, open source collection of compilers with support for C, C++, fotran, and other languages, and includes important libraries for supporting your compilation and sometimes software execution. Your software compilation may require certain versions of a compiler which should be noted in the installation instructions or system dependencies documention. Currently the Access Points have GCC 8.5.0 as the default version, but newer versions of GCC may also be available - to learn more please contact support@osg-htc.org .","title":"Select the appropriate compiler and compilation options"},{"location":"htc_workloads/using_software/compiling-applications/#static-versus-dynamic-linking-during-compilation","text":"Binary code often depends on additional information (i.e. instructions) from other software, known as libraries, for proper execution. The default behavior when compiling, is for the final binary to be \"dynamically linked\" to libraries that it depends on, such that when the binary is executed, it will look for these library files on the system that it is running on. Thus a copy of the appropriate library files will need to be available to your software wherever it runs. OSPool users can transfer a copy of the necessary libraries along with with their jobs to manage such dependencies if not supported by the execute node that your jobs run on. However, the option exists to \"statically link\" the library dependencies of your software. By statically linking libraries during compilation, the library code will be directly packaged with your software binary meaning the libraries will always be available to your software which your software to run on more execute nodes. To statically link libraries during compilation, use the -static flag when running gcc , use --enable-static when running a configure script, or set your LD_FLAGS environment variable to --enable-static (e.g. export LD_FLAGS=\"--enable-static\" ).","title":"Static versus dynamic linking during compilation"},{"location":"htc_workloads/using_software/compiling-applications/#get-access-to-libraries-needed-for-your-software","text":"As described above, your software may require additional software, known as libraries, for compilation and execution. For greatest portability of your software, we recommend installing the libraries needed for your software and transferring a copy of the libraries along with your subsequent jobs. When using libraries that you have installed yourself, you will likely need to add these libraries to your LIBRARY_PATH environment variable before compiling your software. There may also be additional environment variables that will need to be defined or modified for software compilation, this information should be provided in the installtion instructions of your software. For any libraries added to LIBRARY_PATH before software compilation, you'll also need to add these same libraries to your LD_LIBRARY_PATH as a step in your job's executable bash script before executing your software.","title":"Get access to libraries needed for your software"},{"location":"htc_workloads/using_software/compiling-applications/#perform-your-compilation","text":"Software compilation is easiest to perform interactively, and OSPool users are welcome to compile software directly on their assigned Access Point. This will ensure that your application is built on an environment that is similar to the majority of the compute nodes on OSG. Because OSG Access Points currently use the Alma/CentOS Linux 8 operating system (which are similar to the more general Red Hat Enterprise Linux, or RHEL distribution), your software will, generally, only be compatible for execution on RHEL 9 or similar operating systems. You can use the requirements statement of your HTCondor submit file to direct your jobs to execute nodes with specific operating systems, for instance: requirements = (OSGVO_OS_STRING == \"RHEL 9\") Software installation typically includes three steps: 1.) configuration, 2.) compilation, and 3.) \"installation\" which places the compiled code in a specific location. In most cases, these steps will be achieved with the following commands: ./configure make make install Most software is written to install to a default location, however your OSG Access Point account is not authorized to write to these default system locations. Instead, you will want to create a folder for your software installation in your home directory and use an option in the configuration step that will install the software to this folder: ./configure --prefix=/home/username/path where username should be replaced with your OSG username and path replaced with the path to the directory you created for your software installation.","title":"Perform your compilation"},{"location":"htc_workloads/using_software/compiling-applications/#watch-out-for-hardware-feature-detection","text":"Some software builds might try to optimize the software for the particular host you are building on. In general this is a good idea (optimized code will perform better), but be aware that not all execution endpoints on OSG are the same. If your software picks up hardware features such as AVX/AVX2, you might have to ensure the jobs are running on hardware with those features. For example, if your software requires AVX2: requirements = (OSGVO_OS_STRING == \"RHEL 9\") && (HAS_AVX2 == True) Please see Control Where Your Jobs Run / Job Requirements","title":"Watch out for hardware feature detection"},{"location":"htc_workloads/using_software/compiling-applications/#use-your-software","text":"When submitting jobs, you will need to transfer a copy of your compiled software, and any dynamically-linked dependencies that you also installed. Our Introduction to Data Management on OSG guide is a good starting point for more information for selecting the appropriate methods for transferring you software. Depending on your job workflow, it may be possible to directly specify your executable binary as the executable in your HTCondor submit file. When using your software in subsequent job submissions, be sure to add additional commands to the executable bash script to define evironment variables, like for instance LD_LIBRARY_PATH , that may be needed to properly execute your software.","title":"Use Your Software"},{"location":"htc_workloads/using_software/compiling-applications/#get-additional-assistance","text":"If you have questions or need assistance, please contact support@osg-htc.org .","title":"Get Additional Assistance"},{"location":"htc_workloads/using_software/containers-docker/","text":"Containers - Docker \u00b6 The OSPool is using Apptainer/Singularity to execute containers. It is recommended that if you are building your own custom container, you use the Apptainer/Singularity image defintion format . However, Docker images can also be used on the OSPool and a Docker image is sometimes the more appropriate choice. For example: There is an existing image on Docker Hub You found a Dockerfile which meets your requirements You have Docker installed on your own machine and want to develop the code/image locally before using it on the OSPool This guide contains examples on how to build your own Docker image, how to convert a Docker image to Apptainer/Singularity, and how to import a Docker image from the Docker Hub. Building Your Own Docker Image \u00b6 If you already have an existing Docker container image, skip to Preparing Docker Containers for HTCondor Jobs . Otherwise, continue reading. Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. Building \u00b6 There are two main methods for generating your own container image. Editing the Dockerfile Editing the default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs. Dockerfile \u00b6 Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu 22.04 image that would look like this: FROM hub.opensciencegrid.org/htc/ubuntu:22.04 Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update -y && \\ apt-get install -y build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast . Editing an Image Interactively \u00b6 You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu 22.04 image. $ docker pull hub.opensciencegrid.org/htc/ubuntu:22.04 We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> hub.opensciencegrid.org/htc/ubuntu:22.04 /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name. Preparing Docker Containers for HTCondor Jobs \u00b6 Once you have a Docker container image, whether created by you or found on DockerHub, you should convert it to the \"sif\" image format for the best experience on the OSpool. Convert Docker containers on Docker Hub or online \u00b6 If the Docker container you want to use is online, on a site like Docker Hub, you can log in to your Access Point and run a single command to convert it to a .sif image: $ apptainer build my-container.sif docker://owner/repository:tag Where the path at the end of the command is customized to be the container image you want to use. Convert Docker containers on your computer \u00b6 If you have built a Docker image on your own host, you can save it as a tar file and then convert it to an Apptainer/Singularity SIF image. First find the image id: $ docker image list REPOSITORY IMAGE ID awesome/science f1e7972c55bc Using the image id, save the image to a tar file: $ docker save f1e7972c55bc -o my-container.tar Transfer my-container.tar to the OSPool access point, and use Apptainer to convert it to a SIF image: $ apptainer build my-container.sif docker-archive://my-container.tar Using Containers in HTCondor Jobs \u00b6 After converting the Docker image to a sif format, you can use the image in your job as described in the Apptainer/Singularity Guide . Special Cases \u00b6 ENTRYPOINT and ENV \u00b6 Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool. Apptainer/Singularity Environment \u00b6 One approach for setting up the environment for an image which will be converted to Apptainer/Singularity, is to put a file under /.singularity.d/env/ . These files will be sourced when the container get instantiated. For example, if you have Conda environment, add this to the end of your Dockerfile: # set up environment for when using the container, this is for when # we invoke the container with Apptainer/Singularity RUN mkdir -p /.singularity.d/env && \\ echo \". /opt/conda/etc/profile.d/conda.sh\" >>/.singularity.d/env/91-environment.sh && \\ echo \"conda activate\" >>/.singularity.d/env/91-environment.sh","title":"Containers - Docker"},{"location":"htc_workloads/using_software/containers-docker/#containers-docker","text":"The OSPool is using Apptainer/Singularity to execute containers. It is recommended that if you are building your own custom container, you use the Apptainer/Singularity image defintion format . However, Docker images can also be used on the OSPool and a Docker image is sometimes the more appropriate choice. For example: There is an existing image on Docker Hub You found a Dockerfile which meets your requirements You have Docker installed on your own machine and want to develop the code/image locally before using it on the OSPool This guide contains examples on how to build your own Docker image, how to convert a Docker image to Apptainer/Singularity, and how to import a Docker image from the Docker Hub.","title":"Containers - Docker"},{"location":"htc_workloads/using_software/containers-docker/#building-your-own-docker-image","text":"If you already have an existing Docker container image, skip to Preparing Docker Containers for HTCondor Jobs . Otherwise, continue reading.","title":"Building Your Own Docker Image"},{"location":"htc_workloads/using_software/containers-docker/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools.","title":"Identify Components"},{"location":"htc_workloads/using_software/containers-docker/#building","text":"There are two main methods for generating your own container image. Editing the Dockerfile Editing the default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs.","title":"Building"},{"location":"htc_workloads/using_software/containers-docker/#dockerfile","text":"Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu 22.04 image that would look like this: FROM hub.opensciencegrid.org/htc/ubuntu:22.04 Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update -y && \\ apt-get install -y build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast .","title":"Dockerfile"},{"location":"htc_workloads/using_software/containers-docker/#editing-an-image-interactively","text":"You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu 22.04 image. $ docker pull hub.opensciencegrid.org/htc/ubuntu:22.04 We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> hub.opensciencegrid.org/htc/ubuntu:22.04 /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name.","title":"Editing an Image Interactively"},{"location":"htc_workloads/using_software/containers-docker/#preparing-docker-containers-for-htcondor-jobs","text":"Once you have a Docker container image, whether created by you or found on DockerHub, you should convert it to the \"sif\" image format for the best experience on the OSpool.","title":"Preparing Docker Containers for HTCondor Jobs"},{"location":"htc_workloads/using_software/containers-docker/#convert-docker-containers-on-docker-hub-or-online","text":"If the Docker container you want to use is online, on a site like Docker Hub, you can log in to your Access Point and run a single command to convert it to a .sif image: $ apptainer build my-container.sif docker://owner/repository:tag Where the path at the end of the command is customized to be the container image you want to use.","title":"Convert Docker containers on Docker Hub or online"},{"location":"htc_workloads/using_software/containers-docker/#convert-docker-containers-on-your-computer","text":"If you have built a Docker image on your own host, you can save it as a tar file and then convert it to an Apptainer/Singularity SIF image. First find the image id: $ docker image list REPOSITORY IMAGE ID awesome/science f1e7972c55bc Using the image id, save the image to a tar file: $ docker save f1e7972c55bc -o my-container.tar Transfer my-container.tar to the OSPool access point, and use Apptainer to convert it to a SIF image: $ apptainer build my-container.sif docker-archive://my-container.tar","title":"Convert Docker containers on your computer"},{"location":"htc_workloads/using_software/containers-docker/#using-containers-in-htcondor-jobs","text":"After converting the Docker image to a sif format, you can use the image in your job as described in the Apptainer/Singularity Guide .","title":"Using Containers in HTCondor Jobs"},{"location":"htc_workloads/using_software/containers-docker/#special-cases","text":"","title":"Special Cases"},{"location":"htc_workloads/using_software/containers-docker/#entrypoint-and-env","text":"Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool.","title":"ENTRYPOINT and ENV"},{"location":"htc_workloads/using_software/containers-docker/#apptainersingularity-environment","text":"One approach for setting up the environment for an image which will be converted to Apptainer/Singularity, is to put a file under /.singularity.d/env/ . These files will be sourced when the container get instantiated. For example, if you have Conda environment, add this to the end of your Dockerfile: # set up environment for when using the container, this is for when # we invoke the container with Apptainer/Singularity RUN mkdir -p /.singularity.d/env && \\ echo \". /opt/conda/etc/profile.d/conda.sh\" >>/.singularity.d/env/91-environment.sh && \\ echo \"conda activate\" >>/.singularity.d/env/91-environment.sh","title":"Apptainer/Singularity Environment"},{"location":"htc_workloads/using_software/containers-singularity/","text":"Containers - Apptainer/Singularity \u00b6 This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool. This guide describes how to create your own Apptainer/Singularity container \"image\" (the blueprint for the container). Do You Need to Build a Container? \u00b6 If there is an existing Docker container or Apptainer/Singularity container with the software you need, you can proceed with using these options to submit a job. * See OSPool-provided containers here * Using an existing Docker container * Using an existing Apptainer/Singularity container If you can't find a good option among existing containers, you may need to build your own. See this section of the guide for more information. OSG-Provided Apptainer/Singularity Images \u00b6 The OSG Team maintains a set of images that are already in the OSG Apptainer/Singularity repository. A list of ready-to-use containers can be found on this page . If the software you need isn't already supported in a listed container, you can create your own container or use any container image in Docker Hub. How to explore these containers is shown below . Building Your Own Apptainer/Singularity Container \u00b6 Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. Apptainer/Singularity Build \u00b6 If you are building an image for the first time, the temporary cache directory of the apptainer image needs to be defined. The following commands define the cache location of the apptainer image to be built. Please run the commands in the terminal of your access point. $mkdir $HOME/tmp $export TMPDIR=$HOME/tmp $export APPTAINER_TMPDIR=$HOME/tmp $export APPTAINER_CACHEDIR=$HOME/tmp To build a custom a Apptainer/Singularity image, create a folder on your access point. Inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post # system packages apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda create -y -n \"myenv\" python=3.9 conda activate myenv conda update --all conda install -y -n \"myenv\" -c conda-forge pytorch Another good section to include is %environment . This is executed before your job and lets the container configure the environment. Example: %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate myenv See the Apptainer documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed on OSG. The final image.def looks like: Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 %post # system packages apt-get update -y apt-get install -y \\ build-essential \\ wget # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda create -y -n \"myenv\" python=3.9 conda activate myenv conda update --all conda install -y -n \"myenv\" -c conda-forge pytorch %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate myenv Once your build spec is ready, you can \"build\" the container image by running this command: $ apptainer build my-container.sif image.def Once the image is built, test it on an OSG-managed access point, and use it in your HTCondor jobs. Exploring Apptainer/Singularity Images on the Access Points \u00b6 Just like it is important to test your codes and jobs at a small scale, you should make sure that your Apptainer/Singularity container is working correctly before using it in jobs. One way to test your container image on our system is to test it on an OSG-managed access point. To do so, first log in to your assigned access point. Start an interactive session with the Apptainer/Singularity \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: apptainer shell my-container.sif If you want to test an existing container produced by OSG Staff, use the full path provided in this guide . This example will give you an interactive shell. You can explore the container and test your code with your own inputs from your /home directory, which is automatically mounted (but note - $HOME will not be available to your jobs later). Once you are down exploring, exit the container by running exit or with CTRL+D . Using Singularity or Apptainer Images in an HTCondor Job \u00b6 Once you have a \".sif\" container image file with all your needed software, you can use this file as part of an HTCondor job. Upload the Container Image to the OSDF \u00b6 The image will be resused for each job, and thus the preferred transfer method is OSDF . Store the .sif file under your personal data area on your access point (see table here ). Use the Container in an HTCondor Job \u00b6 Once the image is placed in your OSDF space, you can use an OSDF url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSPool username. Example: +SingularityImage = \"osdf:///ospool/apXX/data/USERNAME/my-custom-image-v1.sif\" <other usual submit file lines> queue Be aware that OSDF aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. Common Issues \u00b6 FATAL: kernel too old If you get a *FATAL: kernel too old* error, it means that the glibc version in the image is too new for the kernel on the host. You can work around this problem by specifying the minimum host kernel. For example, if you want to run the Ubuntu 18.04 image, specfy a minimum host kernel of 3.10.0, formatted as 31000 (major * 10000 + minor * 100 + patch): Requirements = HAS_SINGULARITY == True && OSG_HOST_KERNEL_VERSION >= 31000","title":"Containers - Apptainer/Singularity"},{"location":"htc_workloads/using_software/containers-singularity/#containers-apptainersingularity","text":"This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool. This guide describes how to create your own Apptainer/Singularity container \"image\" (the blueprint for the container).","title":"Containers - Apptainer/Singularity"},{"location":"htc_workloads/using_software/containers-singularity/#do-you-need-to-build-a-container","text":"If there is an existing Docker container or Apptainer/Singularity container with the software you need, you can proceed with using these options to submit a job. * See OSPool-provided containers here * Using an existing Docker container * Using an existing Apptainer/Singularity container If you can't find a good option among existing containers, you may need to build your own. See this section of the guide for more information.","title":"Do You Need to Build a Container?"},{"location":"htc_workloads/using_software/containers-singularity/#osg-provided-apptainersingularity-images","text":"The OSG Team maintains a set of images that are already in the OSG Apptainer/Singularity repository. A list of ready-to-use containers can be found on this page . If the software you need isn't already supported in a listed container, you can create your own container or use any container image in Docker Hub. How to explore these containers is shown below .","title":"OSG-Provided Apptainer/Singularity Images"},{"location":"htc_workloads/using_software/containers-singularity/#building-your-own-apptainersingularity-container","text":"","title":"Building Your Own Apptainer/Singularity Container"},{"location":"htc_workloads/using_software/containers-singularity/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name.","title":"Identify Components"},{"location":"htc_workloads/using_software/containers-singularity/#apptainersingularity-build","text":"If you are building an image for the first time, the temporary cache directory of the apptainer image needs to be defined. The following commands define the cache location of the apptainer image to be built. Please run the commands in the terminal of your access point. $mkdir $HOME/tmp $export TMPDIR=$HOME/tmp $export APPTAINER_TMPDIR=$HOME/tmp $export APPTAINER_CACHEDIR=$HOME/tmp To build a custom a Apptainer/Singularity image, create a folder on your access point. Inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post # system packages apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda create -y -n \"myenv\" python=3.9 conda activate myenv conda update --all conda install -y -n \"myenv\" -c conda-forge pytorch Another good section to include is %environment . This is executed before your job and lets the container configure the environment. Example: %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate myenv See the Apptainer documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed on OSG. The final image.def looks like: Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 %post # system packages apt-get update -y apt-get install -y \\ build-essential \\ wget # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda create -y -n \"myenv\" python=3.9 conda activate myenv conda update --all conda install -y -n \"myenv\" -c conda-forge pytorch %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate myenv Once your build spec is ready, you can \"build\" the container image by running this command: $ apptainer build my-container.sif image.def Once the image is built, test it on an OSG-managed access point, and use it in your HTCondor jobs.","title":"Apptainer/Singularity Build"},{"location":"htc_workloads/using_software/containers-singularity/#exploring-apptainersingularity-images-on-the-access-points","text":"Just like it is important to test your codes and jobs at a small scale, you should make sure that your Apptainer/Singularity container is working correctly before using it in jobs. One way to test your container image on our system is to test it on an OSG-managed access point. To do so, first log in to your assigned access point. Start an interactive session with the Apptainer/Singularity \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: apptainer shell my-container.sif If you want to test an existing container produced by OSG Staff, use the full path provided in this guide . This example will give you an interactive shell. You can explore the container and test your code with your own inputs from your /home directory, which is automatically mounted (but note - $HOME will not be available to your jobs later). Once you are down exploring, exit the container by running exit or with CTRL+D .","title":"Exploring Apptainer/Singularity Images on the Access Points"},{"location":"htc_workloads/using_software/containers-singularity/#using-singularity-or-apptainer-images-in-an-htcondor-job","text":"Once you have a \".sif\" container image file with all your needed software, you can use this file as part of an HTCondor job.","title":"Using Singularity or Apptainer Images in an HTCondor Job"},{"location":"htc_workloads/using_software/containers-singularity/#upload-the-container-image-to-the-osdf","text":"The image will be resused for each job, and thus the preferred transfer method is OSDF . Store the .sif file under your personal data area on your access point (see table here ).","title":"Upload the Container Image to the OSDF"},{"location":"htc_workloads/using_software/containers-singularity/#use-the-container-in-an-htcondor-job","text":"Once the image is placed in your OSDF space, you can use an OSDF url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSPool username. Example: +SingularityImage = \"osdf:///ospool/apXX/data/USERNAME/my-custom-image-v1.sif\" <other usual submit file lines> queue Be aware that OSDF aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2.","title":"Use the Container in an HTCondor Job"},{"location":"htc_workloads/using_software/containers-singularity/#common-issues","text":"FATAL: kernel too old If you get a *FATAL: kernel too old* error, it means that the glibc version in the image is too new for the kernel on the host. You can work around this problem by specifying the minimum host kernel. For example, if you want to run the Ubuntu 18.04 image, specfy a minimum host kernel of 3.10.0, formatted as 31000 (major * 10000 + minor * 100 + patch): Requirements = HAS_SINGULARITY == True && OSG_HOST_KERNEL_VERSION >= 31000","title":"Common Issues"},{"location":"htc_workloads/using_software/example-compilation/","text":"Example of Compiling Software For Use on the OSPool \u00b6 Introduction \u00b6 This guide provides a detailed example of compiling software for use from an OSG Access Point. For this example, we will be compiling Samtools which is a very common bioinformatics software for working with aligned sequencing data. We hope that this specific example helps illustrate the general compilation steps that can be applied to many other software compilations. For a general introduction to software compilation, please see our Compiling Software guide . Two Examples \u00b6 This guide provides two examples of compiling Samtools, one without CRAM file support and one with CRAM file support . Why two examples? Currently, to install Samtools with CRAM support requires additional dependencies (aka libraries) that will also need to be installed and most Samtools users are only working with BAM files which does not require CRAM support. Do I need CRAM support for my work? CRAM is an alternative compressed sequence alignment file format to BAM. Learn more at https://www.sanger.ac.uk/tool/cram/ . Compile Samtools Without CRAM Support \u00b6 Step 1. Acquire Samtools source code \u00b6 Samtools source code is available at http://www.htslib.org/download/ . The development code is also available via GitHub at https://github.com/samtools/samtools . On the download page is some important information to make note of: \"[Samtools] uses HTSlib internally [and] these source packages contain their own copies of htslib\" What this means is 1.) HTSlib is a dependency of Samtools and 2.) the HTSlib source code is included with the Samtools source code. Either download the Samtools source code to your computer and upload to your login node, or right-click on the Samtools source code link and copy the link location. Login in to your OSG Access Point and use wget to download the source code directly and extract the tarball: [user@apXX ~]$ wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2 [user@apXX ~]$ tar -xjf samtools-1.10.tar.bz2 The above two commands will create a directory named samtools-1.10 which contains all the code and instructions needed for compiling Samtools and HTSlib. Take a moment to look at the content available in this new directory. Step 2. Read through installation instructions \u00b6 What steps need to be performed for our compilation? What system dependencies exist for our software? Answers to these questions, and other important information, should be available in the installation instructions for your software which will be available online and/or included in the source code. The HTSlib website where the Samtools source code is hosted provides basic installation instructions and refers users to INSTALL (which is a plain text file that can be found in samtools-1.10/ ) for more information. You will also see a README file in the source code directory which will provide important information. README files will always be included with your source code and we recommend reviewing before compiling software. There is also a README and INSTALL file available for HTSlib in the source code directory samtools-1.10/htslib-1.10/ . cd to samtools-1.10 and read through README and INSTALL . As described in INSTALL , the Samtools installation will follow the common configure , make , make install process: Basic Installation ================== To build and install Samtools, 'cd' to the samtools-1.x directory containing the package's source and type the following commands: ./configure make make install The './configure' command checks your build environment and allows various optional functionality to be enabled (see Configuration below). Also described in INSTALL are a number of required and optional system dependencies for installing Samtools and HTSlib (which is itself a dependency of Samtools): System Requirements =================== Samtools and HTSlib depend on the following libraries: Samtools: zlib <http://zlib.net> curses or GNU ncurses (optional, for the 'tview' command) <http://www.gnu.org/software/ncurses/> HTSlib: zlib <http://zlib.net> libbz2 <http://bzip.org/> liblzma <http://tukaani.org/xz/> libcurl <https://curl.haxx.se/> (optional but strongly recommended, for network access) libcrypto <https://www.openssl.org/> (optional, for Amazon S3 support; not needed on MacOS) ... The bzip2 and liblzma dependencies can be removed if full CRAM support is not needed - see HTSlib's INSTALL file for details. Some dependencies are needed to support certain features from Samtools (such as tview and CRAM compression). You will not need tview as this is intended for interactive work which is not currently supported from the OSG Access Points. For this specific compilation example, we will disable both tview and CRAM support - see below for our compilation example that will provide CRAM file support. Following the suggestion in the Samtools INSTALL file, we can view the HTSlib INSTALL file at samtools-1.10/htslib-1.10/INSTALL . Here we will find the necessary information for disabling bzip2 and liblzma dependencies: --disable-bz2 Bzip2 is an optional compression codec format for CRAM, included in HTSlib by default. It can be disabled with --disable-bz2, but be aware that not all CRAM files may be possible to decode. --disable-lzma LZMA is an optional compression codec for CRAM, included in HTSlib by default. It can be disabled with --disable-lzma, but be aware that not all CRAM files may be possible to decode. These are two flags that will need to be used when performing our installation. To determine what libraries are available on our OSG Access Point, we can look at /usr/lib and /usr/lib64 for the various Samtools library dependencies, for example: [user@apXX ~]$ ls /usr/lib* | grep libcurl [user@apXX ~]$ ls /usr/lib* | grep htslib Although we will find matches for libcurl , we will not find any htslib files meaning that HTSlib is not currently installed on the login node, nor is it currently available as a module. This means that HTSlib will also need to be compiled. Luckly, the Samtools developers have conveniently included the HTSlib source code with the Samtools source code and have made it possible to compile both Samtools and HTSlib at the same time. From the Samtools INSTALL file, is the following: By default, configure looks for an HTSlib source tree within or alongside the samtools source directory; if there are several likely candidates, you will have to choose one via this option. This mean that we don't have to do anything extra to get HTSlib installed because the Samtools installation will do it by default. When performing your compilation, if your compiler is unable to locate the necessary libraries, or if newer versions of libraries are needed, it will result in an error - this makes for an alternative method of determining whether your system has the appropriate libraries for your software and more often than not, installation by trial and error is a common approach. However, taking a little bit of time before hand and looking for library files can save you time and frustration during software compilation. Step 3. Perform Samtools compilation \u00b6 We now have all of the information needed to start our compilation of Samtools without CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a directory, called my-software , for organizing all compiled software in the home directory: [user@apXX ~]$ mkdir $HOME/my-software [user@apXX ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next we'll change to the Samtools source code directory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this step will execute the configure script and allows us to modify various details about our Samtools installation. We will be executing configure with several flags: [user@apXX samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --disable-bz2 --disable-lzma --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed, --disable-bz2 and --disable-lzma to disable lzma and bzip2 dependencies for CRAM, and --without-curses to disable tview support. Next run the final two commands: [user@apXX samtools-1.10]$ make [user@apXX samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@apXX samtools-1.10]$ cd ~ [user@apXX ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@apXX ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement. Step 4. Make our software portable \u00b6 Our subsequent job submissions on the OSPool will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@apXX ~]$ cd my-software/ [user@apXX my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@apXX my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@apXX my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Overview: Data Staging and Transfer to Jobs . To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory. Step 5. Use Samtools in our jobs \u00b6 Now that Samtools has been compiled we can submit jobs that use this software. Below is an example submit file for a job that will use Samtools with a BAM file named my-sample.bam which is <100MB in size: #samtools.sub log = samtools.$(Cluster).log error = samtools.$(Cluster)_$(Process).err output = samtools.$(Cluster)_$(Process).out executable = samtools.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, my-sample.bam should_transfer_files = YES when_to_transfer_output = ON_EXIT +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 4 and also includes an important requirements attribute which tells HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and add this software to the PATH enviroment variable: #!/bin/bash # samtools.sh # untar software tar -xzf samtools-1.10.tar.gz # modify environment variables export PATH=$_CONDOR_SCRATCH_DIR/samtools-1.10/bin:$PATH # run samtools commands ... Compile Samtools With CRAM Support \u00b6 This example includes steps to install and use a library and to use a module, which are both currently needed for compiling Samtools with CRAM support. The steps in this example assume that you have performed Step 1 and Step 2 in the above example for compiling Samtools without CRAM support. Step 2. Read through installation instructions, continued \u00b6 From both the Samtools and HTSlib INSTALL files, we know that both bzip2 and libzlma are required for CRAM support. We can check our system for these libraries: [user@apXX ~]$ ls /usr/lib* | grep libz [user@apXX ~]$ ls /usr/lib* | grep libbz2 which will reveal that both sets of libraries are available on the login. However if we were to attempt Samtools installation with CRAM support right now we would find that this results in an error when performing the configure step. If the libraries are present, why do we get this error? This error is due to differences between types of library files. For example, running ls /usr/lib* | grep libbz2 will return two matches, libbz2.so.1 and libbz2.so.1.0.6 . But running ls /usr/lib* | grep liblz will return four matches including three .so and one .a files. Our Samtools compilation specifically requires the .a type of library file for both libbz2 and liblzma and the absence of this type of library file in /usr/lib64 is why compilation will fail without additional steps. Step 3. Compile liblzma \u00b6 To compile Samtools with CRAM support requires that we first compile liblzma . Following the same approach as we did for Samtools, first we acquire a copy of the the latest liblzma source code, then review the installation instructions. From our online search we will that liblzma is availble from the XZ Utils library package. [user@apXX ~]$ wget https://tukaani.org/xz/xz-5.2.5.tar.gz [user@apXX ~]$ tar -xzf xz-5.2.5.tar.gz Then review the installation instructions and check for dependencies. Everything that is needed for the default installation of XZ utils is currently available on the login node. [user@apXX ~]$ cd xz-5.2.5/ [user@apXX xz-5.2.5]$ less INSTALL Perform the XZ Utils compilation: [user@apXX xz-5.2.5]$ mkdir $HOME/my-software/xz-5.2.5 [user@apXX xz-5.2.5]$ ./configure --prefix=$HOME/my-software/xz-5.2.5 [user@apXX xz-5.2.5]$ make [user@apXX xz-5.2.5]$ make install [user@apXX xz-5.2.5]$ ls -F $HOME/my-software/xz-5.2.5 /bin /include /lib /share Success! Lastly we need to set some environment variables so that Samtools knows where to find this library: [user@apXX xz-5.2.5]$ export PATH=$HOME/my-software/xz-5.2.5/bin:$PATH [user@apXX xz-5.2.5]$ export LIBRARY_PATH=$HOME/my-software/xz-5.2.5/lib:$LIBRARY_PATH [user@apXX xz-5.2.5]$ export LD_LIBRARY_PATH=$LIBRARY_PATH Step 4. Load bzip2 module \u00b6 After installing XZ Utils and setting our environment variable, next we will load the bzip2 module: [user@apXX xz-5.2.5]$ module load bzip2/1.0.6 Loading this module will further modify some of your environment variables so that Samtools is able to locate the bzip2 library files. Step 5. Compile Samtools \u00b6 After compiling XZ Utils (which provides liblzma ) and loading the bzip2 1.0.6 module, we are now ready to compile Samtools with CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a common directory, called my-software , for organizing all compiled software in the home directory: [user@apXX ~]$ mkdir $HOME/my-software [user@apXX ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next, we will change our directory to the Samtools source code directory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this file is a script that allows us to modify various details about our Samtools installation and we will be executing configure with a flag that disables tview : [user@apXX samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed and --without-curses to disable tview support. Next run the final two commands: [user@apXX samtools-1.10]$ make [user@apXX samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@apXX samtools-1.10]$ cd ~ [user@apXX ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@apXX ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement. Step 6. Make our software portable \u00b6 Our subsequent job submissions on the OSPool will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@apXX ~]$ cd my-software/ [user@apXX my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@apXX my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@apXX my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG . Follow the these same steps for creating a tar archive of the xz-5.2.5 library as well. To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory. Step 7. Use Samtools in our jobs \u00b6 Now that Samtools has been compiled we can submit jobs that use this software. For Samtools with CRAM we will also need to bring along a copy of XZ Utils (which includes the liblzma library) and ensure that our jobs have access to the bzip2 1.0.6 module. Below is an example submit file for a job that will use Samtools with a Fasta file genome.fa' and CRAM file named my-sample.cram` which is <100MB in size: #samtools-cram.sub log = samtools-cram.$(Cluster).log error = samtools-cram.$(Cluster)_$(Process).err output = samtools-cram.$(Cluster)_$(Process).out executable = samtools-cram.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, /home/username/my-software/xz-5.2.5.tar.gz, genome.fa, my-sample.cram should_transfer_files = YES when_to_transfer_output = ON_EXIT +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 6 as well as a copy of XZ Utils installation from Step 3 . This submit file also includes an important requirements which tell HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system/ The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and XZ Util tar archives, modify the PATH and LD_LIBRARY_PATH enviroments of our job, and load the bzip2 module: #!/bin/bash # samtools-cram.sh # untar software and libraries tar -xzf samtools-1.10.tar.gz tar -xzf xz-5.2.5.tar.gz # modify environment variables export LD_LIBRARY_PATH=$_CONDOR_SCRATCH_DIR/xz-5.2.5/lib:$LD_LIBRARY_PATH export PATH=$_CONDOR_SCRATCH_DIR/samtools-1.10/bin:$_CONDOR_SCRATCH_DIR/xz-5.2.5/bin:$PATH # load bzip2 module module load bzip2/1.0.6 # run samtools commands ...","title":"Example Software Compilation"},{"location":"htc_workloads/using_software/example-compilation/#example-of-compiling-software-for-use-on-the-ospool","text":"","title":"Example of Compiling Software For Use on the OSPool"},{"location":"htc_workloads/using_software/example-compilation/#introduction","text":"This guide provides a detailed example of compiling software for use from an OSG Access Point. For this example, we will be compiling Samtools which is a very common bioinformatics software for working with aligned sequencing data. We hope that this specific example helps illustrate the general compilation steps that can be applied to many other software compilations. For a general introduction to software compilation, please see our Compiling Software guide .","title":"Introduction"},{"location":"htc_workloads/using_software/example-compilation/#two-examples","text":"This guide provides two examples of compiling Samtools, one without CRAM file support and one with CRAM file support . Why two examples? Currently, to install Samtools with CRAM support requires additional dependencies (aka libraries) that will also need to be installed and most Samtools users are only working with BAM files which does not require CRAM support. Do I need CRAM support for my work? CRAM is an alternative compressed sequence alignment file format to BAM. Learn more at https://www.sanger.ac.uk/tool/cram/ .","title":"Two Examples"},{"location":"htc_workloads/using_software/example-compilation/#compile-samtools-without-cram-support","text":"","title":"Compile Samtools Without CRAM Support"},{"location":"htc_workloads/using_software/example-compilation/#step-1-acquire-samtools-source-code","text":"Samtools source code is available at http://www.htslib.org/download/ . The development code is also available via GitHub at https://github.com/samtools/samtools . On the download page is some important information to make note of: \"[Samtools] uses HTSlib internally [and] these source packages contain their own copies of htslib\" What this means is 1.) HTSlib is a dependency of Samtools and 2.) the HTSlib source code is included with the Samtools source code. Either download the Samtools source code to your computer and upload to your login node, or right-click on the Samtools source code link and copy the link location. Login in to your OSG Access Point and use wget to download the source code directly and extract the tarball: [user@apXX ~]$ wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2 [user@apXX ~]$ tar -xjf samtools-1.10.tar.bz2 The above two commands will create a directory named samtools-1.10 which contains all the code and instructions needed for compiling Samtools and HTSlib. Take a moment to look at the content available in this new directory.","title":"Step 1. Acquire Samtools source code"},{"location":"htc_workloads/using_software/example-compilation/#step-2-read-through-installation-instructions","text":"What steps need to be performed for our compilation? What system dependencies exist for our software? Answers to these questions, and other important information, should be available in the installation instructions for your software which will be available online and/or included in the source code. The HTSlib website where the Samtools source code is hosted provides basic installation instructions and refers users to INSTALL (which is a plain text file that can be found in samtools-1.10/ ) for more information. You will also see a README file in the source code directory which will provide important information. README files will always be included with your source code and we recommend reviewing before compiling software. There is also a README and INSTALL file available for HTSlib in the source code directory samtools-1.10/htslib-1.10/ . cd to samtools-1.10 and read through README and INSTALL . As described in INSTALL , the Samtools installation will follow the common configure , make , make install process: Basic Installation ================== To build and install Samtools, 'cd' to the samtools-1.x directory containing the package's source and type the following commands: ./configure make make install The './configure' command checks your build environment and allows various optional functionality to be enabled (see Configuration below). Also described in INSTALL are a number of required and optional system dependencies for installing Samtools and HTSlib (which is itself a dependency of Samtools): System Requirements =================== Samtools and HTSlib depend on the following libraries: Samtools: zlib <http://zlib.net> curses or GNU ncurses (optional, for the 'tview' command) <http://www.gnu.org/software/ncurses/> HTSlib: zlib <http://zlib.net> libbz2 <http://bzip.org/> liblzma <http://tukaani.org/xz/> libcurl <https://curl.haxx.se/> (optional but strongly recommended, for network access) libcrypto <https://www.openssl.org/> (optional, for Amazon S3 support; not needed on MacOS) ... The bzip2 and liblzma dependencies can be removed if full CRAM support is not needed - see HTSlib's INSTALL file for details. Some dependencies are needed to support certain features from Samtools (such as tview and CRAM compression). You will not need tview as this is intended for interactive work which is not currently supported from the OSG Access Points. For this specific compilation example, we will disable both tview and CRAM support - see below for our compilation example that will provide CRAM file support. Following the suggestion in the Samtools INSTALL file, we can view the HTSlib INSTALL file at samtools-1.10/htslib-1.10/INSTALL . Here we will find the necessary information for disabling bzip2 and liblzma dependencies: --disable-bz2 Bzip2 is an optional compression codec format for CRAM, included in HTSlib by default. It can be disabled with --disable-bz2, but be aware that not all CRAM files may be possible to decode. --disable-lzma LZMA is an optional compression codec for CRAM, included in HTSlib by default. It can be disabled with --disable-lzma, but be aware that not all CRAM files may be possible to decode. These are two flags that will need to be used when performing our installation. To determine what libraries are available on our OSG Access Point, we can look at /usr/lib and /usr/lib64 for the various Samtools library dependencies, for example: [user@apXX ~]$ ls /usr/lib* | grep libcurl [user@apXX ~]$ ls /usr/lib* | grep htslib Although we will find matches for libcurl , we will not find any htslib files meaning that HTSlib is not currently installed on the login node, nor is it currently available as a module. This means that HTSlib will also need to be compiled. Luckly, the Samtools developers have conveniently included the HTSlib source code with the Samtools source code and have made it possible to compile both Samtools and HTSlib at the same time. From the Samtools INSTALL file, is the following: By default, configure looks for an HTSlib source tree within or alongside the samtools source directory; if there are several likely candidates, you will have to choose one via this option. This mean that we don't have to do anything extra to get HTSlib installed because the Samtools installation will do it by default. When performing your compilation, if your compiler is unable to locate the necessary libraries, or if newer versions of libraries are needed, it will result in an error - this makes for an alternative method of determining whether your system has the appropriate libraries for your software and more often than not, installation by trial and error is a common approach. However, taking a little bit of time before hand and looking for library files can save you time and frustration during software compilation.","title":"Step 2. Read through installation instructions"},{"location":"htc_workloads/using_software/example-compilation/#step-3-perform-samtools-compilation","text":"We now have all of the information needed to start our compilation of Samtools without CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a directory, called my-software , for organizing all compiled software in the home directory: [user@apXX ~]$ mkdir $HOME/my-software [user@apXX ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next we'll change to the Samtools source code directory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this step will execute the configure script and allows us to modify various details about our Samtools installation. We will be executing configure with several flags: [user@apXX samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --disable-bz2 --disable-lzma --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed, --disable-bz2 and --disable-lzma to disable lzma and bzip2 dependencies for CRAM, and --without-curses to disable tview support. Next run the final two commands: [user@apXX samtools-1.10]$ make [user@apXX samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@apXX samtools-1.10]$ cd ~ [user@apXX ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@apXX ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement.","title":"Step 3. Perform Samtools compilation"},{"location":"htc_workloads/using_software/example-compilation/#step-4-make-our-software-portable","text":"Our subsequent job submissions on the OSPool will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@apXX ~]$ cd my-software/ [user@apXX my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@apXX my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@apXX my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Overview: Data Staging and Transfer to Jobs . To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory.","title":"Step 4. Make our software portable"},{"location":"htc_workloads/using_software/example-compilation/#step-5-use-samtools-in-our-jobs","text":"Now that Samtools has been compiled we can submit jobs that use this software. Below is an example submit file for a job that will use Samtools with a BAM file named my-sample.bam which is <100MB in size: #samtools.sub log = samtools.$(Cluster).log error = samtools.$(Cluster)_$(Process).err output = samtools.$(Cluster)_$(Process).out executable = samtools.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, my-sample.bam should_transfer_files = YES when_to_transfer_output = ON_EXIT +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 4 and also includes an important requirements attribute which tells HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and add this software to the PATH enviroment variable: #!/bin/bash # samtools.sh # untar software tar -xzf samtools-1.10.tar.gz # modify environment variables export PATH=$_CONDOR_SCRATCH_DIR/samtools-1.10/bin:$PATH # run samtools commands ...","title":"Step 5. Use Samtools in our jobs"},{"location":"htc_workloads/using_software/example-compilation/#compile-samtools-with-cram-support","text":"This example includes steps to install and use a library and to use a module, which are both currently needed for compiling Samtools with CRAM support. The steps in this example assume that you have performed Step 1 and Step 2 in the above example for compiling Samtools without CRAM support.","title":"Compile Samtools With CRAM Support"},{"location":"htc_workloads/using_software/example-compilation/#step-2-read-through-installation-instructions-continued","text":"From both the Samtools and HTSlib INSTALL files, we know that both bzip2 and libzlma are required for CRAM support. We can check our system for these libraries: [user@apXX ~]$ ls /usr/lib* | grep libz [user@apXX ~]$ ls /usr/lib* | grep libbz2 which will reveal that both sets of libraries are available on the login. However if we were to attempt Samtools installation with CRAM support right now we would find that this results in an error when performing the configure step. If the libraries are present, why do we get this error? This error is due to differences between types of library files. For example, running ls /usr/lib* | grep libbz2 will return two matches, libbz2.so.1 and libbz2.so.1.0.6 . But running ls /usr/lib* | grep liblz will return four matches including three .so and one .a files. Our Samtools compilation specifically requires the .a type of library file for both libbz2 and liblzma and the absence of this type of library file in /usr/lib64 is why compilation will fail without additional steps.","title":"Step 2. Read through installation instructions, continued"},{"location":"htc_workloads/using_software/example-compilation/#step-3-compile-liblzma","text":"To compile Samtools with CRAM support requires that we first compile liblzma . Following the same approach as we did for Samtools, first we acquire a copy of the the latest liblzma source code, then review the installation instructions. From our online search we will that liblzma is availble from the XZ Utils library package. [user@apXX ~]$ wget https://tukaani.org/xz/xz-5.2.5.tar.gz [user@apXX ~]$ tar -xzf xz-5.2.5.tar.gz Then review the installation instructions and check for dependencies. Everything that is needed for the default installation of XZ utils is currently available on the login node. [user@apXX ~]$ cd xz-5.2.5/ [user@apXX xz-5.2.5]$ less INSTALL Perform the XZ Utils compilation: [user@apXX xz-5.2.5]$ mkdir $HOME/my-software/xz-5.2.5 [user@apXX xz-5.2.5]$ ./configure --prefix=$HOME/my-software/xz-5.2.5 [user@apXX xz-5.2.5]$ make [user@apXX xz-5.2.5]$ make install [user@apXX xz-5.2.5]$ ls -F $HOME/my-software/xz-5.2.5 /bin /include /lib /share Success! Lastly we need to set some environment variables so that Samtools knows where to find this library: [user@apXX xz-5.2.5]$ export PATH=$HOME/my-software/xz-5.2.5/bin:$PATH [user@apXX xz-5.2.5]$ export LIBRARY_PATH=$HOME/my-software/xz-5.2.5/lib:$LIBRARY_PATH [user@apXX xz-5.2.5]$ export LD_LIBRARY_PATH=$LIBRARY_PATH","title":"Step 3. Compile liblzma"},{"location":"htc_workloads/using_software/example-compilation/#step-4-load-bzip2-module","text":"After installing XZ Utils and setting our environment variable, next we will load the bzip2 module: [user@apXX xz-5.2.5]$ module load bzip2/1.0.6 Loading this module will further modify some of your environment variables so that Samtools is able to locate the bzip2 library files.","title":"Step 4. Load bzip2 module"},{"location":"htc_workloads/using_software/example-compilation/#step-5-compile-samtools","text":"After compiling XZ Utils (which provides liblzma ) and loading the bzip2 1.0.6 module, we are now ready to compile Samtools with CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a common directory, called my-software , for organizing all compiled software in the home directory: [user@apXX ~]$ mkdir $HOME/my-software [user@apXX ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next, we will change our directory to the Samtools source code directory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this file is a script that allows us to modify various details about our Samtools installation and we will be executing configure with a flag that disables tview : [user@apXX samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed and --without-curses to disable tview support. Next run the final two commands: [user@apXX samtools-1.10]$ make [user@apXX samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@apXX samtools-1.10]$ cd ~ [user@apXX ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@apXX ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement.","title":"Step 5. Compile Samtools"},{"location":"htc_workloads/using_software/example-compilation/#step-6-make-our-software-portable","text":"Our subsequent job submissions on the OSPool will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@apXX ~]$ cd my-software/ [user@apXX my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@apXX my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@apXX my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG . Follow the these same steps for creating a tar archive of the xz-5.2.5 library as well. To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory.","title":"Step 6. Make our software portable"},{"location":"htc_workloads/using_software/example-compilation/#step-7-use-samtools-in-our-jobs","text":"Now that Samtools has been compiled we can submit jobs that use this software. For Samtools with CRAM we will also need to bring along a copy of XZ Utils (which includes the liblzma library) and ensure that our jobs have access to the bzip2 1.0.6 module. Below is an example submit file for a job that will use Samtools with a Fasta file genome.fa' and CRAM file named my-sample.cram` which is <100MB in size: #samtools-cram.sub log = samtools-cram.$(Cluster).log error = samtools-cram.$(Cluster)_$(Process).err output = samtools-cram.$(Cluster)_$(Process).out executable = samtools-cram.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, /home/username/my-software/xz-5.2.5.tar.gz, genome.fa, my-sample.cram should_transfer_files = YES when_to_transfer_output = ON_EXIT +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 6 as well as a copy of XZ Utils installation from Step 3 . This submit file also includes an important requirements which tell HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system/ The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and XZ Util tar archives, modify the PATH and LD_LIBRARY_PATH enviroments of our job, and load the bzip2 module: #!/bin/bash # samtools-cram.sh # untar software and libraries tar -xzf samtools-1.10.tar.gz tar -xzf xz-5.2.5.tar.gz # modify environment variables export LD_LIBRARY_PATH=$_CONDOR_SCRATCH_DIR/xz-5.2.5/lib:$LD_LIBRARY_PATH export PATH=$_CONDOR_SCRATCH_DIR/samtools-1.10/bin:$_CONDOR_SCRATCH_DIR/xz-5.2.5/bin:$PATH # load bzip2 module module load bzip2/1.0.6 # run samtools commands ...","title":"Step 7. Use Samtools in our jobs"},{"location":"htc_workloads/using_software/software-overview/","text":"Using Software on the Open Science Pool \u00b6 Overview of Software Options \u00b6 There are several options available for managing the software needs of your work within the Open Science Pool (OSPool). For most cases, it will be advantageous for you to install the software needed for your jobs. This not only gives you the greatest control over your computing environment, but will also make your jobs more distributable, allowing you to run jobs at more locations. * The OSPool can support most popular, open source software that fit the distributed high throughput computing model. * We do not have or support most commercial software due to licensing issues. Here we review options, and provide links to additonal information, for using software installed by users, software available as precompiled binaries or via containers. More details and instructions on installing software from source code, precompiled binaries/prebuilt executables, and on creating and using containers can be found on the OSPool documentation website , under the \"Software\" section. Use Precompiled Binaries and Prebuilt Executables \u00b6 Some software may be available as a precompiled binary or prebuilt executable which provides a quick and easy way to run a program without the need for installation from source code. Binaries and executables are software files that are ready to run as is, however binaries should always be tested beforehand. There are several important considerations for using precompiled binaries on the OSPool: 1) only binary files compiled against a Linux operating system are suitable for use on the OSPool, 2) some softwares have system and hardware dependencies that must be met in order to run properly, and 3) the available binaries may not have been compiled with the feaures or configuration needed for your work. Install Software from Source Code \u00b6 When installing software from source code on an OSPool Access Point, your software will be specifically compiled against the Red Hat Enterprise Linux (RHEL) 9 operating system used on these nodes. In most cases, subsequent jobs that use this software will also need to run on a RHEL 9 OS, which can be specified by the requirements attribute of your HTCondor submit files as described in the guide linked above. Use Docker and Apptainer Containers \u00b6 Container systems provide users with customizable and reproducable computing and software environments. The Open Science Pool is compatible with both Apptainer and Docker containers - the latter will be converted to a Apptainer image and added to the OSG container image repository. For more information about Docker, please see: Docker Home Page and Apptainer/Singularity, please see: Apptainer Home Page Apptainer/ Singularity has become the preferred containerization method in scientific computing. This talk is an example of how containers are used in scientific computing. Users can choose from a set of pre-defined containers already available within OSG , or can use published or custom made containers. For jobs submitted to the OSPool, it does not matter whether you provide a Docker or Apptainer/Singularity image. Either is compatible with our system and can be used with little to no modification. Determining factors on when to use Apptainer/Singularity images over Docker images include if an image already exists and if you have experience building images in one for format and not the other. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables do not need to run any commands to start the container. Request Help with Installing Software \u00b6 If you believe none of the options described above are applicable for your software, send an email to support@osg-htc.org that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced We will do our best to help you create a portable installation. Additional Resources \u00b6 Watch this video from the 2021 OSG Virtual School for more information about using software on OSG:","title":"Overview: Software on the Open Science Pool"},{"location":"htc_workloads/using_software/software-overview/#using-software-on-the-open-science-pool","text":"","title":"Using Software on the Open Science Pool"},{"location":"htc_workloads/using_software/software-overview/#overview-of-software-options","text":"There are several options available for managing the software needs of your work within the Open Science Pool (OSPool). For most cases, it will be advantageous for you to install the software needed for your jobs. This not only gives you the greatest control over your computing environment, but will also make your jobs more distributable, allowing you to run jobs at more locations. * The OSPool can support most popular, open source software that fit the distributed high throughput computing model. * We do not have or support most commercial software due to licensing issues. Here we review options, and provide links to additonal information, for using software installed by users, software available as precompiled binaries or via containers. More details and instructions on installing software from source code, precompiled binaries/prebuilt executables, and on creating and using containers can be found on the OSPool documentation website , under the \"Software\" section.","title":"Overview of Software Options"},{"location":"htc_workloads/using_software/software-overview/#use-precompiled-binaries-and-prebuilt-executables","text":"Some software may be available as a precompiled binary or prebuilt executable which provides a quick and easy way to run a program without the need for installation from source code. Binaries and executables are software files that are ready to run as is, however binaries should always be tested beforehand. There are several important considerations for using precompiled binaries on the OSPool: 1) only binary files compiled against a Linux operating system are suitable for use on the OSPool, 2) some softwares have system and hardware dependencies that must be met in order to run properly, and 3) the available binaries may not have been compiled with the feaures or configuration needed for your work.","title":"Use Precompiled Binaries and Prebuilt Executables"},{"location":"htc_workloads/using_software/software-overview/#install-software-from-source-code","text":"When installing software from source code on an OSPool Access Point, your software will be specifically compiled against the Red Hat Enterprise Linux (RHEL) 9 operating system used on these nodes. In most cases, subsequent jobs that use this software will also need to run on a RHEL 9 OS, which can be specified by the requirements attribute of your HTCondor submit files as described in the guide linked above.","title":"Install Software from Source Code"},{"location":"htc_workloads/using_software/software-overview/#use-docker-and-apptainer-containers","text":"Container systems provide users with customizable and reproducable computing and software environments. The Open Science Pool is compatible with both Apptainer and Docker containers - the latter will be converted to a Apptainer image and added to the OSG container image repository. For more information about Docker, please see: Docker Home Page and Apptainer/Singularity, please see: Apptainer Home Page Apptainer/ Singularity has become the preferred containerization method in scientific computing. This talk is an example of how containers are used in scientific computing. Users can choose from a set of pre-defined containers already available within OSG , or can use published or custom made containers. For jobs submitted to the OSPool, it does not matter whether you provide a Docker or Apptainer/Singularity image. Either is compatible with our system and can be used with little to no modification. Determining factors on when to use Apptainer/Singularity images over Docker images include if an image already exists and if you have experience building images in one for format and not the other. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables do not need to run any commands to start the container.","title":"Use Docker and Apptainer Containers"},{"location":"htc_workloads/using_software/software-overview/#request-help-with-installing-software","text":"If you believe none of the options described above are applicable for your software, send an email to support@osg-htc.org that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced We will do our best to help you create a portable installation.","title":"Request Help with Installing Software"},{"location":"htc_workloads/using_software/software-overview/#additional-resources","text":"Watch this video from the 2021 OSG Virtual School for more information about using software on OSG:","title":"Additional Resources"},{"location":"htc_workloads/using_software/software-request/","text":"Request Help with Your Software \u00b6 A large number of software packages can be used by compiling a portable installation or using a container (many community sofwares are already available in authoritative containers). If you believe none of these options ( described here ) are applicable for your software, please get in touch with a simple email to [support@osg-htc.org][support] that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced As long as this code is: available to the public in source form (e.g. open source) licensed to all users, and does not require a license key would not be better supported by another approach (which are usually preferable) we should be able to help you create a portable installation with the 'right' solution.","title":"Software request"},{"location":"htc_workloads/using_software/software-request/#request-help-with-your-software","text":"A large number of software packages can be used by compiling a portable installation or using a container (many community sofwares are already available in authoritative containers). If you believe none of these options ( described here ) are applicable for your software, please get in touch with a simple email to [support@osg-htc.org][support] that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced As long as this code is: available to the public in source form (e.g. open source) licensed to all users, and does not require a license key would not be better supported by another approach (which are usually preferable) we should be able to help you create a portable installation with the 'right' solution.","title":"Request Help with Your Software"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/","text":"Overview: Submit Jobs to the OSPool using HTCondor \u00b6 Purpose \u00b6 This guide discusses the mechanics of creating and submitting jobs to the OSPool using HTCondor. OSPool Workflow Overview \u00b6 The process of running computational workflows on OSG resources follows the following outline: Terminology: Access point is where you login and stage your data, executables/scripts, and software to use in jobs. HTCondor is a job scheduling software that will run your jobs out on the OSPool execution points. All jobs must be submitted to HTCondor to run out on the OSPool. The Open Science Pool (OSPool) is the set of resources your job runs on. It is composed of execution points, as well as other technologies, that compose the cpus, memory, and disk space that will run the computations of your jobs. Run Jobs on the OSPool using HTCondor \u00b6 We are going to run the traditional 'hello world' program with a OSPool twist. In order to demonstrate the distributed resource nature of OSPool HTC System, we will produce a 'Hello CHTC' message 3 times, where each message is produced within is its own 'job'. Since you will not run execution commands yourself (HTCondor will do it for you), you need to tell HTCondor how to run the jobs for you in the form of a submit file, which describes the set of jobs. Note: You must be logged into an OSPool Access Point for the following example to work. 1. Prepare an executable \u00b6 First, create the executable script you would like HTCondor to run. For our example, copy the text below and paste it into a file called hello-ospool.sh (we recommend using a command line text editor) in your home directory. #!/bin/bash # # hello-ospool.sh # My very first OSPool job # # print a 'hello' message to the job's terminal output: echo \"Hello OSPool from Job $1 running on `whoami`@`hostname`\" # # keep this job running for a few minutes so you'll see it in the queue: sleep 180 This script would be run locally on our terminal by typing hello-ospool.sh <FirstArgument> . However, to run it on the OSPool, we will use our HTCondor submit file to run the hello-ospool.sh executable and to automatically pass different arguments to our script. 2. Prepare a submit file \u00b6 Create your HTCondor submit file, which you will use to tell HTCondor what job to run and how to run it. Copy the text below, and paste it into file called hello-ospool.sub . This is the file you will submit to HTCondor to describe your jobs (known as the submit file). # hello-ospool.sub # My very first HTCondor submit file # Specify your executable (single binary or a script that runs several # commands) and arguments to be passed to jobs. # $(Process) will be a integer number for each job, starting with \"0\" # and increasing for the relevant number of jobs. executable = hello-ospool.sh arguments = $(Process) # Specify the name of the log, standard error, and standard output (or \"screen output\") files. Wherever you see $(Cluster), HTCondor will insert the # queue number assigned to this set of jobs at the time of submission. log = hello-ospool_$(Cluster)_$(Process).log error = hello-ospool_$(Cluster)_$(Process).err output = hello-ospool_$(Cluster)_$(Process).out # This lines *would* be used if there were any other files # needed for the executable to use. # transfer_input_files = file1,/absolute/pathto/file2,etc # Specify Job duration category as \"Medium\" (expected runtime <10 hr) or \"Long\" (expected runtime <20 hr). +JobDurationCategory = \"Medium\" # Tell HTCondor requirements (e.g., operating system) your job needs, # what amount of compute resources each job will need on the computer where it runs. requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_cpus = 1 request_memory = 1GB request_disk = 5GB # Tell HTCondor to run 3 instances of our job: queue 3 By using the \"$1\" variable in our hello-ospool.shexecutable, we are telling HTCondor to fetch the value of the argument in the first position in the submit file and to insert it in location of \"$1\" in our executable file. Therefore, when HTCondor runs this executable, it will pass the $(Process) value for each job and hello-ospool.sh will insert that value for \"$1\" in hello-ospool.sh. More information on special variables like \"$1\", \"$2\", and \"$@\" can be found here . Additionally, the JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cMedium\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . We encourage users with long jobs to implement self-checkpoint when possible. Why Job Duration Categories? To maximize the value of the capacity contributed by the different organizations to the OSPool, users are requested to identify a duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool. Every job submitted from an OSG-managed access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing . 3. Submit the job \u00b6 Now, submit your job to HTCondor\u2019s queue by using the command condor_submit and providing the name of the submit file you created above: [alice@ap40]$ condor_submit hello-ospool.sub The condor_submit command actually submits your jobs to HTCondor. If all goes well, you will see output from the condor_submit command that appears as: Submitting job(s)... 3 job(s) submitted to cluster 36062145. 4. Check the job status \u00b6 To check on the status of your jobs in the queue, run the following command: [alice@ap40]$ condor_q The output of `condor_q` should look like this: -- Schedd: ap40.uw.osg-htc.org : <128.104.101.92:9618?... @ 04/14/23 15:35:17 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS Alice ID: 3606214 4/14 12:31 2 1 _ 3 36062145.0-2 3 jobs; 2 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended By default, condor_q shows jobs grouped into batches by batch name (if provided), or executable name. To show all of your jobs on individual lines, add the -nobatch option. To see a live update of the status of your jobs, use the command condor_watch_q . (To exit the live view, use the keyboard shortcut Ctrl + C .) 5. Examine the results \u00b6 When your jobs complete after a few minutes, they'll leave the queue. If you do a listing of your /home directory with the command ls -l , you should see something like: [alice@submit]$ ls -l total 28 -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_0.err -rw-r--r-- 1 alice alice 60 Apr 14 15:37 hello-ospool_36062145_0.out -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_0.log -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_1.err -rw-r--r-- 1 alice alice 60 Apr 14 15:37 hello-ospool_36062145_1.out -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_1.log -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_2.err -rw-r--r-- 1 alice alice 60 Apr 14 15:37 hello-ospool_36062145_2.out -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_2.log -rw-rw-r-- 1 alice alice 241 Apr 14 15:33 hello-ospool.sh -rw-rw-r-- 1 alice alice 1387 Apr 14 15:33 hello-ospool.sub Useful information is provided in the user log, standard error, and standard output files. HTCondor creates a transaction log of everything that happens to your jobs. Looking at the log file is very useful for debugging problems that may arise. Additionally, at the completion of a job, the .log file will print a table describing the amount of compute resources requested in the submit file compared to the amount the job actually used. An excerpt from hello-ospool_36062145_0.log produced due the submission of the 3 jobs will looks like this: \u2026 005 (36062145.000.000) 2023-04-14 12:36:09 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 72 - Run Bytes Sent By Job 265 - Run Bytes Received By Job 72 - Total Bytes Sent By Job 265 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 0 1 1 Disk (KB) : 118 1024 1810509281 Memory (MB) : 54 1024 1024 Job terminated of its own accord at 2023-04-14T17:36:09Z with exit-code 0. And, if you look at one of the output files, you should see something like this: Hello OSPool from Job 0 running on alice@e389.chtc.wisc.edu. Congratulations. You've run your first jobs in the OSPool! Important Workflow Elements \u00b6 A. Removing Jobs To remove a specific job, use condor_rm <JobID, ClusterID, Username> . Example: [alice@ap40]$ condor_rm 845638.0 B. Importance of Testing & Resource Optimization Examine Job Success Within the log file, you can see information about the completion of each job, including a system error code (as seen in \"return value 0\"). You can use this code, as well as information in your \".err\" file and other output files, to determine what issues your job(s) may have had, if any. Improve Efficiency Researchers with input and output files greater than 1GB, should store them in their /protected directory instead of /home to improve file transfer efficiency. See our data transfer guides to learn more. Get the Right Resource Requests Be sure to always add or modify the following lines in your submit files, as appropriate, and after running a few tests. Submit file entry Resources your jobs will run on request_cpus = cpus Matches each job to a computer \"slot\" with at least this many CPU cores. request_disk = kilobytes Matches each job to a slot with at least this much disk space, in units of KB. request_memory = megabytes Matches each job to a slot with at least this much memory (RAM), in units of MB. Determining Memory and Disk Requirements. The log file also indicates how much memory and disk each job used, so that you can first test a few jobs before submitting many more with more accurate request values. When you request too little, your jobs will be terminated by HTCondor and set to \"hold\" status to flag that job as requiring your attention. To learn more about why a job as gone on hold, use condor_q -hold . When you request too much, your jobs may not match to as many available \"slots\" as they could otherwise, and your overall throughput will suffer. You Have the Basics, Now Run Your OWN Jobs \u00b6 Check out the HTCondor Job Submission Intro video , which introduces various ways to specify differences between jobs (e.g. parameters, different input filenames, etc.), ways to organize your data, etc. and our full set of OSPool User Guides to begin submitting your own jobs.","title":"Overview: Submit Jobs to the OSPool using HTCondor"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#overview-submit-jobs-to-the-ospool-using-htcondor","text":"","title":"Overview: Submit Jobs to the OSPool using HTCondor"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#purpose","text":"This guide discusses the mechanics of creating and submitting jobs to the OSPool using HTCondor.","title":"Purpose"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#ospool-workflow-overview","text":"The process of running computational workflows on OSG resources follows the following outline: Terminology: Access point is where you login and stage your data, executables/scripts, and software to use in jobs. HTCondor is a job scheduling software that will run your jobs out on the OSPool execution points. All jobs must be submitted to HTCondor to run out on the OSPool. The Open Science Pool (OSPool) is the set of resources your job runs on. It is composed of execution points, as well as other technologies, that compose the cpus, memory, and disk space that will run the computations of your jobs.","title":"OSPool Workflow Overview"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#run-jobs-on-the-ospool-using-htcondor","text":"We are going to run the traditional 'hello world' program with a OSPool twist. In order to demonstrate the distributed resource nature of OSPool HTC System, we will produce a 'Hello CHTC' message 3 times, where each message is produced within is its own 'job'. Since you will not run execution commands yourself (HTCondor will do it for you), you need to tell HTCondor how to run the jobs for you in the form of a submit file, which describes the set of jobs. Note: You must be logged into an OSPool Access Point for the following example to work.","title":"Run Jobs on the OSPool using HTCondor"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#1-prepare-an-executable","text":"First, create the executable script you would like HTCondor to run. For our example, copy the text below and paste it into a file called hello-ospool.sh (we recommend using a command line text editor) in your home directory. #!/bin/bash # # hello-ospool.sh # My very first OSPool job # # print a 'hello' message to the job's terminal output: echo \"Hello OSPool from Job $1 running on `whoami`@`hostname`\" # # keep this job running for a few minutes so you'll see it in the queue: sleep 180 This script would be run locally on our terminal by typing hello-ospool.sh <FirstArgument> . However, to run it on the OSPool, we will use our HTCondor submit file to run the hello-ospool.sh executable and to automatically pass different arguments to our script.","title":"1. Prepare an executable"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#2-prepare-a-submit-file","text":"Create your HTCondor submit file, which you will use to tell HTCondor what job to run and how to run it. Copy the text below, and paste it into file called hello-ospool.sub . This is the file you will submit to HTCondor to describe your jobs (known as the submit file). # hello-ospool.sub # My very first HTCondor submit file # Specify your executable (single binary or a script that runs several # commands) and arguments to be passed to jobs. # $(Process) will be a integer number for each job, starting with \"0\" # and increasing for the relevant number of jobs. executable = hello-ospool.sh arguments = $(Process) # Specify the name of the log, standard error, and standard output (or \"screen output\") files. Wherever you see $(Cluster), HTCondor will insert the # queue number assigned to this set of jobs at the time of submission. log = hello-ospool_$(Cluster)_$(Process).log error = hello-ospool_$(Cluster)_$(Process).err output = hello-ospool_$(Cluster)_$(Process).out # This lines *would* be used if there were any other files # needed for the executable to use. # transfer_input_files = file1,/absolute/pathto/file2,etc # Specify Job duration category as \"Medium\" (expected runtime <10 hr) or \"Long\" (expected runtime <20 hr). +JobDurationCategory = \"Medium\" # Tell HTCondor requirements (e.g., operating system) your job needs, # what amount of compute resources each job will need on the computer where it runs. requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_cpus = 1 request_memory = 1GB request_disk = 5GB # Tell HTCondor to run 3 instances of our job: queue 3 By using the \"$1\" variable in our hello-ospool.shexecutable, we are telling HTCondor to fetch the value of the argument in the first position in the submit file and to insert it in location of \"$1\" in our executable file. Therefore, when HTCondor runs this executable, it will pass the $(Process) value for each job and hello-ospool.sh will insert that value for \"$1\" in hello-ospool.sh. More information on special variables like \"$1\", \"$2\", and \"$@\" can be found here . Additionally, the JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cMedium\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . We encourage users with long jobs to implement self-checkpoint when possible. Why Job Duration Categories? To maximize the value of the capacity contributed by the different organizations to the OSPool, users are requested to identify a duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool. Every job submitted from an OSG-managed access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing .","title":"2. Prepare a submit file"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#3-submit-the-job","text":"Now, submit your job to HTCondor\u2019s queue by using the command condor_submit and providing the name of the submit file you created above: [alice@ap40]$ condor_submit hello-ospool.sub The condor_submit command actually submits your jobs to HTCondor. If all goes well, you will see output from the condor_submit command that appears as: Submitting job(s)... 3 job(s) submitted to cluster 36062145.","title":"3. Submit the job"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#4-check-the-job-status","text":"To check on the status of your jobs in the queue, run the following command: [alice@ap40]$ condor_q The output of `condor_q` should look like this: -- Schedd: ap40.uw.osg-htc.org : <128.104.101.92:9618?... @ 04/14/23 15:35:17 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS Alice ID: 3606214 4/14 12:31 2 1 _ 3 36062145.0-2 3 jobs; 2 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended By default, condor_q shows jobs grouped into batches by batch name (if provided), or executable name. To show all of your jobs on individual lines, add the -nobatch option. To see a live update of the status of your jobs, use the command condor_watch_q . (To exit the live view, use the keyboard shortcut Ctrl + C .)","title":"4. Check the job status"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#5-examine-the-results","text":"When your jobs complete after a few minutes, they'll leave the queue. If you do a listing of your /home directory with the command ls -l , you should see something like: [alice@submit]$ ls -l total 28 -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_0.err -rw-r--r-- 1 alice alice 60 Apr 14 15:37 hello-ospool_36062145_0.out -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_0.log -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_1.err -rw-r--r-- 1 alice alice 60 Apr 14 15:37 hello-ospool_36062145_1.out -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_1.log -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_2.err -rw-r--r-- 1 alice alice 60 Apr 14 15:37 hello-ospool_36062145_2.out -rw-r--r-- 1 alice alice 0 Apr 14 15:37 hello-ospool_36062145_2.log -rw-rw-r-- 1 alice alice 241 Apr 14 15:33 hello-ospool.sh -rw-rw-r-- 1 alice alice 1387 Apr 14 15:33 hello-ospool.sub Useful information is provided in the user log, standard error, and standard output files. HTCondor creates a transaction log of everything that happens to your jobs. Looking at the log file is very useful for debugging problems that may arise. Additionally, at the completion of a job, the .log file will print a table describing the amount of compute resources requested in the submit file compared to the amount the job actually used. An excerpt from hello-ospool_36062145_0.log produced due the submission of the 3 jobs will looks like this: \u2026 005 (36062145.000.000) 2023-04-14 12:36:09 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 72 - Run Bytes Sent By Job 265 - Run Bytes Received By Job 72 - Total Bytes Sent By Job 265 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 0 1 1 Disk (KB) : 118 1024 1810509281 Memory (MB) : 54 1024 1024 Job terminated of its own accord at 2023-04-14T17:36:09Z with exit-code 0. And, if you look at one of the output files, you should see something like this: Hello OSPool from Job 0 running on alice@e389.chtc.wisc.edu. Congratulations. You've run your first jobs in the OSPool!","title":"5. Examine the results"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#important-workflow-elements","text":"A. Removing Jobs To remove a specific job, use condor_rm <JobID, ClusterID, Username> . Example: [alice@ap40]$ condor_rm 845638.0 B. Importance of Testing & Resource Optimization Examine Job Success Within the log file, you can see information about the completion of each job, including a system error code (as seen in \"return value 0\"). You can use this code, as well as information in your \".err\" file and other output files, to determine what issues your job(s) may have had, if any. Improve Efficiency Researchers with input and output files greater than 1GB, should store them in their /protected directory instead of /home to improve file transfer efficiency. See our data transfer guides to learn more. Get the Right Resource Requests Be sure to always add or modify the following lines in your submit files, as appropriate, and after running a few tests. Submit file entry Resources your jobs will run on request_cpus = cpus Matches each job to a computer \"slot\" with at least this many CPU cores. request_disk = kilobytes Matches each job to a slot with at least this much disk space, in units of KB. request_memory = megabytes Matches each job to a slot with at least this much memory (RAM), in units of MB. Determining Memory and Disk Requirements. The log file also indicates how much memory and disk each job used, so that you can first test a few jobs before submitting many more with more accurate request values. When you request too little, your jobs will be terminated by HTCondor and set to \"hold\" status to flag that job as requiring your attention. To learn more about why a job as gone on hold, use condor_q -hold . When you request too much, your jobs may not match to as many available \"slots\" as they could otherwise, and your overall throughput will suffer.","title":"Important Workflow Elements"},{"location":"htc_workloads/workload_planning/htcondor_job_submission/#you-have-the-basics-now-run-your-own-jobs","text":"Check out the HTCondor Job Submission Intro video , which introduces various ways to specify differences between jobs (e.g. parameters, different input filenames, etc.), ways to organize your data, etc. and our full set of OSPool User Guides to begin submitting your own jobs.","title":"You Have the Basics, Now Run Your OWN Jobs"},{"location":"htc_workloads/workload_planning/jobdurationcategory/","text":"Indicate the Duration Category of Your Jobs \u00b6 Why Job Duration Categories? \u00b6 To maximize the value of the capacity contributed by the different organizations to the Open Science Pool (OSPool), users are requested to identify one of three duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool, honoring the community\u2019s shared responsibility for efficient use of the contributed resources. As a reminder, jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing (see further below). Every job submitted from an OSG-managed access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Specify a Job Duration Category \u00b6 The JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cLong\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . If your jobs are self-checkpointing, see \u201cSelf-Checkpointing Jobs\u201d, further below. Test Jobs for Expected Duration \u00b6 As part of the preparation for running a full-scale job batch , users should test a subset (first ~10, then 100 or 1000) of their jobs with the Medium or Long categories, and then review actual job execution durations in the job log files. If the user expects potentially significant variation in job durations within a single batch, a longer JobDurationCategory may be warranted relative to the duration of test jobs. Or, if variations in job duration may be predictable, the user may choose to submit different subsets of jobs with different Job Duration Categories. OSG Facilitators have a lot of experience with approaches for achieving shorter jobs (e.g. breaking up work into shorter, more numerous jobs; self-checkpointing; automated sequential job submissions; etc.) Get in touch, and we'll help you work through a solution!! support@osg-htc.org Maximum Allowed Duration \u00b6 Jobs in each category will be placed on hold in the queue if they run longer than their Maximum Allowed Duration (starting Tuesday, Nov 16, 2021). In that case, the user may remove and resubmit the jobs, identifying a longer category. Jobs that test as longer than 20 hours are not a good fit for the OSPool resources, and should not be submitted prior to contacting support@osg-htc.org to discuss options . The Maximum Allowed Durations are longer than the Expected Job Durations in order to accommodate CPU speed variations across OSPool computing resources, as well as other contributions to job duration that may not be apparent in smaller test batches. Similarly, Long jobs held after running longer than 40 hours represent significant wasted capacity and should never be released or resubmitted by the user without first taking steps to modify and test the jobs to run shorter. Self-Checkpointing Jobs \u00b6 Jobs that self-checkpoint at least every 10 hours are an excellent way for users to run jobs that would otherwise be longer in total execution time than the durations listed above. Jobs that complete a checkpoint at least as often as allowed for their JobDurationCategory will not be held. We are excited to help you think through and implement self-checkpointing. Get in touch via support@osg-htc.org if you have questions. :)","title":"Job Duration Categories"},{"location":"htc_workloads/workload_planning/jobdurationcategory/#indicate-the-duration-category-of-your-jobs","text":"","title":"Indicate the Duration Category of Your Jobs"},{"location":"htc_workloads/workload_planning/jobdurationcategory/#why-job-duration-categories","text":"To maximize the value of the capacity contributed by the different organizations to the Open Science Pool (OSPool), users are requested to identify one of three duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool, honoring the community\u2019s shared responsibility for efficient use of the contributed resources. As a reminder, jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing (see further below). Every job submitted from an OSG-managed access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput.","title":"Why Job Duration Categories?"},{"location":"htc_workloads/workload_planning/jobdurationcategory/#specify-a-job-duration-category","text":"The JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cLong\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . If your jobs are self-checkpointing, see \u201cSelf-Checkpointing Jobs\u201d, further below.","title":"Specify a Job Duration Category"},{"location":"htc_workloads/workload_planning/jobdurationcategory/#test-jobs-for-expected-duration","text":"As part of the preparation for running a full-scale job batch , users should test a subset (first ~10, then 100 or 1000) of their jobs with the Medium or Long categories, and then review actual job execution durations in the job log files. If the user expects potentially significant variation in job durations within a single batch, a longer JobDurationCategory may be warranted relative to the duration of test jobs. Or, if variations in job duration may be predictable, the user may choose to submit different subsets of jobs with different Job Duration Categories. OSG Facilitators have a lot of experience with approaches for achieving shorter jobs (e.g. breaking up work into shorter, more numerous jobs; self-checkpointing; automated sequential job submissions; etc.) Get in touch, and we'll help you work through a solution!! support@osg-htc.org","title":"Test Jobs for Expected Duration"},{"location":"htc_workloads/workload_planning/jobdurationcategory/#maximum-allowed-duration","text":"Jobs in each category will be placed on hold in the queue if they run longer than their Maximum Allowed Duration (starting Tuesday, Nov 16, 2021). In that case, the user may remove and resubmit the jobs, identifying a longer category. Jobs that test as longer than 20 hours are not a good fit for the OSPool resources, and should not be submitted prior to contacting support@osg-htc.org to discuss options . The Maximum Allowed Durations are longer than the Expected Job Durations in order to accommodate CPU speed variations across OSPool computing resources, as well as other contributions to job duration that may not be apparent in smaller test batches. Similarly, Long jobs held after running longer than 40 hours represent significant wasted capacity and should never be released or resubmitted by the user without first taking steps to modify and test the jobs to run shorter.","title":"Maximum Allowed Duration"},{"location":"htc_workloads/workload_planning/jobdurationcategory/#self-checkpointing-jobs","text":"Jobs that self-checkpoint at least every 10 hours are an excellent way for users to run jobs that would otherwise be longer in total execution time than the durations listed above. Jobs that complete a checkpoint at least as often as allowed for their JobDurationCategory will not be held. We are excited to help you think through and implement self-checkpointing. Get in touch via support@osg-htc.org if you have questions. :)","title":"Self-Checkpointing Jobs"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/","text":"Determining the Amount of Resources to Request in a Submit File \u00b6 Learning Objectives \u00b6 This guide discuses the following: Best practices for testing jobs and scaling up your analysis. How to determine the amount of resources (CPU, memory, disk space) to request in a submit file. Overview \u00b6 Much of HTCondor's power comes from the ability to run a large number of jobs simultaneously. To optimize your work with a high-throughput computing (HTC) approach, you will need to test and optimize the resource requests of those jobs to only request the amount of memory, disk, and cpus truly needed. This is an important practice that will maximize your throughput by optimizing the number of potential 'slots' in the OSPool that your jobs can match to, reducing the overall turnaround time for completing a whole batch. This guide will describe best practices and general tips for testing your job resource requests before scaling up to submit your full set of jobs. Additional information is also available from the following \"Introduction to High Throughput Computing with HTCondor\" 2020 OSG Virtual Pilot School lecture video: Always Start With Test Jobs \u00b6 Submitting test jobs is an important first step for optimizing the resource requests of your jobs. We always recommend submitting a few (3-10) test jobs first before scaling up. If you plan to submit thousands of jobs, you may even want to run an intermediate test of 100-1,000 jobs to catch any failures or holds that mean your jobs have additional requirements they need to specify. Some general tips for test jobs: Select smaller data sets or subsets of data for your first test jobs. Using smaller data will keep the resource needs of your jobs low which will help get test jobs to start and complete sooner, when you're just making sure that your submit file and other logistical aspects of jobs submission are as you want them. If possible, submit test jobs that will reproduce results you've gotten using another system. This approach can be used as a good \"sanity check\" as you'll be able to compare the results of the test to those previously obtained. After initial tests complete successfully, scale up to larger or full-size data sets; if your jobs span a range of input file sizes, submit tests using the smallest and largest inputs to examine the range of resources that these jobs may need. Give your test jobs and associated HTCondor log , error , output , and submit files meaningful names so you know which results refer to which tests. Requesting CPUs, Memory, and Disk Space in the HTCondor Submit File \u00b6 In the HTCondor submit file, you must explicitly request the number of CPUs (i.e. cores), and the amount of disk and memory that the job needs to complete successfully, and identify a JobDurationCategory . When you submit a job for the first time, you may not know just how much to request and that's OK. Below are some suggestions for making resource requests for initial test jobs. For requesting CPU cores start by requesting a single cpu. With single-cpu jobs, you will see your jobs start sooner. Ultimately you will be able to achieve greater throughput with single cpus jobs compared to jobs that request and use multiple cpus. Keep in mind, requesting more CPU cores for a job does not mean that your jobs will use more cpus. Rather, you want to make sure that your CPU request matches the number of cores (i.e. 'threads' or 'processes') that you expect your software to use. (Most softwares only use 1 CPU core, by default.) There is limited support for multicore work in OSG. To learn more, see our guide on Multicore Jobs Depending on how long you expect your test jobs to take on a single core, you may need to identify a non-default JobDurationCategory , or consider implementing self-checkpointing. To inform initial disk requests always look at the size of your input files. At a minimum, you need to request enough disk to support all of the input files, executable, and the output you expect, but don't forget that the standard 'error' and 'output' files you specify will capture 'terminal' output that may add up, too. If many of your input and output files are compressed (i.e. zipped or tarballs) you will need to factor that into your estimates for disk usage as these files will take up additional space once uncompressed in the job. For your initial tests it is OK to request more disk than your job may need so that the test completes successfully. The key is to adjust disk requests for subsequent jobs based on the results of these test jobs. Estimating memory requests can sometimes be tricky. If you've performed the same or similar work on another computer, consider using the amount of memory (i.e. RAM) from that computer as a starting point. For instance, most laptop computers these days will have 8 or 16 GB of memory, which is okay to start with if you know a single job will succeed on your laptop. For your initial tests it is OK to request more memory than your job may need so that the test completes successfully. The key is to adjust memory requests for subsequent jobs based on the results of these test jobs. If you find that memory usage will vary greatly across a batch of jobs, we can assist you with creating dynamic memory requests in your submit files. Optimize Job Resource Requests For Subsequent Jobs \u00b6 As always, reviewing the HTCondor log file from past jobs is a great way to learn about the resource needs of your jobs. Optimizing the resources requested for each job may help your job run faster and achieve more throughput. HTCondor will report the memory, disk, and cpu usage of your jobs at the end of the HTCondor .log file. The amount of each resource requested in the submit file is listed under the \"Request\" column and information about the amount of each resource actually utilized to complete the job is provided in the \"Usage\" column. For example: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 12 1000000 26703078 Memory (MB) : 0 1000 1000 One quick option to query your log files is to use the Unix tool grep . For example: [user@login]$ grep \"Disk (KB)\" my-job.log The above will return all lines in my-job.log that report the disk usage, request, and allocation of all jobs reported in that log file. Alternatively, condor_history can be used to query details from recently completed job submissions. HTCondor's history is continuously updating with information from new jobs, so condor_history is best performed shortly after the jobs of interest enter/leave the queue. Submit Multiple Jobs Using A Single Submit File \u00b6 Once you have a single test job that completes successfully, the next step is to submit a small batch of test jobs (e.g. 5 or 10 jobs) using a single submit file . Use this small-scale multi-job submission test to ensure that all jobs complete successfully, produce the desired output, and do not conflict with each other when submitted together. Once you are confident that the jobs will complete as desired, then scale up to submitting the entire set of jobs. Monitoring Job Status and Obtaining Run Information \u00b6 Gathering information about how, what, and where a job ran can be important for both troubleshooting and optimizing a workflow. The following commands are a great way to learn more about your jobs: Command Description condor_q Shows the queue information for your jobs. Includes information such as batch name and total jobs. condor_q <JobID> -l Prints all information related to a job including attributes and run information about a job in the queue. Output includes JobDurationCategory , ServerTime , SubmitFile , etc. Also works with condor_history . condor_q <JobID> -af <AttributeName1> <AttributeName2> Prints information about an attribute or list of attributes for a single job using the autoformat -af flag. The list of possible attributes can be found using condor_q <JobID> -l . Also works with condor_history . condor_q -constraint '<Attribute> == \"<value>\"' The -constraint flag allows users to find all jobs with a certain value for a given parameter. This flag supports searching by more than one parameter and different operators (e.g. =!= ). Also works with condor_history . condor_q -better-analyze <JobID> -pool <PoolName> Shows a list of the number of slots matching a job's requirements. For more information, see Troubleshooting Job Errors . Additional condor_q flags involved in optimizing and troubleshooting jobs include: Flag Description -nobatch Combined with condor_q , this flag will list jobs individually and not by batch. -hold Show only jobs in the \"on hold\" state and the reason for that. An action from the user is expected to solve the problem. -run Show your running jobs and related info, like how much time they have been running, where they are running, etc. -dag Organize condor_q output by DAG. More information about the commands and flags above can be found in the HTCondor manual . Avoid Exceeding Disk Quotas in /home and /protected \u00b6 To prevent errors or workflow interruption, be sure to estimate the input and output needed for all of your concurrently running jobs. By default, after your job terminates HTCondor will transfer back any new or modified files from the top-level directory where the job ran, back to your /home directory. Efficiently manage output by including steps to remove intermediate and/or unnecessary files as part of your job. Workflow Management \u00b6 To help manage complicated workflows, consider a workflow manager such as HTCondor's built-in DAGman or the HTCondor-compatible Pegasus workflow tool.","title":"Determining the Amount of Resources to Request in a Submit File "},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#determining-the-amount-of-resources-to-request-in-a-submit-file","text":"","title":"Determining the Amount of Resources to Request in a Submit File"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#learning-objectives","text":"This guide discuses the following: Best practices for testing jobs and scaling up your analysis. How to determine the amount of resources (CPU, memory, disk space) to request in a submit file.","title":"Learning Objectives"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#overview","text":"Much of HTCondor's power comes from the ability to run a large number of jobs simultaneously. To optimize your work with a high-throughput computing (HTC) approach, you will need to test and optimize the resource requests of those jobs to only request the amount of memory, disk, and cpus truly needed. This is an important practice that will maximize your throughput by optimizing the number of potential 'slots' in the OSPool that your jobs can match to, reducing the overall turnaround time for completing a whole batch. This guide will describe best practices and general tips for testing your job resource requests before scaling up to submit your full set of jobs. Additional information is also available from the following \"Introduction to High Throughput Computing with HTCondor\" 2020 OSG Virtual Pilot School lecture video:","title":"Overview"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#always-start-with-test-jobs","text":"Submitting test jobs is an important first step for optimizing the resource requests of your jobs. We always recommend submitting a few (3-10) test jobs first before scaling up. If you plan to submit thousands of jobs, you may even want to run an intermediate test of 100-1,000 jobs to catch any failures or holds that mean your jobs have additional requirements they need to specify. Some general tips for test jobs: Select smaller data sets or subsets of data for your first test jobs. Using smaller data will keep the resource needs of your jobs low which will help get test jobs to start and complete sooner, when you're just making sure that your submit file and other logistical aspects of jobs submission are as you want them. If possible, submit test jobs that will reproduce results you've gotten using another system. This approach can be used as a good \"sanity check\" as you'll be able to compare the results of the test to those previously obtained. After initial tests complete successfully, scale up to larger or full-size data sets; if your jobs span a range of input file sizes, submit tests using the smallest and largest inputs to examine the range of resources that these jobs may need. Give your test jobs and associated HTCondor log , error , output , and submit files meaningful names so you know which results refer to which tests.","title":"Always Start With Test Jobs"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#requesting-cpus-memory-and-disk-space-in-the-htcondor-submit-file","text":"In the HTCondor submit file, you must explicitly request the number of CPUs (i.e. cores), and the amount of disk and memory that the job needs to complete successfully, and identify a JobDurationCategory . When you submit a job for the first time, you may not know just how much to request and that's OK. Below are some suggestions for making resource requests for initial test jobs. For requesting CPU cores start by requesting a single cpu. With single-cpu jobs, you will see your jobs start sooner. Ultimately you will be able to achieve greater throughput with single cpus jobs compared to jobs that request and use multiple cpus. Keep in mind, requesting more CPU cores for a job does not mean that your jobs will use more cpus. Rather, you want to make sure that your CPU request matches the number of cores (i.e. 'threads' or 'processes') that you expect your software to use. (Most softwares only use 1 CPU core, by default.) There is limited support for multicore work in OSG. To learn more, see our guide on Multicore Jobs Depending on how long you expect your test jobs to take on a single core, you may need to identify a non-default JobDurationCategory , or consider implementing self-checkpointing. To inform initial disk requests always look at the size of your input files. At a minimum, you need to request enough disk to support all of the input files, executable, and the output you expect, but don't forget that the standard 'error' and 'output' files you specify will capture 'terminal' output that may add up, too. If many of your input and output files are compressed (i.e. zipped or tarballs) you will need to factor that into your estimates for disk usage as these files will take up additional space once uncompressed in the job. For your initial tests it is OK to request more disk than your job may need so that the test completes successfully. The key is to adjust disk requests for subsequent jobs based on the results of these test jobs. Estimating memory requests can sometimes be tricky. If you've performed the same or similar work on another computer, consider using the amount of memory (i.e. RAM) from that computer as a starting point. For instance, most laptop computers these days will have 8 or 16 GB of memory, which is okay to start with if you know a single job will succeed on your laptop. For your initial tests it is OK to request more memory than your job may need so that the test completes successfully. The key is to adjust memory requests for subsequent jobs based on the results of these test jobs. If you find that memory usage will vary greatly across a batch of jobs, we can assist you with creating dynamic memory requests in your submit files.","title":"Requesting CPUs, Memory, and Disk Space in the HTCondor Submit File"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#optimize-job-resource-requests-for-subsequent-jobs","text":"As always, reviewing the HTCondor log file from past jobs is a great way to learn about the resource needs of your jobs. Optimizing the resources requested for each job may help your job run faster and achieve more throughput. HTCondor will report the memory, disk, and cpu usage of your jobs at the end of the HTCondor .log file. The amount of each resource requested in the submit file is listed under the \"Request\" column and information about the amount of each resource actually utilized to complete the job is provided in the \"Usage\" column. For example: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 12 1000000 26703078 Memory (MB) : 0 1000 1000 One quick option to query your log files is to use the Unix tool grep . For example: [user@login]$ grep \"Disk (KB)\" my-job.log The above will return all lines in my-job.log that report the disk usage, request, and allocation of all jobs reported in that log file. Alternatively, condor_history can be used to query details from recently completed job submissions. HTCondor's history is continuously updating with information from new jobs, so condor_history is best performed shortly after the jobs of interest enter/leave the queue.","title":"Optimize Job Resource Requests For Subsequent Jobs"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#submit-multiple-jobs-using-a-single-submit-file","text":"Once you have a single test job that completes successfully, the next step is to submit a small batch of test jobs (e.g. 5 or 10 jobs) using a single submit file . Use this small-scale multi-job submission test to ensure that all jobs complete successfully, produce the desired output, and do not conflict with each other when submitted together. Once you are confident that the jobs will complete as desired, then scale up to submitting the entire set of jobs.","title":"Submit Multiple Jobs Using A Single Submit File"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#monitoring-job-status-and-obtaining-run-information","text":"Gathering information about how, what, and where a job ran can be important for both troubleshooting and optimizing a workflow. The following commands are a great way to learn more about your jobs: Command Description condor_q Shows the queue information for your jobs. Includes information such as batch name and total jobs. condor_q <JobID> -l Prints all information related to a job including attributes and run information about a job in the queue. Output includes JobDurationCategory , ServerTime , SubmitFile , etc. Also works with condor_history . condor_q <JobID> -af <AttributeName1> <AttributeName2> Prints information about an attribute or list of attributes for a single job using the autoformat -af flag. The list of possible attributes can be found using condor_q <JobID> -l . Also works with condor_history . condor_q -constraint '<Attribute> == \"<value>\"' The -constraint flag allows users to find all jobs with a certain value for a given parameter. This flag supports searching by more than one parameter and different operators (e.g. =!= ). Also works with condor_history . condor_q -better-analyze <JobID> -pool <PoolName> Shows a list of the number of slots matching a job's requirements. For more information, see Troubleshooting Job Errors . Additional condor_q flags involved in optimizing and troubleshooting jobs include: Flag Description -nobatch Combined with condor_q , this flag will list jobs individually and not by batch. -hold Show only jobs in the \"on hold\" state and the reason for that. An action from the user is expected to solve the problem. -run Show your running jobs and related info, like how much time they have been running, where they are running, etc. -dag Organize condor_q output by DAG. More information about the commands and flags above can be found in the HTCondor manual .","title":"Monitoring Job Status and Obtaining Run Information"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#avoid-exceeding-disk-quotas-in-home-and-protected","text":"To prevent errors or workflow interruption, be sure to estimate the input and output needed for all of your concurrently running jobs. By default, after your job terminates HTCondor will transfer back any new or modified files from the top-level directory where the job ran, back to your /home directory. Efficiently manage output by including steps to remove intermediate and/or unnecessary files as part of your job.","title":"Avoid Exceeding Disk Quotas in /home and /protected"},{"location":"htc_workloads/workload_planning/preparing-to-scale-up/#workflow-management","text":"To help manage complicated workflows, consider a workflow manager such as HTCondor's built-in DAGman or the HTCondor-compatible Pegasus workflow tool.","title":"Workflow Management"},{"location":"htc_workloads/workload_planning/roadmap/","text":"Roadmap to HTC Workload Submission \u00b6 Overview \u00b6 This guide lays out the steps needed to go from logging in to an OSPool Access Point to running a full scale high throughput computing (HTC) workload on the Open Science Pool (OSPool) . The steps listed here apply to any new workload submission, whether you are a long-time OSPool user or just getting started with your first workload, with helpful links to our documentation pages. This guide assumes that you have applied for an OSPool Access Point account and have been approved after meeting with a Research Computing Facilitator. If you don't yet have an account, you can apply for one here or contact us with any questions you have. Learning how to get started on the OSG does not need to end with this document or our guides! Learn about our training opportunities and personal facilitation support in the Getting Help section below. 1. Introduction to the OSPool and OSG Resources \u00b6 The Open Science Pool is best-suited for computing work that can be run as many, independent tasks, in an approach called \"high throughput computing.\" For more information on what kind of work is a good fit for the OSPool, see Is the Open Science Pool for You? . Learn more about the services provided by the OSG in this video: 2. Log on to an OSPool Access Point \u00b6 If you have not done so, apply for an account here . A Research Computing Facilitator will contact you within one business day to arrange a meeting to discuss your computational goals and to activate your account. Note that there are multiple classes of access points provided. When your account was activated, you should have been told which access point your account belongs to: Log In to \"uw.osg-htc.org\" Access Points (e.g., ap40.uw.osg-htc.org) If your account is on the uw.osg-htc.org Access Points (e.g., accounts on ap40.uw.osg-htc.org), follow instructions in this guide for logging in: Log In to uw.osg-htc.org Access Points Log In to \"OSG Connect\" Access Points (e.g., ap20.uc.osg-htc.org) If your account is on the OSG Connect Access points (e.g., accounts on ap20.uc.osg-htc.org, ap21.uc.osg-htc.org), follow instructions in this guide for logging in: Log In to OSG Connect Access Points 3. Learn to Submit HTCondor Jobs \u00b6 Computational work is run on the OSPool by submitting \u201cjobs\u201d to the HTCondor scheduler. Jobs submitted to HTCondor are then scheduled and run on different resources that are part of the Open Science Pool. Before submitting your own computational work, it is important to understand how HTCondor job submission works. The following guide outlines how job submission works, and it includes a tutorial for submitting and monitoring a basic job. Overview: Submit Jobs to the OSPool using HTCondor 4. Test a First Job \u00b6 After learning about the basics of HTCondor job submission, you will need to generate your own HTCondor job \u2014 including the software needed by the job and the appropriate mechanism to handle the data. We recommend starting a single test job. Prepare your software \u00b6 Software is an integral part of your HTC workflow. Whether you\u2019ve written it yourself, inherited it from your research group, or use common open-source packages, any required executables and libraries will need to be made available to your jobs if they are to run on the OSPool. We recommend most users to use containers to prepare their software, as containers ensure a consistent software environment across all machines. Users should read through Using Software on the Open Science Pool to help you determine the best way to provide your software. We also have the following guides/tutorials for each major software portability approach: To use Apptainer/Singularity/Docker containers for your jobs (recommended for most users), see the Create an Apptainer/Singularity Container Image To install your own software , begin with the guide on Compiling Software and then complete the Example Software Compilation tutorial . To use precompiled binaries , try the example presented in the AutoDock Vina tutorial and/or the Julia tutorial . Finally, here are some additional guides specific to some of the most common scripting languages and software tools used on OSPool: Python R Tensorflow BLAST Manage your data \u00b6 The data for your jobs will need to be transferred to each job that runs in the OSPool, and HTCondor has built-in features for getting data to jobs. Our Data Management guide discussed the relevant approaches, when to use them, and where to stage data for each. Assign the Appropriate Job Duration Category \u00b6 Jobs running in the OSPool may be interrupted at any time and will be re-run by HTCondor, unless a single execution of a job exceeds the allowed duration. Jobs expected to take longer than 10 hours will need to identify themselves as 'Long' according to our Job Duration policies . Note that jobs expected to take longer than 20 hours are not a good fit for the OSPool (see Is the Open Science Pool for You? ) without implementing self-checkpointing (see Special Use Cases ). 5. Scale Up \u00b6 After you have a sample job running successfully, you should scale up in one or two steps (first run a subset consisting of several jobs, before running ALL of them). HTCondor has many useful features that make it easy to submit multiple jobs with the same submit file. Easily submit multiple jobs Scaling up after success with test jobs discusses how to test your jobs for duration, memory and disk usage, and the total amount of space your jobs will need. 6. Special Use Cases \u00b6 If you think any of the below applies to you, please get in touch and our facilitation team will be happy to discuss your individual case. Run sequential workflows of jobs: Workflows with HTCondor's DAGMan Implement self-checkpointing for long jobs: HTCondor Checkpointing Guide Submit more than 10,000 jobs at once: FAQ Larger or speciality resource requests: GPUs: GPU Jobs Multiple CPUs: Multicore Jobs Large Memory: Large Memory Jobs Getting Help \u00b6 The OSG Facilitation team is here to help with questions and issues that come up as you work through these roadmap steps. We are available via email, office hours, appointments, and offer regular training opportunities! Our purpose is to assist you with achieving your computational goals, so we want to hear from you! Get Help OSPool Training","title":"Roadmap to HTC Workload Submission"},{"location":"htc_workloads/workload_planning/roadmap/#roadmap-to-htc-workload-submission","text":"","title":"Roadmap to HTC Workload Submission"},{"location":"htc_workloads/workload_planning/roadmap/#overview","text":"This guide lays out the steps needed to go from logging in to an OSPool Access Point to running a full scale high throughput computing (HTC) workload on the Open Science Pool (OSPool) . The steps listed here apply to any new workload submission, whether you are a long-time OSPool user or just getting started with your first workload, with helpful links to our documentation pages. This guide assumes that you have applied for an OSPool Access Point account and have been approved after meeting with a Research Computing Facilitator. If you don't yet have an account, you can apply for one here or contact us with any questions you have. Learning how to get started on the OSG does not need to end with this document or our guides! Learn about our training opportunities and personal facilitation support in the Getting Help section below.","title":"Overview"},{"location":"htc_workloads/workload_planning/roadmap/#1-introduction-to-the-ospool-and-osg-resources","text":"The Open Science Pool is best-suited for computing work that can be run as many, independent tasks, in an approach called \"high throughput computing.\" For more information on what kind of work is a good fit for the OSPool, see Is the Open Science Pool for You? . Learn more about the services provided by the OSG in this video:","title":"1. Introduction to the OSPool and OSG Resources"},{"location":"htc_workloads/workload_planning/roadmap/#2-log-on-to-an-ospool-access-point","text":"If you have not done so, apply for an account here . A Research Computing Facilitator will contact you within one business day to arrange a meeting to discuss your computational goals and to activate your account. Note that there are multiple classes of access points provided. When your account was activated, you should have been told which access point your account belongs to: Log In to \"uw.osg-htc.org\" Access Points (e.g., ap40.uw.osg-htc.org) If your account is on the uw.osg-htc.org Access Points (e.g., accounts on ap40.uw.osg-htc.org), follow instructions in this guide for logging in: Log In to uw.osg-htc.org Access Points Log In to \"OSG Connect\" Access Points (e.g., ap20.uc.osg-htc.org) If your account is on the OSG Connect Access points (e.g., accounts on ap20.uc.osg-htc.org, ap21.uc.osg-htc.org), follow instructions in this guide for logging in: Log In to OSG Connect Access Points","title":"2. Log on to an OSPool Access Point"},{"location":"htc_workloads/workload_planning/roadmap/#3-learn-to-submit-htcondor-jobs","text":"Computational work is run on the OSPool by submitting \u201cjobs\u201d to the HTCondor scheduler. Jobs submitted to HTCondor are then scheduled and run on different resources that are part of the Open Science Pool. Before submitting your own computational work, it is important to understand how HTCondor job submission works. The following guide outlines how job submission works, and it includes a tutorial for submitting and monitoring a basic job. Overview: Submit Jobs to the OSPool using HTCondor","title":"3. Learn to Submit HTCondor Jobs"},{"location":"htc_workloads/workload_planning/roadmap/#4-test-a-first-job","text":"After learning about the basics of HTCondor job submission, you will need to generate your own HTCondor job \u2014 including the software needed by the job and the appropriate mechanism to handle the data. We recommend starting a single test job.","title":"4. Test a First Job"},{"location":"htc_workloads/workload_planning/roadmap/#prepare-your-software","text":"Software is an integral part of your HTC workflow. Whether you\u2019ve written it yourself, inherited it from your research group, or use common open-source packages, any required executables and libraries will need to be made available to your jobs if they are to run on the OSPool. We recommend most users to use containers to prepare their software, as containers ensure a consistent software environment across all machines. Users should read through Using Software on the Open Science Pool to help you determine the best way to provide your software. We also have the following guides/tutorials for each major software portability approach: To use Apptainer/Singularity/Docker containers for your jobs (recommended for most users), see the Create an Apptainer/Singularity Container Image To install your own software , begin with the guide on Compiling Software and then complete the Example Software Compilation tutorial . To use precompiled binaries , try the example presented in the AutoDock Vina tutorial and/or the Julia tutorial . Finally, here are some additional guides specific to some of the most common scripting languages and software tools used on OSPool: Python R Tensorflow BLAST","title":"Prepare your software"},{"location":"htc_workloads/workload_planning/roadmap/#manage-your-data","text":"The data for your jobs will need to be transferred to each job that runs in the OSPool, and HTCondor has built-in features for getting data to jobs. Our Data Management guide discussed the relevant approaches, when to use them, and where to stage data for each.","title":"Manage your data"},{"location":"htc_workloads/workload_planning/roadmap/#assign-the-appropriate-job-duration-category","text":"Jobs running in the OSPool may be interrupted at any time and will be re-run by HTCondor, unless a single execution of a job exceeds the allowed duration. Jobs expected to take longer than 10 hours will need to identify themselves as 'Long' according to our Job Duration policies . Note that jobs expected to take longer than 20 hours are not a good fit for the OSPool (see Is the Open Science Pool for You? ) without implementing self-checkpointing (see Special Use Cases ).","title":"Assign the Appropriate Job Duration Category"},{"location":"htc_workloads/workload_planning/roadmap/#5-scale-up","text":"After you have a sample job running successfully, you should scale up in one or two steps (first run a subset consisting of several jobs, before running ALL of them). HTCondor has many useful features that make it easy to submit multiple jobs with the same submit file. Easily submit multiple jobs Scaling up after success with test jobs discusses how to test your jobs for duration, memory and disk usage, and the total amount of space your jobs will need.","title":"5. Scale Up"},{"location":"htc_workloads/workload_planning/roadmap/#6-special-use-cases","text":"If you think any of the below applies to you, please get in touch and our facilitation team will be happy to discuss your individual case. Run sequential workflows of jobs: Workflows with HTCondor's DAGMan Implement self-checkpointing for long jobs: HTCondor Checkpointing Guide Submit more than 10,000 jobs at once: FAQ Larger or speciality resource requests: GPUs: GPU Jobs Multiple CPUs: Multicore Jobs Large Memory: Large Memory Jobs","title":"6. Special Use Cases"},{"location":"htc_workloads/workload_planning/roadmap/#getting-help","text":"The OSG Facilitation team is here to help with questions and issues that come up as you work through these roadmap steps. We are available via email, office hours, appointments, and offer regular training opportunities! Our purpose is to assist you with achieving your computational goals, so we want to hear from you! Get Help OSPool Training","title":"Getting Help"},{"location":"overview/account_setup/comanage-access/","text":"Log In to uw.osg-htc.org Access Points \u00b6 This guide is for users who were notified by a member of the OSG team that they will be using the uw.osg-htc.org Access Points. To use the uw.osg-htc.org Access Points ( ap40.uw.osg-htc.org ), you must have first registered and have your account approved as described here . Log in \u00b6 To log in, you can authenticate using one of two methods: Browser-based web authentication (requires access to web browser) SSH key pair authentication (requires uploading an SSH key) Option 1: Log in via Web Authentication \u00b6 Logging in via web authentication requires no preparatory steps beyond having access to an internet browser. To authenticate using this approach: Open a terminal and enter the following command, being sure to replace username with your uw.osg-htc.org username: ssh username@ap40.uw.osg-htc.org Upon hitting enter, the following text should appear with a unique URL, similar to the one in the example below: Authenticate at ----------------- https://cilogon.org/device/?user_code=FF4-ZX6-9LK ----------------- Type 'Enter' when you authenticate. Open your unique https:// link in your web browser. When using some terminal applications, you may be able to click on the link to open it. Otherwise, copy the link and paste it into a web browser, and hit enter. You will be redirected to a new page where you will be prompted to login using your institutional credentials. Once you have done so, a new page will appear with the following text: \"You have successfully approved the user code. Please return to your device for further instructions.\" Return to your terminal, and press the 'Enter' key to complete the login process. Option 2: Log in via SSH Key Pair Authentication \u00b6 It is also possible to authenticate using an SSH key pair, if you prefer. Logging in using SSH keys does not require access to an internet browser.. The process below describes how to upload a public key to the registration website. It assumes that a private/public key pair has already been generated. If you need to generate a key pair, see this guide . Return to the Registration Page and login using your institutional credentials, if prompted. Click your name at the top right. In the dropdown box, click \"My Profile (OSG)\" button. On the right hand side of your profile, click \"Authenticators\" link. On the authenticators page, click the \"Manage\" button. On the new SSH Keys page, click \"Add SSH Key\" and browse your computer to upload your public SSH key. You can now log in to ap40.uw.osg-htc.org from the terminal using the following command, being sure to replace username with your uw.osg-htc.org username: ssh username@ap40.uw.osg-htc.org When you run this command, you may be asked for your SSH key passphrase. Enter your corresponding passphrase and you should be logged in to ap40.uw.osg-htc.org . Known Issues \u00b6 Existing Account Error message: SORID \"http://cilogon.org/serverA/users/20186\" is already associated with EnvSource Try logging into COmanage . If you can log in, email us with your name and we will add you to the appropriate group. Privacy enhancing plugins Error message: Identifier (SORID) variable \"REDIRECT_OIDC CLAIM sub\" not set We have seen this happen when a user has various \"privacy enhancing\" plugins installed in the browser and it blocks the necessary flow from fully happening. Try a different web browser without any plugins installed. Get Help \u00b6 For questions regarding logging in or creating an account, contact us at support@osg-htc.org .","title":"Log In to uw.osg-htc.org Access Points"},{"location":"overview/account_setup/comanage-access/#log-in-to-uwosg-htcorg-access-points","text":"This guide is for users who were notified by a member of the OSG team that they will be using the uw.osg-htc.org Access Points. To use the uw.osg-htc.org Access Points ( ap40.uw.osg-htc.org ), you must have first registered and have your account approved as described here .","title":"Log In to uw.osg-htc.org Access Points"},{"location":"overview/account_setup/comanage-access/#log-in","text":"To log in, you can authenticate using one of two methods: Browser-based web authentication (requires access to web browser) SSH key pair authentication (requires uploading an SSH key)","title":"Log in"},{"location":"overview/account_setup/comanage-access/#option-1-log-in-via-web-authentication","text":"Logging in via web authentication requires no preparatory steps beyond having access to an internet browser. To authenticate using this approach: Open a terminal and enter the following command, being sure to replace username with your uw.osg-htc.org username: ssh username@ap40.uw.osg-htc.org Upon hitting enter, the following text should appear with a unique URL, similar to the one in the example below: Authenticate at ----------------- https://cilogon.org/device/?user_code=FF4-ZX6-9LK ----------------- Type 'Enter' when you authenticate. Open your unique https:// link in your web browser. When using some terminal applications, you may be able to click on the link to open it. Otherwise, copy the link and paste it into a web browser, and hit enter. You will be redirected to a new page where you will be prompted to login using your institutional credentials. Once you have done so, a new page will appear with the following text: \"You have successfully approved the user code. Please return to your device for further instructions.\" Return to your terminal, and press the 'Enter' key to complete the login process.","title":"Option 1: Log in via Web Authentication"},{"location":"overview/account_setup/comanage-access/#option-2-log-in-via-ssh-key-pair-authentication","text":"It is also possible to authenticate using an SSH key pair, if you prefer. Logging in using SSH keys does not require access to an internet browser.. The process below describes how to upload a public key to the registration website. It assumes that a private/public key pair has already been generated. If you need to generate a key pair, see this guide . Return to the Registration Page and login using your institutional credentials, if prompted. Click your name at the top right. In the dropdown box, click \"My Profile (OSG)\" button. On the right hand side of your profile, click \"Authenticators\" link. On the authenticators page, click the \"Manage\" button. On the new SSH Keys page, click \"Add SSH Key\" and browse your computer to upload your public SSH key. You can now log in to ap40.uw.osg-htc.org from the terminal using the following command, being sure to replace username with your uw.osg-htc.org username: ssh username@ap40.uw.osg-htc.org When you run this command, you may be asked for your SSH key passphrase. Enter your corresponding passphrase and you should be logged in to ap40.uw.osg-htc.org .","title":"Option 2: Log in via SSH Key Pair Authentication"},{"location":"overview/account_setup/comanage-access/#known-issues","text":"Existing Account Error message: SORID \"http://cilogon.org/serverA/users/20186\" is already associated with EnvSource Try logging into COmanage . If you can log in, email us with your name and we will add you to the appropriate group. Privacy enhancing plugins Error message: Identifier (SORID) variable \"REDIRECT_OIDC CLAIM sub\" not set We have seen this happen when a user has various \"privacy enhancing\" plugins installed in the browser and it blocks the necessary flow from fully happening. Try a different web browser without any plugins installed.","title":"Known Issues"},{"location":"overview/account_setup/comanage-access/#get-help","text":"For questions regarding logging in or creating an account, contact us at support@osg-htc.org .","title":"Get Help"},{"location":"overview/account_setup/connect-access/","text":"Log In to uc.osg-htc.org Access Points \u00b6 This guide is for users who were notified by a member of the OSG team that they will be using the \"OSG Connect\" Access Points. Do not go through the steps of this guide until advised to by a Research Computing Facilitator To use the \"OSG Connect\" Access Points ( ap20.uc.osg-htc.org , ap21.uc.osg-htc.org ), you must have first registered and have your account approved as described here . If this is your first time logging in using OSG Connect, you'll need to first set up multi factor authentication . If this is your first time logging in from your current device, you'll need to add an SSH key to your web profile . If you've already set up an SSH key on your current device, you are ready to log in ! Log In \u00b6 If you have recently set up multi factor authentication or set up an SSH key for your current device , please wait 15 minutes before trying to log in. For Mac, Linux, or newer versions of Windows \u00b6 Open a terminal and type the following command, where you replace your_osg_connect_username and your_osg_login_node with the appropriate values for your account: ssh your_osg_connect_username@your_osg_login_node It will ask for the passphrase for your ssh key (if you set one), then for a \"Verification code\" which you should get by going to the TOTP client you used to set up multi factor authentication. After entering the six digit code, you should be logged in. Note that when you are typing your passphrase and verification code, your typing will NOT appear on the terminal, but the information is being entered! Finding your account information \u00b6 Before you can connect, you will need to know your username and which login node your account is assigned to. You can find this information on your profile from the OSG Connect website. Go to https://www.osgconnect.net and sign in with your institution credentials that you used to request an account. Click \"Profile\" in the top right corner. The box on the left side contains your login information. The login node address should be listed in the top right of this box. The UNIX User Name on the left side of this box is your username. For older versions of Windows \u00b6 On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type the address of your assigned login node as the hostname (see \"Determine which login node to use\" above). In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you previously generated. Return to \"Session\". Then Name your session Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created at Step 4 in PuTTYgen) when prompted to do so. When prompted for a \"Verification Code\", go to the TOTP client you used to set up two-factor authentication, above, and enter the six digit code from the client into your PuTTY terminal prompt. The following video demonstrates the key generation and login process from Putty: Add a public SSH key to your web profile \u00b6 Log in to OSG Connect Access Points is via SSH key. To generate an SSH key pair, see this guide and then proceed with the following steps. To add your public key to the OSG Connect log in node: Go to www.osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Copy/paste the public key which is found in the .pub file into the \"SSH Public Key\" text box. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . If you used the first set of key-generating instructions it is the content of ~/.ssh/id_rsa.pub and for the second (using PuTTYgen), it is the content from step 7 above. Click \"Update Profile\" The key is now added to your profile in the OSG Connect website. This will automatically be added to the login nodes within a couple hours. Can I Use Multiple Keys? \u00b6 Yes! If you want to log into OSG Connect from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your OSG Connect profile. Add multi factor authentication to your web profile \u00b6 Multi factor authentication means that you will use 2 different methods to authenticate when you log in. The first factor is the ssh key you added above. The second factor is a 6 digit code from one of your devices. OSGConnect uses the TOTP (Time-based One-time Password) standard - any TOTP client should work. Some common clients include: FreeOTP Google Authenticator DUO TOTP clients are most commonly used from smartphones. If you do not have a smartphone or are otherwise struggling to access or use a TOTP client, please contact the facilitation team: support@osg-htc.org Once you have a TOTP client, configure it to be used with OSG Connect: Go to https://osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Check the \"Set up Multi-Factor Authentication\" at the bottom and hit Apply. In the Multi-Factor Authentication box, follow the instructions (scan the QR code with your TOTP client) Important: after setting up multi-factor authentication using your TOTP client, you will need to wait 15 minutes before logging in.","title":"Log In to OSG Connect Access Points"},{"location":"overview/account_setup/connect-access/#log-in-to-ucosg-htcorg-access-points","text":"This guide is for users who were notified by a member of the OSG team that they will be using the \"OSG Connect\" Access Points. Do not go through the steps of this guide until advised to by a Research Computing Facilitator To use the \"OSG Connect\" Access Points ( ap20.uc.osg-htc.org , ap21.uc.osg-htc.org ), you must have first registered and have your account approved as described here . If this is your first time logging in using OSG Connect, you'll need to first set up multi factor authentication . If this is your first time logging in from your current device, you'll need to add an SSH key to your web profile . If you've already set up an SSH key on your current device, you are ready to log in !","title":"Log In to uc.osg-htc.org Access Points"},{"location":"overview/account_setup/connect-access/#log-in","text":"If you have recently set up multi factor authentication or set up an SSH key for your current device , please wait 15 minutes before trying to log in.","title":"Log In"},{"location":"overview/account_setup/connect-access/#for-mac-linux-or-newer-versions-of-windows","text":"Open a terminal and type the following command, where you replace your_osg_connect_username and your_osg_login_node with the appropriate values for your account: ssh your_osg_connect_username@your_osg_login_node It will ask for the passphrase for your ssh key (if you set one), then for a \"Verification code\" which you should get by going to the TOTP client you used to set up multi factor authentication. After entering the six digit code, you should be logged in. Note that when you are typing your passphrase and verification code, your typing will NOT appear on the terminal, but the information is being entered!","title":"For Mac, Linux, or newer versions of Windows"},{"location":"overview/account_setup/connect-access/#finding-your-account-information","text":"Before you can connect, you will need to know your username and which login node your account is assigned to. You can find this information on your profile from the OSG Connect website. Go to https://www.osgconnect.net and sign in with your institution credentials that you used to request an account. Click \"Profile\" in the top right corner. The box on the left side contains your login information. The login node address should be listed in the top right of this box. The UNIX User Name on the left side of this box is your username.","title":"Finding your account information"},{"location":"overview/account_setup/connect-access/#for-older-versions-of-windows","text":"On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type the address of your assigned login node as the hostname (see \"Determine which login node to use\" above). In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you previously generated. Return to \"Session\". Then Name your session Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created at Step 4 in PuTTYgen) when prompted to do so. When prompted for a \"Verification Code\", go to the TOTP client you used to set up two-factor authentication, above, and enter the six digit code from the client into your PuTTY terminal prompt. The following video demonstrates the key generation and login process from Putty:","title":"For older versions of Windows"},{"location":"overview/account_setup/connect-access/#add-a-public-ssh-key-to-your-web-profile","text":"Log in to OSG Connect Access Points is via SSH key. To generate an SSH key pair, see this guide and then proceed with the following steps. To add your public key to the OSG Connect log in node: Go to www.osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Copy/paste the public key which is found in the .pub file into the \"SSH Public Key\" text box. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . If you used the first set of key-generating instructions it is the content of ~/.ssh/id_rsa.pub and for the second (using PuTTYgen), it is the content from step 7 above. Click \"Update Profile\" The key is now added to your profile in the OSG Connect website. This will automatically be added to the login nodes within a couple hours.","title":"Add a public SSH key to your web profile"},{"location":"overview/account_setup/connect-access/#can-i-use-multiple-keys","text":"Yes! If you want to log into OSG Connect from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your OSG Connect profile.","title":"Can I Use Multiple Keys?"},{"location":"overview/account_setup/connect-access/#add-multi-factor-authentication-to-your-web-profile","text":"Multi factor authentication means that you will use 2 different methods to authenticate when you log in. The first factor is the ssh key you added above. The second factor is a 6 digit code from one of your devices. OSGConnect uses the TOTP (Time-based One-time Password) standard - any TOTP client should work. Some common clients include: FreeOTP Google Authenticator DUO TOTP clients are most commonly used from smartphones. If you do not have a smartphone or are otherwise struggling to access or use a TOTP client, please contact the facilitation team: support@osg-htc.org Once you have a TOTP client, configure it to be used with OSG Connect: Go to https://osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Check the \"Set up Multi-Factor Authentication\" at the bottom and hit Apply. In the Multi-Factor Authentication box, follow the instructions (scan the QR code with your TOTP client) Important: after setting up multi-factor authentication using your TOTP client, you will need to wait 15 minutes before logging in.","title":"Add multi factor authentication to your web profile"},{"location":"overview/account_setup/generate-add-sshkey/","text":"Generate SSH Keys For Login \u00b6 Overview \u00b6 One way to connect to an OSG-managed Access Point is an SSH key. This guide details how to create an SSH key. Once created, it needs to be added to your web profile in order to enable log in to an Access Point. Generate SSH Keys \u00b6 We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key to the OSG Connect website or COmanage, but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop). Unix-based operating system (Linux/Mac) or latest Windows 10 versions \u00b6 We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... The part you want to upload is the content of the .pub file (~/.ssh/id_rsa.pub) The following video demonstrates the key generation process from the terminal: Windows, using Putty to log in \u00b6 If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" button to save the private key. You must save the private key. You will need it to connect to your machine. Right-click in the text field labeled \"Public key for pasting into OpenSSH authorized_keys file\" and choose Select All. Right-click again in the same text field and choose Copy. Next Steps \u00b6 After generating the key, you will need to upload it to a web profile to use it for log in. If you have an account on an uw.osg-htc.org Access Point (account created through https://registry.cilogon.org/registry/) follow the instructions here: Log In to uw.osg-htc.org Access Points If you have an account on \"OSG Connect\" Access Points (account created through https://www.osgconnect.net/), follow the instructions here: Log In to OSG Connect Access Points Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Generate SSH Keys"},{"location":"overview/account_setup/generate-add-sshkey/#generate-ssh-keys-for-login","text":"","title":"Generate SSH Keys For Login"},{"location":"overview/account_setup/generate-add-sshkey/#overview","text":"One way to connect to an OSG-managed Access Point is an SSH key. This guide details how to create an SSH key. Once created, it needs to be added to your web profile in order to enable log in to an Access Point.","title":"Overview"},{"location":"overview/account_setup/generate-add-sshkey/#generate-ssh-keys","text":"We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key to the OSG Connect website or COmanage, but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop).","title":"Generate SSH Keys"},{"location":"overview/account_setup/generate-add-sshkey/#unix-based-operating-system-linuxmac-or-latest-windows-10-versions","text":"We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... The part you want to upload is the content of the .pub file (~/.ssh/id_rsa.pub) The following video demonstrates the key generation process from the terminal:","title":"Unix-based operating system (Linux/Mac) or latest Windows 10 versions"},{"location":"overview/account_setup/generate-add-sshkey/#windows-using-putty-to-log-in","text":"If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" button to save the private key. You must save the private key. You will need it to connect to your machine. Right-click in the text field labeled \"Public key for pasting into OpenSSH authorized_keys file\" and choose Select All. Right-click again in the same text field and choose Copy.","title":"Windows, using Putty to log in"},{"location":"overview/account_setup/generate-add-sshkey/#next-steps","text":"After generating the key, you will need to upload it to a web profile to use it for log in. If you have an account on an uw.osg-htc.org Access Point (account created through https://registry.cilogon.org/registry/) follow the instructions here: Log In to uw.osg-htc.org Access Points If you have an account on \"OSG Connect\" Access Points (account created through https://www.osgconnect.net/), follow the instructions here: Log In to OSG Connect Access Points","title":"Next Steps"},{"location":"overview/account_setup/generate-add-sshkey/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"overview/account_setup/is-it-for-you/","text":"Computation on the Open Science Pool \u00b6 The OSG is a nationally-funded consortium of computing resources at more than one hundred institutional partners that, together, offer a strategic advantage for computing work that can be run as numerous short tasks that can execute independent of one another. For researchers who are not part of an organization with their own pool in the OSG, we offer the Open Science Pool (OSPool) , with dozens of campuses contributing excess computing capacity in support of open science. The OSPool is available for US-affiliated academic, government, and non-profit research projects and groups for their High Throughput Computing (HTC) workflows. Learn more about the services provided by the OSG that can support your HTC workload: Computational Fit on the OSPool \u00b6 For problems that can be run as numerous independent jobs (a high-throughput approach) and have requirements represented in the first two columns of the table below, the significant capacity of the OSPool can transform the types of questions that researchers are able to tackle. Importantly, many compute tasks that may appear to not be a good fit can be modified in simple ways to take advantage, and we'd love to discuss options with you! Ideal jobs! Still advantageous Maybe not, but get in touch! Expected Throughput: 1000s concurrent jobs 100s concurrent jobs let's discuss! Per-Job Requirements CPU cores 1 < 8 > 8 (or MPI) GPUs 0 1 > 1 Walltime < 10 hrs* < 20 hrs* > 20 hrs (not a good fit) RAM < few GB < 40 GB > 40 GB Input < 500 MB < 10 GB > 10 GB** Output < 1GB < 10 GB > 10 GB** Software pre-compiled binaries, containers Most other than ---> Licensed software, non-Linux * or checkpointable ** per job; you can work with a multi-TB dataset on the OSPool if it can be split into pieces! Some examples of work that have been a good fit for the OSPool and benefited from using its resources include: image analysis (including MRI, GIS, etc.) text-based analysis, including DNA read mapping and other bioinformatics hyper/parameter sweeps Monte Carlo methods and other model optimization Quickstart Resources \u00b6 Introduction to OSG the Distributed High Throughput Computing framework from the annual OSG User School : Full OSG User Documentation including our Roadmap to HTC Workload Submission OSG User Training materials . Any researcher affiliated with an academic, non-profit, or government US-based research project is welcome to attend our trainings. Learn more and chat with a Research Computing Facilitator by signing up for OSPool account","title":"Computation on the Open Science Pool"},{"location":"overview/account_setup/is-it-for-you/#computation-on-the-open-science-pool","text":"The OSG is a nationally-funded consortium of computing resources at more than one hundred institutional partners that, together, offer a strategic advantage for computing work that can be run as numerous short tasks that can execute independent of one another. For researchers who are not part of an organization with their own pool in the OSG, we offer the Open Science Pool (OSPool) , with dozens of campuses contributing excess computing capacity in support of open science. The OSPool is available for US-affiliated academic, government, and non-profit research projects and groups for their High Throughput Computing (HTC) workflows. Learn more about the services provided by the OSG that can support your HTC workload:","title":"Computation on the Open Science Pool"},{"location":"overview/account_setup/is-it-for-you/#computational-fit-on-the-ospool","text":"For problems that can be run as numerous independent jobs (a high-throughput approach) and have requirements represented in the first two columns of the table below, the significant capacity of the OSPool can transform the types of questions that researchers are able to tackle. Importantly, many compute tasks that may appear to not be a good fit can be modified in simple ways to take advantage, and we'd love to discuss options with you! Ideal jobs! Still advantageous Maybe not, but get in touch! Expected Throughput: 1000s concurrent jobs 100s concurrent jobs let's discuss! Per-Job Requirements CPU cores 1 < 8 > 8 (or MPI) GPUs 0 1 > 1 Walltime < 10 hrs* < 20 hrs* > 20 hrs (not a good fit) RAM < few GB < 40 GB > 40 GB Input < 500 MB < 10 GB > 10 GB** Output < 1GB < 10 GB > 10 GB** Software pre-compiled binaries, containers Most other than ---> Licensed software, non-Linux * or checkpointable ** per job; you can work with a multi-TB dataset on the OSPool if it can be split into pieces! Some examples of work that have been a good fit for the OSPool and benefited from using its resources include: image analysis (including MRI, GIS, etc.) text-based analysis, including DNA read mapping and other bioinformatics hyper/parameter sweeps Monte Carlo methods and other model optimization","title":"Computational Fit on the OSPool"},{"location":"overview/account_setup/is-it-for-you/#quickstart-resources","text":"Introduction to OSG the Distributed High Throughput Computing framework from the annual OSG User School : Full OSG User Documentation including our Roadmap to HTC Workload Submission OSG User Training materials . Any researcher affiliated with an academic, non-profit, or government US-based research project is welcome to attend our trainings. Learn more and chat with a Research Computing Facilitator by signing up for OSPool account","title":"Quickstart Resources"},{"location":"overview/account_setup/registration-and-login/","text":"Start Here: Overview of Requesting OSPool Access \u00b6 The major steps to get started on the OSPool are: apply for access to the OSPool meet with a facilitation team member for an short consultation and orientation. register for a specific OSPool Access Point log in to your designated Access Point Each of these is detailed in the guide below. Once you've gone through these steps, you should be able to begin running work! Apply for OSPool Access \u00b6 To start, fill out the interest form on this OSG Portal site: OSPool Consultation Request This will send the Research Facilitation team an email. We will be in touch to set up an orientation meeting, and confirm if you are joining an existing project on the OSPool or starting a new one. Orientation Meeting \u00b6 The orientation meeting generally takes about 20-30 minutes and is a chance to talk about your work, how it will fit on the OSPool, and some practical next steps for getting started. Register for an Access Point \u00b6 Before or during the orientation meeting, you will be prompted to register for an account on a specific OSPool Access Point. The current default are uw.osg-htc.org Access Points. Please do not register for an Access Point until you have been instructed to do so by a Research Computing Facilitator! Register for uw.osg-htc.org Access Points \u00b6 To register for the ap40.uw.osg-htc.org access point, submit an application using the following steps: Visit this account registration page . You will be redirected to the CILogon sign in page. Select your institution and use your institutional credentials to login. You will use these credentials later to login so it is important to remember the institution you use at this step. If you have issues signing in using your institutional credentials, contact us at support@osg-htc.org . Once you sign in, you will be redirected to the User Enrollment page. Click \"Begin\" and enter your name and email address in the following page. In many cases, this information will be automatically populated. If desired, it is possible to manually edit any information automatically filled in. Once you have entered your information, click \"SUBMIT\". After submitting your application, you will receive an email from registry@cilogon.org to verify your email address. Click the link listed in the email to be redirected to a page confirm your invitation details. Click the \"ACCEPT\" button to complete this step. Register for uc.osg-htc.org Access Points \u00b6 You can register for OSG Connect Access Points here: OSG Connect Account Registration Only do this if you have been instructed to do so by a Research Computing Facilitator! Account Approval by a Research Computing Facilitator \u00b6 If a meeting has not already been scheduled with a Research Computing Facilitator, one of the facilitation team will contact you about arranging a short consultation. Following the meeting, the Facilitator will approve your account and add your profile to any relevant OSG \u2018project\u2019 names. Once your account is ready, the Facilitator will email you with your account details. Log In \u00b6 Once you've gone through the steps above, you should have an account on on OSPool Access Point! Follow the instructions below to learn how to log in to you OSPool Access Point. Accounts for all new users are created on uw.osg-htc.org Access Points unless otherwise specified. Log In to uw.osg-htc.org Access Points (e.g., ap40.uw.osg-htc.org ) If your account is on the uw.osg-htc.org Access Points (e.g., accounts on ap40.uw.osg-htc.org ), follow instructions in this guide for logging in: Log In to uw.osg-htc.org Access Points Log In to uc.osg-htc.org Access Points (e.g., ap20.uc.osg-htc.org ) If your account is on the uc.osg-htc.org Access Points (e.g., accounts on ap20.uc.osg-htc.org , ap21.uc.osg-htc.org ), follow instructions in this guide for logging in: Log In to uc.osg-htc.org Access Points","title":"Start Here: Overview of Requesting OSPool Access"},{"location":"overview/account_setup/registration-and-login/#start-here-overview-of-requesting-ospool-access","text":"The major steps to get started on the OSPool are: apply for access to the OSPool meet with a facilitation team member for an short consultation and orientation. register for a specific OSPool Access Point log in to your designated Access Point Each of these is detailed in the guide below. Once you've gone through these steps, you should be able to begin running work!","title":"Start Here: Overview of Requesting OSPool Access"},{"location":"overview/account_setup/registration-and-login/#apply-for-ospool-access","text":"To start, fill out the interest form on this OSG Portal site: OSPool Consultation Request This will send the Research Facilitation team an email. We will be in touch to set up an orientation meeting, and confirm if you are joining an existing project on the OSPool or starting a new one.","title":"Apply for OSPool Access"},{"location":"overview/account_setup/registration-and-login/#orientation-meeting","text":"The orientation meeting generally takes about 20-30 minutes and is a chance to talk about your work, how it will fit on the OSPool, and some practical next steps for getting started.","title":"Orientation Meeting"},{"location":"overview/account_setup/registration-and-login/#register-for-an-access-point","text":"Before or during the orientation meeting, you will be prompted to register for an account on a specific OSPool Access Point. The current default are uw.osg-htc.org Access Points. Please do not register for an Access Point until you have been instructed to do so by a Research Computing Facilitator!","title":"Register for an Access Point"},{"location":"overview/account_setup/registration-and-login/#register-for-uwosg-htcorg-access-points","text":"To register for the ap40.uw.osg-htc.org access point, submit an application using the following steps: Visit this account registration page . You will be redirected to the CILogon sign in page. Select your institution and use your institutional credentials to login. You will use these credentials later to login so it is important to remember the institution you use at this step. If you have issues signing in using your institutional credentials, contact us at support@osg-htc.org . Once you sign in, you will be redirected to the User Enrollment page. Click \"Begin\" and enter your name and email address in the following page. In many cases, this information will be automatically populated. If desired, it is possible to manually edit any information automatically filled in. Once you have entered your information, click \"SUBMIT\". After submitting your application, you will receive an email from registry@cilogon.org to verify your email address. Click the link listed in the email to be redirected to a page confirm your invitation details. Click the \"ACCEPT\" button to complete this step.","title":"Register for uw.osg-htc.org Access Points"},{"location":"overview/account_setup/registration-and-login/#register-for-ucosg-htcorg-access-points","text":"You can register for OSG Connect Access Points here: OSG Connect Account Registration Only do this if you have been instructed to do so by a Research Computing Facilitator!","title":"Register for uc.osg-htc.org Access Points"},{"location":"overview/account_setup/registration-and-login/#account-approval-by-a-research-computing-facilitator","text":"If a meeting has not already been scheduled with a Research Computing Facilitator, one of the facilitation team will contact you about arranging a short consultation. Following the meeting, the Facilitator will approve your account and add your profile to any relevant OSG \u2018project\u2019 names. Once your account is ready, the Facilitator will email you with your account details.","title":"Account Approval by a Research Computing Facilitator"},{"location":"overview/account_setup/registration-and-login/#log-in","text":"Once you've gone through the steps above, you should have an account on on OSPool Access Point! Follow the instructions below to learn how to log in to you OSPool Access Point. Accounts for all new users are created on uw.osg-htc.org Access Points unless otherwise specified. Log In to uw.osg-htc.org Access Points (e.g., ap40.uw.osg-htc.org ) If your account is on the uw.osg-htc.org Access Points (e.g., accounts on ap40.uw.osg-htc.org ), follow instructions in this guide for logging in: Log In to uw.osg-htc.org Access Points Log In to uc.osg-htc.org Access Points (e.g., ap20.uc.osg-htc.org ) If your account is on the uc.osg-htc.org Access Points (e.g., accounts on ap20.uc.osg-htc.org , ap21.uc.osg-htc.org ), follow instructions in this guide for logging in: Log In to uc.osg-htc.org Access Points","title":"Log In"},{"location":"overview/account_setup/starting-project/","text":"Set and View Project Usage \u00b6 Background \u00b6 The OSG team assigns individual user accounts to \"projects\". These projects are a way to track usage hours and capture information about the types of research using the OSPool. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. You must be a member of a project before you can use an OSPool Access Point to submit jobs. The next section of this guide describes the process for joining a project. Default Behavior (one project) \u00b6 By default, you are added to a project when your OSG account is created. This project will be automatically added to your job submissions for tracking usage. Choose a Project (multiple projects) \u00b6 If you are affiliated with multiple groups using the OSPool and are a member of multiple projects, you will want to set the project name in your submit file. Run the following command to see a list of projects you belong to: grep $USER /etc/condor/UserToProjectMap.txt You can manually set the project for a set of jobs by putting this option in the submit file: +ProjectName=\"ProjectName\" View Metrics For Your Project \u00b6 The project's resource usage appears in the OSG accounting system, GRACC , specifically, in this OSPool Usage Dashboard At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, or your institution. You can adjust the time range displayed on the top right corner.","title":"Set and View Project Usage"},{"location":"overview/account_setup/starting-project/#set-and-view-project-usage","text":"","title":"Set and View Project Usage"},{"location":"overview/account_setup/starting-project/#background","text":"The OSG team assigns individual user accounts to \"projects\". These projects are a way to track usage hours and capture information about the types of research using the OSPool. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. You must be a member of a project before you can use an OSPool Access Point to submit jobs. The next section of this guide describes the process for joining a project.","title":"Background"},{"location":"overview/account_setup/starting-project/#default-behavior-one-project","text":"By default, you are added to a project when your OSG account is created. This project will be automatically added to your job submissions for tracking usage.","title":"Default Behavior (one project)"},{"location":"overview/account_setup/starting-project/#choose-a-project-multiple-projects","text":"If you are affiliated with multiple groups using the OSPool and are a member of multiple projects, you will want to set the project name in your submit file. Run the following command to see a list of projects you belong to: grep $USER /etc/condor/UserToProjectMap.txt You can manually set the project for a set of jobs by putting this option in the submit file: +ProjectName=\"ProjectName\"","title":"Choose a Project (multiple projects)"},{"location":"overview/account_setup/starting-project/#view-metrics-for-your-project","text":"The project's resource usage appears in the OSG accounting system, GRACC , specifically, in this OSPool Usage Dashboard At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, or your institution. You can adjust the time range displayed on the top right corner.","title":"View Metrics For Your Project"},{"location":"overview/references/acknowledgeOSG/","text":"Acknowledge the OSG \u00b6 This page has been moved to the OSG Website .","title":"Acknowledge the OSG"},{"location":"overview/references/acknowledgeOSG/#acknowledge-the-osg","text":"This page has been moved to the OSG Website .","title":"Acknowledge the OSG"},{"location":"overview/references/contact-information/","text":"Contact OSG for non-Support Inquiries \u00b6 For media contact, leadership, or general questions about OSG, please see our main website or send an email to webmaster@osg-htc.org. For OSG policies and executive information, email Frank Wuerthwein (OSG Executive Director). For help managing an OSG Mailing list membership, lease refer to our managing mailing list membership document . To get started using OSG resources, for support or operational issues, or to request OSPool account information, email support@osg-htc.org . For any assistance or technical questions regarding jobs or data, please see our page on how to Get Help and/or contact the OSG Research Facilitation team at support@osg-htc.org","title":"Contact OSG for non-Support Inquiries"},{"location":"overview/references/contact-information/#contact-osg-for-non-support-inquiries","text":"For media contact, leadership, or general questions about OSG, please see our main website or send an email to webmaster@osg-htc.org. For OSG policies and executive information, email Frank Wuerthwein (OSG Executive Director). For help managing an OSG Mailing list membership, lease refer to our managing mailing list membership document . To get started using OSG resources, for support or operational issues, or to request OSPool account information, email support@osg-htc.org . For any assistance or technical questions regarding jobs or data, please see our page on how to Get Help and/or contact the OSG Research Facilitation team at support@osg-htc.org","title":"Contact OSG for non-Support Inquiries"},{"location":"overview/references/frequently-asked-questions/","text":"Frequently Asked Questions \u00b6 Getting Started \u00b6 Who is eligible to request an OSG account? Any researcher affiliated with a U.S. institution (college, university, national laboratory or research foundation) is eligible to use OSG resources for their work. Researchers outside of the U.S. with affiliations to U.S. groups may be eligible for membership if they are sponsored by a collaborator within the U.S. How do I request an OSG account? Please visit our website for the most up-to-date information on requesting an account. Once your account request has been received, a Research Computing Facilitator will contact you within one business day to arrange a meeting to learn about your computational goals and to create your account. How do I change the project my jobs are affiliated with? The OSG team assigns individual user accounts to \"projects\" upon account creation. These projects are a way to track usage hours and capture information about the types of research running on OSG resources. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. If you only belong to a single project, that project will be charged automatically when you submit jobs. Run the following command to see a list of projects you belong to: $ grep $USER /etc/condor/UserToProjectMap.txt If need to run jobs under a different project you are a member of, you can manually set the project for those jobs by putting this option in the submit file: +ProjectName=\"ProjectName\" Can I use my ACCESS allocation? There are two ways OSG interfaces with ACCESS: You can get an allocation for the OSPool. This will allow you to run OSPool jobs and have the usage charged to your ACCESS credits, and can be useful if you already have an allocation. If you only need to use OSG resources, we recommend you come directly to our system. You can manage your workloads on the OSPool access points, and run those jobs on other ACCESS resources. This is a capability still in development. Workshops and Training \u00b6 Do you offer training sessions and workshops? We offer virtual trainings twice-a-month, as well as an annual, week-long summer school for OSG users. We also participate in additional external conferences and events throughout the year. Information about upcoming and past events, including workshop dates and locations, is available on our website. Who may attend OSG workshops? Workshops are available to any researcher affiliated with a U.S. academic, non-profit, or government institution. How to cite or acknowledge OSG? Whenever you make use of OSG resources, services or tools, we request you acknowledge OSG in your presentations and publications using the informtion provided on the Acknowledging the OSG Consortium page. Software \u00b6 What software packages are available? In general, we support most software that fits the distributed high throughput computing model (e.g., open source). Users are encouraged to download and install their own software on our Access Points. Additionally, users may install their software into a Docker container which can run on OSG as an Apptainer image or use one of our existing containers. See the Software guides on the OSPool documentation website for more information. Are there any restrictions on installing commercial softwares? We can only *directly* support software that is freely distributable. At present, we do not have or support most commercial software due to licensing issues. (One exception is running MATLAB standalone executables which have been compiled with the MATLAB Compiler Runtime). Software that is licensed to individual users (and not to be shared between users) can be staged within the user's `/home` or `/protected` directories, but should not be staged in OSG's `/public` data staging locations. See OSPool policies for more information. Please get in touch with any questions about licensed software. Can I request for system wide installation of the open source software useful for my research? We recommend users use Docker or Apptainer containers if jobs require system wide installations of software. Visit the OSPool Documentation website to learn more about creating your own container. Running Jobs \u00b6 What type of computation is a good match or NOT a good match for the OSG's Open Science Pool? The OSG provides computing resources through the Open Science Pool for high throughput computing workloads. You can get the most of out OSG resources by breaking up a single large computational task into many smaller tasks for the fastest overall turnaround. This approach can be invaluable in accelerating your computational work and thus your research. Please see our Computation on the Open Science Pool page for more details on how to determine if your work matches up well with OSG's high throughput computing model. What job scheduler is being used on the Open Science Pool? We use a task scheduling software called HTCondor to schedule and run jobs. How do I submit a computing job? Jobs are submitted via HTCondor scheduler. Please see our Roadmap to HTC Workload Submission guide for more details on submitting and managing jobs. How many jobs can I have in the queue? The number of jobs that are submitted to the queue by any one user cannot not exceed 10,000 without adding a special statement to the submit file. If you have more jobs than that, we ask that you include the following statement in your submit file: max_idle = 2000 This is the maximum number of jobs that you will have in the \"Idle\" or \"Held\" state for the submitted batch of jobs at any given time. Using a value of 2000 will ensure that your jobs continue to apply a constant pressure on the queue, but will not fill up the queue unnecessarily (which helps the scheduler to perform optimally). How do I view usage metrics for my project? The project's resource usage appears in the OSG accounting system, GRid ACcounting Collector (GRACC) . Additional dashboards are available to help filter information of interest. At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, or your institution. Why specify +JobDurationCategory in the HTCondor submit file? To maximize the value of the capacity contributed by the different organizations to the OSPool, users are requested to identify a duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool. Every job submitted from an OSG-managed access point must be labeled with a Job Duration Category upon submission. By knowing the expected job duration, OSG will be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted, without self-checkpointing. Details on how to specify +JobDurationCategory can be found in our Overview: Submit Jobs to the OSPool using HTCondor and Roadmap to HTC Workload Submission guides. Data Storage and Transfer \u00b6 What is the best way to process large volume of data? There may be more than one solution that is available to researchers to process large amounts of data. Contact a Facilitator at for a free, individual consulation to learn about your options. How do I transfer my data to and from OSG Access Points? You can transfer data using `scp`, `rsync`, or other common Unix tools. See Using scp To Transfer Files for more details. Is there any support for private data? Data stored in `/protected` and in `/home` is not publically accessible. Sensitive data, such as HIPAA data, is not allowed to be uploaded or analyzed using OSG resources. Is data backed up on OSG resources? Our data storage locations are not backed up nor are they intended for long-term storage. If the data is not being used for active computing work, it should not be stored on OSG systems. Can I get a quota increase? Yes. Contact support@osg-htc.org if you think you'll need a quota increase for `/home`, `/public`, or `/protected` to accommodate a set of concurrently-running jobs. We can suppport very large amounts of data, the default quotas are just a starting point. Will I get notified about hitting quota limits? The best place to see your quota status is in the login message. Workflow Management \u00b6 How do I run and manage complex workflows? For workflows that have multiple steps and/or multiple files, we advise using a workflow management system. A workflow management system allows you to define different computational steps in your workflow and indicate how inputs and outputs should be transferred between these steps. Once you define a workflow, the workflow management system will then run your workflow, automatically retrying failed jobs and transferrring files between different steps. What workflow management systems are recommended on OSG? We support DAGMan and Pegasus for workflow management.","title":"Frequently Asked Questions"},{"location":"overview/references/frequently-asked-questions/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"overview/references/frequently-asked-questions/#getting-started","text":"Who is eligible to request an OSG account? Any researcher affiliated with a U.S. institution (college, university, national laboratory or research foundation) is eligible to use OSG resources for their work. Researchers outside of the U.S. with affiliations to U.S. groups may be eligible for membership if they are sponsored by a collaborator within the U.S. How do I request an OSG account? Please visit our website for the most up-to-date information on requesting an account. Once your account request has been received, a Research Computing Facilitator will contact you within one business day to arrange a meeting to learn about your computational goals and to create your account. How do I change the project my jobs are affiliated with? The OSG team assigns individual user accounts to \"projects\" upon account creation. These projects are a way to track usage hours and capture information about the types of research running on OSG resources. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. If you only belong to a single project, that project will be charged automatically when you submit jobs. Run the following command to see a list of projects you belong to: $ grep $USER /etc/condor/UserToProjectMap.txt If need to run jobs under a different project you are a member of, you can manually set the project for those jobs by putting this option in the submit file: +ProjectName=\"ProjectName\" Can I use my ACCESS allocation? There are two ways OSG interfaces with ACCESS: You can get an allocation for the OSPool. This will allow you to run OSPool jobs and have the usage charged to your ACCESS credits, and can be useful if you already have an allocation. If you only need to use OSG resources, we recommend you come directly to our system. You can manage your workloads on the OSPool access points, and run those jobs on other ACCESS resources. This is a capability still in development.","title":"Getting Started"},{"location":"overview/references/frequently-asked-questions/#workshops-and-training","text":"Do you offer training sessions and workshops? We offer virtual trainings twice-a-month, as well as an annual, week-long summer school for OSG users. We also participate in additional external conferences and events throughout the year. Information about upcoming and past events, including workshop dates and locations, is available on our website. Who may attend OSG workshops? Workshops are available to any researcher affiliated with a U.S. academic, non-profit, or government institution. How to cite or acknowledge OSG? Whenever you make use of OSG resources, services or tools, we request you acknowledge OSG in your presentations and publications using the informtion provided on the Acknowledging the OSG Consortium page.","title":"Workshops and Training"},{"location":"overview/references/frequently-asked-questions/#software","text":"What software packages are available? In general, we support most software that fits the distributed high throughput computing model (e.g., open source). Users are encouraged to download and install their own software on our Access Points. Additionally, users may install their software into a Docker container which can run on OSG as an Apptainer image or use one of our existing containers. See the Software guides on the OSPool documentation website for more information. Are there any restrictions on installing commercial softwares? We can only *directly* support software that is freely distributable. At present, we do not have or support most commercial software due to licensing issues. (One exception is running MATLAB standalone executables which have been compiled with the MATLAB Compiler Runtime). Software that is licensed to individual users (and not to be shared between users) can be staged within the user's `/home` or `/protected` directories, but should not be staged in OSG's `/public` data staging locations. See OSPool policies for more information. Please get in touch with any questions about licensed software. Can I request for system wide installation of the open source software useful for my research? We recommend users use Docker or Apptainer containers if jobs require system wide installations of software. Visit the OSPool Documentation website to learn more about creating your own container.","title":"Software"},{"location":"overview/references/frequently-asked-questions/#running-jobs","text":"What type of computation is a good match or NOT a good match for the OSG's Open Science Pool? The OSG provides computing resources through the Open Science Pool for high throughput computing workloads. You can get the most of out OSG resources by breaking up a single large computational task into many smaller tasks for the fastest overall turnaround. This approach can be invaluable in accelerating your computational work and thus your research. Please see our Computation on the Open Science Pool page for more details on how to determine if your work matches up well with OSG's high throughput computing model. What job scheduler is being used on the Open Science Pool? We use a task scheduling software called HTCondor to schedule and run jobs. How do I submit a computing job? Jobs are submitted via HTCondor scheduler. Please see our Roadmap to HTC Workload Submission guide for more details on submitting and managing jobs. How many jobs can I have in the queue? The number of jobs that are submitted to the queue by any one user cannot not exceed 10,000 without adding a special statement to the submit file. If you have more jobs than that, we ask that you include the following statement in your submit file: max_idle = 2000 This is the maximum number of jobs that you will have in the \"Idle\" or \"Held\" state for the submitted batch of jobs at any given time. Using a value of 2000 will ensure that your jobs continue to apply a constant pressure on the queue, but will not fill up the queue unnecessarily (which helps the scheduler to perform optimally). How do I view usage metrics for my project? The project's resource usage appears in the OSG accounting system, GRid ACcounting Collector (GRACC) . Additional dashboards are available to help filter information of interest. At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, or your institution. Why specify +JobDurationCategory in the HTCondor submit file? To maximize the value of the capacity contributed by the different organizations to the OSPool, users are requested to identify a duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool. Every job submitted from an OSG-managed access point must be labeled with a Job Duration Category upon submission. By knowing the expected job duration, OSG will be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted, without self-checkpointing. Details on how to specify +JobDurationCategory can be found in our Overview: Submit Jobs to the OSPool using HTCondor and Roadmap to HTC Workload Submission guides.","title":"Running Jobs"},{"location":"overview/references/frequently-asked-questions/#data-storage-and-transfer","text":"What is the best way to process large volume of data? There may be more than one solution that is available to researchers to process large amounts of data. Contact a Facilitator at for a free, individual consulation to learn about your options. How do I transfer my data to and from OSG Access Points? You can transfer data using `scp`, `rsync`, or other common Unix tools. See Using scp To Transfer Files for more details. Is there any support for private data? Data stored in `/protected` and in `/home` is not publically accessible. Sensitive data, such as HIPAA data, is not allowed to be uploaded or analyzed using OSG resources. Is data backed up on OSG resources? Our data storage locations are not backed up nor are they intended for long-term storage. If the data is not being used for active computing work, it should not be stored on OSG systems. Can I get a quota increase? Yes. Contact support@osg-htc.org if you think you'll need a quota increase for `/home`, `/public`, or `/protected` to accommodate a set of concurrently-running jobs. We can suppport very large amounts of data, the default quotas are just a starting point. Will I get notified about hitting quota limits? The best place to see your quota status is in the login message.","title":"Data Storage and Transfer"},{"location":"overview/references/frequently-asked-questions/#workflow-management","text":"How do I run and manage complex workflows? For workflows that have multiple steps and/or multiple files, we advise using a workflow management system. A workflow management system allows you to define different computational steps in your workflow and indicate how inputs and outputs should be transferred between these steps. Once you define a workflow, the workflow management system will then run your workflow, automatically retrying failed jobs and transferrring files between different steps. What workflow management systems are recommended on OSG? We support DAGMan and Pegasus for workflow management.","title":"Workflow Management"},{"location":"overview/references/gracc/","text":"OSG Accounting (GRACC) \u00b6 GRACC is the Open Science Pool's accounting system. If you need graphs or high level statistics on your OSG usage, please go to: https://gracc.opensciencegrid.org/ GRACC contains an overwhelming amount of data. As an OSPool user, you are most likely interested in seeing your own usage over time. This can be found under the Open Science Pool - All Usage dashboard here Under the Project drop-down, find your project. You can select multiple ones. In the upper right corner, you can select a different time period. You can then select a different Bin size time range. For example, if you want data for the last year grouped monthly, select \"Last 1 year\" for the Period , and \"1M\" for the Bin size . Here is an example of what the information provided will look like:","title":"OSG Accounting (GRACC)"},{"location":"overview/references/gracc/#osg-accounting-gracc","text":"GRACC is the Open Science Pool's accounting system. If you need graphs or high level statistics on your OSG usage, please go to: https://gracc.opensciencegrid.org/ GRACC contains an overwhelming amount of data. As an OSPool user, you are most likely interested in seeing your own usage over time. This can be found under the Open Science Pool - All Usage dashboard here Under the Project drop-down, find your project. You can select multiple ones. In the upper right corner, you can select a different time period. You can then select a different Bin size time range. For example, if you want data for the last year grouped monthly, select \"Last 1 year\" for the Period , and \"1M\" for the Bin size . Here is an example of what the information provided will look like:","title":"OSG Accounting (GRACC)"},{"location":"overview/references/policy/","text":"Policies for Using OSG Services and the OSPool \u00b6 Access to OSG services and the Open Science Pool (OSPool) is contingent on compliance with the below and with any requests from OSG staff to change practices that cause issues for OSG systems and/or users. Please contact us if you have any questions! We can often help with exceptions to default policies and/or identify available alternative approaches to help you with a perceived barrier. As the below do not cover every possible scenario of potentially disruptive practices, OSG staff reserve the right to take any necessary corrective actions to ensure performance and resource availability for all users from OSG-managed Access Points. This may include the hold or removal of jobs, deletion of user data, deactivation of accounts, etc. In some cases, these actions may need to be taken without notifying the user. By using the OSG resources, users are expected to follow the Open Science Pool acceptable use policy , which includes appropriate scope of use and common user security practices. OSG resources are only available to individuals affiliated with a US-based academic, government, or non-profit organization, or with a research project led by an affiliated sponsor. Users can have up to 10,000 jobs queued, without taking additional steps , and should submit multiple jobs via a single submit file, according to our online guides. Please write to us if you\u2019d like to easily submit more! Do not run computationally-intensive or persistent processes on the Access Points (login nodes). Exceptions include single-threaded software compilation and data management tasks (transfer to/from the Access Point, directory creation, file moving/renaming, untar-ing, etc.). The execution of multi-threaded tasks for job setup or post-processing or software testing will almost certainly cause performance issues and may result in loss of access. Software testing should be executed from within submitted jobs, where job scheduling also provides a more accurate test environment to the user without compromising performance of the Access Points. OSG staff reserve the right to kill any tasks running on the login nodes, in order to ensure performance for all users. Similarly, please contact us to discuss appropriate features and options, rather than running scripts (including cron ) to automate job submission , throttling, resubmission, or ordered execution (e.g. workflows), even if these are executed remotely to coordinate work on OSG-managed Access Points. These almost always end up causing significant issues and/or wasted computing capacity, and we're happy to help you to implement automation tools the integrate with HTCondor. Data Policies : OSG-managed filesystems are not backed up and should be treated as temporary (\u201cscratch\u201d-like) space for active work, only , following OSG policies for data storage and per-job transfers . Some OSG-managed storage spaces are truly \u2018open\u2019 with data available to be downloaded publicly. Of note: Users should keep copies of essential data and software in non-OSG locations, as OSG staff reserve the right to remove data at any time in order to ensure and/or restore system availability, and without prior notice to users. Proprietary data, HIPAA, and data with any other privacy concerns should not be stored on any OSG-managed filesystems or computed on using OSG-managed resources. Similarly, users should follow all licensing requirements when storing and executing software via OSG-managed Access Points. Users should keep their /home directory privileges restricted to their user or group, and should not add \u2018global\u2019 permissions, which will allow other users to potentially make your data public. User-created \u2018open\u2019 network ports are disallowed , unless explicitly permitted following an accepted justification to support@osg-htc.org. (If you\u2019re not sure whether something you want to do will open a port, just get in touch!) The following actions may be taken automatedly or by OSG staff to stop or prevent jobs from causing problems. Please contact us if you\u2019d like help understanding why your jobs were held or removed, and so we can help you avoid problems in the future. Jobs using more memory or disk than requested may be automatically held (see Scaling Up after Test Jobs for tips on requesting the \u2018right\u2019 amount of job resources in your submit file). Jobs running longer than their JobDurationCategory allows for will be held (see Indicate the Job Duration Category of Your Jobs ). Jobs that have executed more than 30 times without completing may be automatically held (likely because they\u2019re too long for OSG). Jobs that have been held more than 14 days may be automatically removed. Jobs queued for more than three months may be automatically removed. Jobs otherwise causing known problems may be held or removed, without prior notification to the user. Held jobs may also be edited to prevent automated release/retry NOTE: in order to respect user email clients, job holds and removals do not come with specific notification to the user, unless configured by the user at the time of submission using HTCondor\u2019s \u2018notification\u2019 feature.","title":"Policies for Using OSG Services and the OSPool "},{"location":"overview/references/policy/#policies-for-using-osg-services-and-the-ospool","text":"Access to OSG services and the Open Science Pool (OSPool) is contingent on compliance with the below and with any requests from OSG staff to change practices that cause issues for OSG systems and/or users. Please contact us if you have any questions! We can often help with exceptions to default policies and/or identify available alternative approaches to help you with a perceived barrier. As the below do not cover every possible scenario of potentially disruptive practices, OSG staff reserve the right to take any necessary corrective actions to ensure performance and resource availability for all users from OSG-managed Access Points. This may include the hold or removal of jobs, deletion of user data, deactivation of accounts, etc. In some cases, these actions may need to be taken without notifying the user. By using the OSG resources, users are expected to follow the Open Science Pool acceptable use policy , which includes appropriate scope of use and common user security practices. OSG resources are only available to individuals affiliated with a US-based academic, government, or non-profit organization, or with a research project led by an affiliated sponsor. Users can have up to 10,000 jobs queued, without taking additional steps , and should submit multiple jobs via a single submit file, according to our online guides. Please write to us if you\u2019d like to easily submit more! Do not run computationally-intensive or persistent processes on the Access Points (login nodes). Exceptions include single-threaded software compilation and data management tasks (transfer to/from the Access Point, directory creation, file moving/renaming, untar-ing, etc.). The execution of multi-threaded tasks for job setup or post-processing or software testing will almost certainly cause performance issues and may result in loss of access. Software testing should be executed from within submitted jobs, where job scheduling also provides a more accurate test environment to the user without compromising performance of the Access Points. OSG staff reserve the right to kill any tasks running on the login nodes, in order to ensure performance for all users. Similarly, please contact us to discuss appropriate features and options, rather than running scripts (including cron ) to automate job submission , throttling, resubmission, or ordered execution (e.g. workflows), even if these are executed remotely to coordinate work on OSG-managed Access Points. These almost always end up causing significant issues and/or wasted computing capacity, and we're happy to help you to implement automation tools the integrate with HTCondor. Data Policies : OSG-managed filesystems are not backed up and should be treated as temporary (\u201cscratch\u201d-like) space for active work, only , following OSG policies for data storage and per-job transfers . Some OSG-managed storage spaces are truly \u2018open\u2019 with data available to be downloaded publicly. Of note: Users should keep copies of essential data and software in non-OSG locations, as OSG staff reserve the right to remove data at any time in order to ensure and/or restore system availability, and without prior notice to users. Proprietary data, HIPAA, and data with any other privacy concerns should not be stored on any OSG-managed filesystems or computed on using OSG-managed resources. Similarly, users should follow all licensing requirements when storing and executing software via OSG-managed Access Points. Users should keep their /home directory privileges restricted to their user or group, and should not add \u2018global\u2019 permissions, which will allow other users to potentially make your data public. User-created \u2018open\u2019 network ports are disallowed , unless explicitly permitted following an accepted justification to support@osg-htc.org. (If you\u2019re not sure whether something you want to do will open a port, just get in touch!) The following actions may be taken automatedly or by OSG staff to stop or prevent jobs from causing problems. Please contact us if you\u2019d like help understanding why your jobs were held or removed, and so we can help you avoid problems in the future. Jobs using more memory or disk than requested may be automatically held (see Scaling Up after Test Jobs for tips on requesting the \u2018right\u2019 amount of job resources in your submit file). Jobs running longer than their JobDurationCategory allows for will be held (see Indicate the Job Duration Category of Your Jobs ). Jobs that have executed more than 30 times without completing may be automatically held (likely because they\u2019re too long for OSG). Jobs that have been held more than 14 days may be automatically removed. Jobs queued for more than three months may be automatically removed. Jobs otherwise causing known problems may be held or removed, without prior notification to the user. Held jobs may also be edited to prevent automated release/retry NOTE: in order to respect user email clients, job holds and removals do not come with specific notification to the user, unless configured by the user at the time of submission using HTCondor\u2019s \u2018notification\u2019 feature.","title":"Policies for Using OSG Services and the OSPool"},{"location":"software_examples/ai/scikit-learn/","text":"scikit-learn \u00b6 scikit-learn is a machine learning toolkit for Python. Below you will find an example on how to use an OSG-provided software container that contains scikit-learn. However, it is good to keep in mind that you have two options when it comes to integrating your own code: If the code is simple, send it with the job (this is what the example uses) For more complex codes, consider extending the provided containers and integrate the code into the new custom container Containers are detailed in our general documentation: Containers - Apptainer/Singularity Scikit-learn Python Code \u00b6 An example scikit-learn machine learning executable is: #!/usr/bin/env python3 # example adopted from https://scikit-learn.org/stable/tutorial/basic/tutorial.html from sklearn import datasets from sklearn import svm iris = datasets.load_iris() digits = datasets.load_digits() # learning clf = svm.SVC(gamma=0.001, C=100.) clf.fit(digits.data[:-1], digits.target[:-1]) # predicting print(clf.predict(digits.data[-1:])) Submit File \u00b6 universe = container container_image = /cvmfs/singularity.opensciencegrid.org/htc/scikit-learn:1.3 log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out executable = run-scikit-learn.py #arguments = # specify both general requirements and gpu requirements if there are any # requirements = True # require_gpus = +JobDurationCategory = \"Medium\" request_gpus = 0 request_cpus = 1 request_memory = 4GB request_disk = 4GB queue 1","title":"scikit-learn"},{"location":"software_examples/ai/scikit-learn/#scikit-learn","text":"scikit-learn is a machine learning toolkit for Python. Below you will find an example on how to use an OSG-provided software container that contains scikit-learn. However, it is good to keep in mind that you have two options when it comes to integrating your own code: If the code is simple, send it with the job (this is what the example uses) For more complex codes, consider extending the provided containers and integrate the code into the new custom container Containers are detailed in our general documentation: Containers - Apptainer/Singularity","title":"scikit-learn"},{"location":"software_examples/ai/scikit-learn/#scikit-learn-python-code","text":"An example scikit-learn machine learning executable is: #!/usr/bin/env python3 # example adopted from https://scikit-learn.org/stable/tutorial/basic/tutorial.html from sklearn import datasets from sklearn import svm iris = datasets.load_iris() digits = datasets.load_digits() # learning clf = svm.SVC(gamma=0.001, C=100.) clf.fit(digits.data[:-1], digits.target[:-1]) # predicting print(clf.predict(digits.data[-1:]))","title":"Scikit-learn Python Code"},{"location":"software_examples/ai/scikit-learn/#submit-file","text":"universe = container container_image = /cvmfs/singularity.opensciencegrid.org/htc/scikit-learn:1.3 log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out executable = run-scikit-learn.py #arguments = # specify both general requirements and gpu requirements if there are any # requirements = True # require_gpus = +JobDurationCategory = \"Medium\" request_gpus = 0 request_cpus = 1 request_memory = 4GB request_disk = 4GB queue 1","title":"Submit File"},{"location":"software_examples/ai/tensorflow/","text":"The OSPool enables AI (Artificial Intelligence) workloads by providing access to GPUs and custom software stacks via containers. An example of this support is the machine learning platform TensorFlow. TensorFlow \u00b6 https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. TensorFlow can be a complicated software to install as it requires many dependencies and specific environmental configurations. Software ontainers solve this problem by defining a full operating system image, containing not only the complex software package, but dependencies and environment configuration as well. Working with GPUs and containers are detailed in the general documentation: GPU Jobs Containers - Apptainer/Singularity TensorFlow Python Code \u00b6 An example TensorFlow executable that builds a machine learning model and evaluates it is: #!/usr/bin/env python3 # example adopted from https://www.tensorflow.org/tutorials/quickstart/beginner import tensorflow as tf print(\"TensorFlow version:\", tf.__version__) # this will show that the GPU was found tf.debugging.set_log_device_placement(True) # load a dataset mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 # build a machine learning model model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10) ]) predictions = model(x_train[:1]).numpy() # convert to probabilities tf.nn.softmax(predictions).numpy() # loss function loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) loss_fn(y_train[:1], predictions).numpy() # compile model model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy']) # train model.fit(x_train, y_train, epochs=5) # evaluate model.evaluate(x_test, y_test, verbose=2) HTCondor Submit File \u00b6 To run this TensorFlow script, create an HTCondor submit file to tell HTCondor how you would like it run on your behalf. An example HTCondor submit file for this job is below. Because TensorFlow is optimized to run with GPUs, make sure to tell HTCondor to assign your job to a GPU machine: universe = container container_image = /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:2.15 log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out executable = run-tf.py #arguments = +JobDurationCategory = \"Medium\" # specify both general requirements and gpu requirements if needed # requirements = True require_gpus = (Capability > 7.5) request_gpus = 1 request_cpus = 1 request_memory = 4GB request_disk = 4GB queue 1 Run TensorFlow \u00b6 Since we have prepared our executable, submit file, and are using an OSG-provided TensorFlow container, we are ready to submit this job to run on one of the OSPool GPU machines. To submit this job to run, type condor_submit TensorFlow.submit . The status of your job can be checked at any time by running condor_q .","title":"TensorFlow"},{"location":"software_examples/ai/tensorflow/#tensorflow","text":"https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. TensorFlow can be a complicated software to install as it requires many dependencies and specific environmental configurations. Software ontainers solve this problem by defining a full operating system image, containing not only the complex software package, but dependencies and environment configuration as well. Working with GPUs and containers are detailed in the general documentation: GPU Jobs Containers - Apptainer/Singularity","title":"TensorFlow"},{"location":"software_examples/ai/tensorflow/#tensorflow-python-code","text":"An example TensorFlow executable that builds a machine learning model and evaluates it is: #!/usr/bin/env python3 # example adopted from https://www.tensorflow.org/tutorials/quickstart/beginner import tensorflow as tf print(\"TensorFlow version:\", tf.__version__) # this will show that the GPU was found tf.debugging.set_log_device_placement(True) # load a dataset mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 # build a machine learning model model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10) ]) predictions = model(x_train[:1]).numpy() # convert to probabilities tf.nn.softmax(predictions).numpy() # loss function loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) loss_fn(y_train[:1], predictions).numpy() # compile model model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy']) # train model.fit(x_train, y_train, epochs=5) # evaluate model.evaluate(x_test, y_test, verbose=2)","title":"TensorFlow Python Code"},{"location":"software_examples/ai/tensorflow/#htcondor-submit-file","text":"To run this TensorFlow script, create an HTCondor submit file to tell HTCondor how you would like it run on your behalf. An example HTCondor submit file for this job is below. Because TensorFlow is optimized to run with GPUs, make sure to tell HTCondor to assign your job to a GPU machine: universe = container container_image = /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:2.15 log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out executable = run-tf.py #arguments = +JobDurationCategory = \"Medium\" # specify both general requirements and gpu requirements if needed # requirements = True require_gpus = (Capability > 7.5) request_gpus = 1 request_cpus = 1 request_memory = 4GB request_disk = 4GB queue 1","title":"HTCondor Submit File"},{"location":"software_examples/ai/tensorflow/#run-tensorflow","text":"Since we have prepared our executable, submit file, and are using an OSG-provided TensorFlow container, we are ready to submit this job to run on one of the OSPool GPU machines. To submit this job to run, type condor_submit TensorFlow.submit . The status of your job can be checked at any time by running condor_q .","title":"Run TensorFlow"},{"location":"software_examples/ai/tutorial-pytorch/","text":"The OSPool can be used as a platform to carry out machine learning and artificial intelligence research. The following tutorial uses the common machine learning framework, PyTorch. Using PyTorch on OSPool \u00b6 The preferred method of using a software on the the OSPool is to use a container. The guide shows two ways of running PyTorch on the OSPool. Firstly, downloading our desired version of PyTorch images from dockerhub. Secondly, how to use an already created singularity container of PyTorch to submit a HTCondor job on OSPool. Pulling an Image from Docker \u00b6 Please note that the docker build will not work on the access point . Apptainer is installed on the access point and users can use Apptainer to either build an image from the definition file or use apptainer pull to create a .sif file from Docker images. At the time the guide is written, the latest version of PyTorch is 2.1.1. Before pulling the image/software from Docker it is a good practice to set up the cache directory of Apptainer. Run the following command on the command prompt [user@ap]$ mkdir $HOME/tmp [user@ap]$ export TMPDIR=$HOME/tmp [user@ap]$ export APPTAINER_TMPDIR=$HOME/tmp [user@ap]$ export APPTAINER_CACHEDIR=$HOME/tmp Now, we pull the image and convert it to a .sif file using apptainer pull [user@ap]$ apptainer pull pytorch-2.1.1.sif docker://pytorch/pytorch:2.1.1-cuda12.1-cudnn8-runtime Transfer the image using OSDF \u00b6 The above command will create a singularity container named pytorch-2.1.1.sif in your current directory. The image will be reused for each job, and thus the preferred transfer method is OSDF . Store the pytorch-2.1.1.sif file under the \"protected\" area on your access point (see table here ), and then use the OSDF url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSPool username. +SingularityImage = \"osdf:///ospool/PROTECTED/<USERNAME>/pytorch-2.1.1.sif\" <other usual submit file lines> queue Using an existing PyTorch container \u00b6 OSG has the pytorch-2.1.1.sif container image. To use the OSG built container just provide the address of the container- '/ospool/uc-shared/public/OSG-Staff/pytorch-2.1.1.sif' in to your submit file +SingularityImage = \"osdf:///ospool/uc-shared/public/OSG-Staff/pytorch-2.1.1.sif\" <other usual submit file lines> queue Running an ML job using PyTorch \u00b6 For this tutorial, we will see how to use PyTorch to run a machine learning workflow from the MNIST database. To download the materials for this tutorial, use the command git clone https://github.com/OSGConnect/tutorial-pytorch The github repository contains a tarball of the MNIST data- MNIST_data.tar.gz , a wrapper script- pytorch_cnn.sh that untars the data and runs the python script- main.py to train a neural network on this MNIST database. The content of the pytorch_cnn.sh wrapper script is given below: #!/bin/bash echo \"Hello OSPool from Job $1 running on `hostname`\" # untar the test and training data tar zxf MNIST_data.tar.gz # run the PyTorch model python main.py --save-model --epochs 20 # remove the data directory rm -r data A submit script- pytorch_cnn.sub is also there to submit the PyTorch job on the OSPool using the container that is provided by OSG. The contents of pytorch_cnn.sub file are: # PyTorch test of convolutional neural network # Submit file +SingularityImage = \"osdf:///ospool/uc-shared/public/OSG-Staff/pytorch-2.1.1.sif\" # set the log, error and output files log = logs/pytorch_cnn.log.txt error = logs/pytorch_cnn.err.txt output = output/pytorch_cnn.out.txt # set the executable to run executable = pytorch_cnn.sh arguments = $(Process) # Transfer the python script and the MNIST database to the compute node transfer_input_files = main.py, MNIST_data.tar.gz should_transfer_files = YES when_to_transfer_output = ON_EXIT # We require a machine with a compatible version of the CUDA driver require_gpus = (DriverVersion >= 10.1) # We must request 1 CPU in addition to 1 GPU request_cpus = 1 request_gpus = 1 # select some memory and disk space request_memory = 3GB request_disk = 5GB # Tell HTCondor to run 1 instance of our job: queue 1 Please note, if you want to use your own container please replace the +SingularityImage attribute accordingly. Create Log Directories and Submit Job \u00b6 You will need to create the logs and output directories to hold the files that will be created for each job. You can create both directories at once with the command mkdir logs output Submit the job using condor_submit pytorch_cnn.sub Output \u00b6 The output of the code will be the CNN Network that was trained. It will be returned to us as a file mnist_cnn.pt . The are also some output stats on the training and test error in the pytorch_cnn.out.txt. file Test set: Average loss: 0.0278, Accuracy: 9909/10000 (99%) Getting help \u00b6 For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the help desk and community forums .","title":"PyTorch"},{"location":"software_examples/ai/tutorial-pytorch/#using-pytorch-on-ospool","text":"The preferred method of using a software on the the OSPool is to use a container. The guide shows two ways of running PyTorch on the OSPool. Firstly, downloading our desired version of PyTorch images from dockerhub. Secondly, how to use an already created singularity container of PyTorch to submit a HTCondor job on OSPool.","title":"Using PyTorch on OSPool"},{"location":"software_examples/ai/tutorial-pytorch/#pulling-an-image-from-docker","text":"Please note that the docker build will not work on the access point . Apptainer is installed on the access point and users can use Apptainer to either build an image from the definition file or use apptainer pull to create a .sif file from Docker images. At the time the guide is written, the latest version of PyTorch is 2.1.1. Before pulling the image/software from Docker it is a good practice to set up the cache directory of Apptainer. Run the following command on the command prompt [user@ap]$ mkdir $HOME/tmp [user@ap]$ export TMPDIR=$HOME/tmp [user@ap]$ export APPTAINER_TMPDIR=$HOME/tmp [user@ap]$ export APPTAINER_CACHEDIR=$HOME/tmp Now, we pull the image and convert it to a .sif file using apptainer pull [user@ap]$ apptainer pull pytorch-2.1.1.sif docker://pytorch/pytorch:2.1.1-cuda12.1-cudnn8-runtime","title":"Pulling an Image from Docker"},{"location":"software_examples/ai/tutorial-pytorch/#transfer-the-image-using-osdf","text":"The above command will create a singularity container named pytorch-2.1.1.sif in your current directory. The image will be reused for each job, and thus the preferred transfer method is OSDF . Store the pytorch-2.1.1.sif file under the \"protected\" area on your access point (see table here ), and then use the OSDF url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSPool username. +SingularityImage = \"osdf:///ospool/PROTECTED/<USERNAME>/pytorch-2.1.1.sif\" <other usual submit file lines> queue","title":"Transfer the image using OSDF"},{"location":"software_examples/ai/tutorial-pytorch/#using-an-existing-pytorch-container","text":"OSG has the pytorch-2.1.1.sif container image. To use the OSG built container just provide the address of the container- '/ospool/uc-shared/public/OSG-Staff/pytorch-2.1.1.sif' in to your submit file +SingularityImage = \"osdf:///ospool/uc-shared/public/OSG-Staff/pytorch-2.1.1.sif\" <other usual submit file lines> queue","title":"Using an existing PyTorch container"},{"location":"software_examples/ai/tutorial-pytorch/#running-an-ml-job-using-pytorch","text":"For this tutorial, we will see how to use PyTorch to run a machine learning workflow from the MNIST database. To download the materials for this tutorial, use the command git clone https://github.com/OSGConnect/tutorial-pytorch The github repository contains a tarball of the MNIST data- MNIST_data.tar.gz , a wrapper script- pytorch_cnn.sh that untars the data and runs the python script- main.py to train a neural network on this MNIST database. The content of the pytorch_cnn.sh wrapper script is given below: #!/bin/bash echo \"Hello OSPool from Job $1 running on `hostname`\" # untar the test and training data tar zxf MNIST_data.tar.gz # run the PyTorch model python main.py --save-model --epochs 20 # remove the data directory rm -r data A submit script- pytorch_cnn.sub is also there to submit the PyTorch job on the OSPool using the container that is provided by OSG. The contents of pytorch_cnn.sub file are: # PyTorch test of convolutional neural network # Submit file +SingularityImage = \"osdf:///ospool/uc-shared/public/OSG-Staff/pytorch-2.1.1.sif\" # set the log, error and output files log = logs/pytorch_cnn.log.txt error = logs/pytorch_cnn.err.txt output = output/pytorch_cnn.out.txt # set the executable to run executable = pytorch_cnn.sh arguments = $(Process) # Transfer the python script and the MNIST database to the compute node transfer_input_files = main.py, MNIST_data.tar.gz should_transfer_files = YES when_to_transfer_output = ON_EXIT # We require a machine with a compatible version of the CUDA driver require_gpus = (DriverVersion >= 10.1) # We must request 1 CPU in addition to 1 GPU request_cpus = 1 request_gpus = 1 # select some memory and disk space request_memory = 3GB request_disk = 5GB # Tell HTCondor to run 1 instance of our job: queue 1 Please note, if you want to use your own container please replace the +SingularityImage attribute accordingly.","title":"Running an ML job using PyTorch"},{"location":"software_examples/ai/tutorial-pytorch/#create-log-directories-and-submit-job","text":"You will need to create the logs and output directories to hold the files that will be created for each job. You can create both directories at once with the command mkdir logs output Submit the job using condor_submit pytorch_cnn.sub","title":"Create Log Directories and Submit Job"},{"location":"software_examples/ai/tutorial-pytorch/#output","text":"The output of the code will be the CNN Network that was trained. It will be returned to us as a file mnist_cnn.pt . The are also some output stats on the training and test error in the pytorch_cnn.out.txt. file Test set: Average loss: 0.0278, Accuracy: 9909/10000 (99%)","title":"Output"},{"location":"software_examples/ai/tutorial-pytorch/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the help desk and community forums .","title":"Getting help"},{"location":"software_examples/bioinformatics/tutorial-blast-split/","text":"High-Throughput BLAST \u00b6 This tutorial will put together several OSG tools and ideas - handling a larger data file, splitting a large file into smaller pieces, and transferring a portable software program. Job components and plan \u00b6 To run BLAST, we need three things: 1. the BLAST program (specifically the blastx binary) 2. a reference database (this is usually a larger file) 3. the file we want to query against the database The database and the input file will each get special treatment. The database we are using is large enough that we will want to use OSG Connect's stashcache capability (more information about that here ). The input file is large enough that a) it is near the upper limit of what is practical to transfer, b) it would take hours to complete a single blastx analysis for it, and c) the resulting output file would be huge. Because the BLAST process is run over the input file line by line, it is scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! By splitting the input query file into smaller pieces, each of the queries can be run as separate jobs. On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database. Get materials and set up files \u00b6 Run the tutorial command: tutorial blast-split Once the tutorial has downloaded, move into the folder and run the download_files.sh script to download the remaining files: cd tutorial-blast-split ./download_files.sh This command will have downloaded and unzipped the BLAST program ( ncbi-blast-2.9.0+ ), the file we want to query ( mouse_rna.fa ) and a set of tools that will split the file into smaller pieces ( gt-1.5.10-Linux_x86_64-64bit-complete ). Next, we will use the command gt from the genome tools package to split our input query file into 2 MB chunks as indicated by the -targetsize flag. To split the file, run this command: ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 2 mouse_rna.fa Later, we'll need a list of the split files, so run this command to generate that list: ls mouse_rna.fa.* > list.txt Examine the submit file \u00b6 The submit file, blast.submit looks like this: executable = run_blast.sh arguments = $(inputfile) transfer_input_files = ncbi-blast-2.9.0+/bin/blastx, $(inputfile), stash:///osgconnect/public/osg/BlastTutorial/pdbaa.tar.gz output = logs/job_$(process).out error = logs/job_$(process).err log = logs/job_$(process).log requirements = OSGVO_OS_STRING == \"RHEL 7\" && Arch == \"X86_64\" request_memory = 2GB request_disk = 1GB request_cpus = 1 queue inputfile from list.txt The executable run_blast.sh is a script that runs blast and takes in a file to query as its argument. We'll look at this script in more detail in a minute. Our job will need to transfer the blastx executable and the input file being used for queries, shown in the transfer_input_files line. Because of the size of our database, we'll be using stash:/// to transfer the database to our job. Note on stash:/// : In this job, we're copying the file from a particular /public folder ( osg/BlastTutorialV1 ), but you have your own /public folder that you could use for the database. If you wanted to try this, you would want to navigate to your /public folder, download the pdbaa.tar.gz file, return to your /home folder, and change the path in the stash:/// command above. This might look like: cd /public/username wget http://stash.osgconnect.net/public/osg/BlastTutorialV1/pdbaa.tar.gz cd /home/username Finally, you may have already noticed that instead of listing the individual input file by name, we've used the following syntax: $(inputfile) . This is a variable that represents the name of an individual input file. We've done this so that we can set the variable as a different file name for each job. We can set the variable by using the queue syntax shown at the bottom of the file: queue inputfile from list.txt This command will pull file names from the list.txt file that we created earlier, and submit one job per file and set the \"inputfile\" variable to that file name. Examine the wrapper script \u00b6 The submit file had a script called run_blast.sh : #!/bin/bash # get input file from arguments inputfile=$1 # Prepare our database and unzip into new dir tar -xzvf pdbaa.tar.gz rm pdbaa.tar.gz # run blast query on input file ./blastx -db pdbaa/pdbaa -query $inputfile -out $inputfile.result It saves the name of the input file, unpacks our database, and then runs the BLAST query from the input file we transferred and used as the argument. Submit the jobs \u00b6 Our jobs should be set and ready to go. To submit them, run this command: condor_submit blast.submit And you should see that 51 jobs have been submitted: Submitting job(s)................................................ 51 job(s) submitted to cluster 90363. You can check on your jobs' progress using condor_q Bonus: a BLAST workflow \u00b6 We had to go through multiple steps to run the jobs above. There was an initial step to split the files and generate a list of them; then we submitted the jobs. These two steps can be tied together in a workflow using the HTCondor DAGMan workflow tool. First, we would create a script ( split_files.sh ) that does the file splitting steps: #!/bin/bash filesize=$1 ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize $filesize mouse_rna.fa ls mouse_rna.fa.* > list.txt This script will need executable permissions: chmod +x split_files.sh Then, we create a DAG workflow file that ties the two steps together: ## DAG: blastrun.dag JOB blast blast.submit SCRIPT PRE blast split_files.sh 2 To submit this DAG, we use this command: condor_submit_dag blastrun.dag","title":"High-Throughput BLAST"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#high-throughput-blast","text":"This tutorial will put together several OSG tools and ideas - handling a larger data file, splitting a large file into smaller pieces, and transferring a portable software program.","title":"High-Throughput BLAST"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#job-components-and-plan","text":"To run BLAST, we need three things: 1. the BLAST program (specifically the blastx binary) 2. a reference database (this is usually a larger file) 3. the file we want to query against the database The database and the input file will each get special treatment. The database we are using is large enough that we will want to use OSG Connect's stashcache capability (more information about that here ). The input file is large enough that a) it is near the upper limit of what is practical to transfer, b) it would take hours to complete a single blastx analysis for it, and c) the resulting output file would be huge. Because the BLAST process is run over the input file line by line, it is scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! By splitting the input query file into smaller pieces, each of the queries can be run as separate jobs. On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database.","title":"Job components and plan"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#get-materials-and-set-up-files","text":"Run the tutorial command: tutorial blast-split Once the tutorial has downloaded, move into the folder and run the download_files.sh script to download the remaining files: cd tutorial-blast-split ./download_files.sh This command will have downloaded and unzipped the BLAST program ( ncbi-blast-2.9.0+ ), the file we want to query ( mouse_rna.fa ) and a set of tools that will split the file into smaller pieces ( gt-1.5.10-Linux_x86_64-64bit-complete ). Next, we will use the command gt from the genome tools package to split our input query file into 2 MB chunks as indicated by the -targetsize flag. To split the file, run this command: ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 2 mouse_rna.fa Later, we'll need a list of the split files, so run this command to generate that list: ls mouse_rna.fa.* > list.txt","title":"Get materials and set up files"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#examine-the-submit-file","text":"The submit file, blast.submit looks like this: executable = run_blast.sh arguments = $(inputfile) transfer_input_files = ncbi-blast-2.9.0+/bin/blastx, $(inputfile), stash:///osgconnect/public/osg/BlastTutorial/pdbaa.tar.gz output = logs/job_$(process).out error = logs/job_$(process).err log = logs/job_$(process).log requirements = OSGVO_OS_STRING == \"RHEL 7\" && Arch == \"X86_64\" request_memory = 2GB request_disk = 1GB request_cpus = 1 queue inputfile from list.txt The executable run_blast.sh is a script that runs blast and takes in a file to query as its argument. We'll look at this script in more detail in a minute. Our job will need to transfer the blastx executable and the input file being used for queries, shown in the transfer_input_files line. Because of the size of our database, we'll be using stash:/// to transfer the database to our job. Note on stash:/// : In this job, we're copying the file from a particular /public folder ( osg/BlastTutorialV1 ), but you have your own /public folder that you could use for the database. If you wanted to try this, you would want to navigate to your /public folder, download the pdbaa.tar.gz file, return to your /home folder, and change the path in the stash:/// command above. This might look like: cd /public/username wget http://stash.osgconnect.net/public/osg/BlastTutorialV1/pdbaa.tar.gz cd /home/username Finally, you may have already noticed that instead of listing the individual input file by name, we've used the following syntax: $(inputfile) . This is a variable that represents the name of an individual input file. We've done this so that we can set the variable as a different file name for each job. We can set the variable by using the queue syntax shown at the bottom of the file: queue inputfile from list.txt This command will pull file names from the list.txt file that we created earlier, and submit one job per file and set the \"inputfile\" variable to that file name.","title":"Examine the submit file"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#examine-the-wrapper-script","text":"The submit file had a script called run_blast.sh : #!/bin/bash # get input file from arguments inputfile=$1 # Prepare our database and unzip into new dir tar -xzvf pdbaa.tar.gz rm pdbaa.tar.gz # run blast query on input file ./blastx -db pdbaa/pdbaa -query $inputfile -out $inputfile.result It saves the name of the input file, unpacks our database, and then runs the BLAST query from the input file we transferred and used as the argument.","title":"Examine the wrapper script"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#submit-the-jobs","text":"Our jobs should be set and ready to go. To submit them, run this command: condor_submit blast.submit And you should see that 51 jobs have been submitted: Submitting job(s)................................................ 51 job(s) submitted to cluster 90363. You can check on your jobs' progress using condor_q","title":"Submit the jobs"},{"location":"software_examples/bioinformatics/tutorial-blast-split/#bonus-a-blast-workflow","text":"We had to go through multiple steps to run the jobs above. There was an initial step to split the files and generate a list of them; then we submitted the jobs. These two steps can be tied together in a workflow using the HTCondor DAGMan workflow tool. First, we would create a script ( split_files.sh ) that does the file splitting steps: #!/bin/bash filesize=$1 ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize $filesize mouse_rna.fa ls mouse_rna.fa.* > list.txt This script will need executable permissions: chmod +x split_files.sh Then, we create a DAG workflow file that ties the two steps together: ## DAG: blastrun.dag JOB blast blast.submit SCRIPT PRE blast split_files.sh 2 To submit this DAG, we use this command: condor_submit_dag blastrun.dag","title":"Bonus: a BLAST workflow"},{"location":"software_examples/bioinformatics/tutorial-bwa/","text":"High-Throughput BWA Read Mapping \u00b6 This tutorial focuses on a subset of the Data Carpentry Genomics workshop curriculum - specifically, this page cover's how to run a BWA workflow on OSG resources. It will use the same general flow as the BWA segment of the Data Carpentry workshop with minor adjustments. The goal of this tutorial is to learn how to convert an existing BWA workflow to run on the OSPool. Get Tutorial Files \u00b6 Logged into the submit node, we will run the tutorial command, that will create a folder for our analysis, as well as some sample files. tutorial bwa Install and Prepare BWA \u00b6 First, we need to install BWA, also called Burrows-Wheeler Aligner. To do this, we will create and navigate to a new folder in our /home directory called software . We will then follow the developer's instructions (https://github.com/lh3/bwa) for using git clone to clone the software and then build the tool using make . cd ~/tutorial-bwa cd software git clone https://github.com/lh3/bwa.git cd bwa make Next, BWA needs to be added to our PATH variables, to test if the installation worked: export PATH=$PATH:/home/$USER/tutorial-bwa/software/bwa/ To check that BWA has been installed correctly, type bwa . You should receive output similar to the following: Program: bwa (alignment via Burrows-Wheeler transformation) Version: 0.7.17-r1198-dirty Contact: Heng Li <hli@ds.dfci.harvard.edu> Usage: bwa <command> [options] Command: index index sequences in the FASTA format mem BWA-MEM algorithm fastmap identify super-maximal exact matches ... Now that we have successfully installed bwa , we will create a portable compressed tarball of this software so that it is smaller and quicker to transport when we submit our jobs to the OSPool. cd ~/tutorial-bwa/software tar -czvf bwa.tar.gz bwa Checking the size of this compressed tarball using ls -lh bwa.tar.gz reveals the file is approximately 4MB. The tarball should stay in /home. Download Data to Analyze \u00b6 Now that we have installed BWA, we need to download data to analyze. For this tutorial, we will be downloading data used in the Data Carpentry workshop. This data includes both the genome of Escherichia coli (E. coli) and paired-end RNA sequencing reads obtained from a study carried out by Blount et al. published in PNAS . Additional information about how the data was modified in preparation for this analysis can be found on the Data Carpentry's workshop website . cd ~/tutorial-bwa ./download_data.sh Investigating the size of the downloaded genome by typing: ls -lh data/ref_genome/ reveals the file is 1.4 MB. Therefore, this file should remain in /home and does not need to be moved to /public. We should also check the trimmed fastq paired-end read files: ls -lh data/trimmed_fastq_small Once everything is downloaded, make sure you're still in the tutorial-bwa directory. cd ~/tutorial-bwa Run a Single Test Job \u00b6 Now that we have all items in our analysis ready, it is time to submit a single test job to map our RNA reads to the E. coli genome. For a single test job, we will choose a single sample to analyze. In the following example, we will align both the forward and reverse reads of SRR2584863 to the E. coli genome. Using a text editor such as nano or vim , we can create an example submit file for this test job called bwa-test.sub containing the following information: universe = vanilla executable = bwa-test.sh # arguments = # need to transfer bwa.tar.gz file, the reference # genome, and the trimmed fastq files transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/SRR2584863_1.trim.sub.fastq, data/trimmed_fastq_small/SRR2584863_2.trim.sub.fastq should_transfer_files = YES when_to_transfer_output = ON_EXIT log = logs/bwa_test_job.log output = logs/bwa_test_job.out error = logs/bwa_test_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 1GB requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue 1 You will notice that the .log, .out, and .error files will be saved to a folder called logs . We need to create this folder using mkdir logs before we submit our job. We will call the script for this analysis bwa-test.sh and it should contain the following information: #!/bin/bash # Script name: bwa-test.sh echo \"Unpacking software\" tar -xzf bwa.tar.gz echo \"Setting PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH echo \"Indexing E. coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for SRR2584863\" bwa mem ecoli_rel606.fasta.gz SRR2584863_1.trim.sub.fastq SRR2584863_2.trim.sub.fastq > SRR2584863.aligned.sam echo \"Done with bwa alignment for SRR2584863!\" echo \"Cleaning up files generated from genome indexing\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa We can submit this single test job to HTCondor by typing: condor_submit bwa-test.sub To check the status of the job, we can use condor_q . Upon the completion of the test job, we should investigate the output to ensure that it is what we expected and also review the .log file to help optimize future resource requests in preparation for scaling up. For example, when we investigate the bwa_test_job.log file created in this analysis, at the bottom of the file we see a resource table: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 253770 1048576 27945123 Memory (MB) : 144 2048 2500 Here we see that we used less than half of both the disk space and memory we requested. In future jobs, we should request a smaller amount of each resource, such as 0.5 GB of disk space and 0.5 GB of memory. Prior to scaling up our analysis, we should run additional test jobs using these resource requests to ensure that they are sufficient to allow our job to complete successfully. Scaling Up to Analyze Multiple Samples \u00b6 In preparation for scaling up, please review our guide on how to scale up after a successful test job and how to easily submit multiple jobs with a single submit file . After reviewing how to submit multiple jobs with a single submit file, it is possible to determine that the most appropriate way to submit multiple jobs for this analysis is to use queue <var> from <list.txt> . To use this option, we first need to create a file with just the sample names/IDs that we want to analyze. To do this, we want to cut all information after the \"_\" symbol to remove the forward/reverse read information and file extensions. For example, we want SRR2584863_1.trim.sub.fastq to become just SRR2584863. We will save the sample names in a file called samples.txt : cd ~/tutorial-bwa cd data/trimmed_fastq_small/ ls *.fastq | cut -f 1 -d '_' | uniq > samples.txt cd ~/tutorial-bwa Now, we can create a new submit file called bwa-alignment.sub to queue a new job for each sample. To make it simpler to start, you can copy the bwa-test.sub file ( cp bwa-test.sub bwa-alignment.sub ) and modify it. universe = vanilla executable = bwa-alignment.sh arguments = $(sample) transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/$(sample)_1.trim.sub.fastq, data/trimmed_fastq_small/$(sample)_2.trim.sub.fastq transfer_output_remaps = \"$(sample).aligned.sam=results/$(sample).aligned.sam\" should_transfer_files = YES when_to_transfer_output = ON_EXIT log = logs/bwa_$(sample)_job.log output = logs/bwa_$(sample)_job.out error = logs/bwa_$(sample)_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 0.5GB request_disk = 0.5GB requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue sample from data/trimmed_fastq_small/samples.txt We will need to create an additional folder to store our aligned sequencing files in a folder called results : mkdir results To store the aligned sequencing files in the results folder, we can add the transfer_output_remaps feature to our submit file. This feature allows us to specify a name and a path to save our output files in the format of \"file1 = path/to/save/file2\", where file1 is the origional name of the document and file2 is the name that we want to save the file using. In the example above, we do not change the name of the resulting output files. This feature also helps us keep an organized working space, rather than having all of our resulting sequencing files be saved to our /home directory. Once our submit file has been updated, we can update our script to look like and call it something like bwa-alignment.sh : #!/bin/bash # Script name: bwa-alignment.sh echo \"Unpackage software\" tar -xzf bwa.tar.gz echo \"Set PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH # Renaming first argument SAMPLE=$1 echo \"Index E.coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for ${SAMPLE}\" bwa mem ecoli_rel606.fasta.gz ${SAMPLE}_1.trim.sub.fastq ${SAMPLE}_2.trim.sub.fastq > ${SAMPLE}.aligned.sam echo \"Done with bwa alignment for ${SAMPLE}!\" echo \"Cleaning up workspace\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa Once ready, we can submit our job to HTCondor by using condor_submit bwa-alignment.sub . When we type condor_q , we see that three jobs have entered the queue (one for each of our three experimental samples). When our jobs are completed, we can confirm that our alignment output results files were created by typing: ls -lh results/* We can also investigate our log, error, and output files in the logs folder to ensure we obtained the resulting output of these files that we expected. For more information about running bioinformatics workflows on the OSG, we recommend our BLAST tutorial as well as our Samtools instillation guide.","title":"High-Throughput BWA Read Mapping"},{"location":"software_examples/bioinformatics/tutorial-bwa/#high-throughput-bwa-read-mapping","text":"This tutorial focuses on a subset of the Data Carpentry Genomics workshop curriculum - specifically, this page cover's how to run a BWA workflow on OSG resources. It will use the same general flow as the BWA segment of the Data Carpentry workshop with minor adjustments. The goal of this tutorial is to learn how to convert an existing BWA workflow to run on the OSPool.","title":"High-Throughput BWA Read Mapping"},{"location":"software_examples/bioinformatics/tutorial-bwa/#get-tutorial-files","text":"Logged into the submit node, we will run the tutorial command, that will create a folder for our analysis, as well as some sample files. tutorial bwa","title":"Get Tutorial Files"},{"location":"software_examples/bioinformatics/tutorial-bwa/#install-and-prepare-bwa","text":"First, we need to install BWA, also called Burrows-Wheeler Aligner. To do this, we will create and navigate to a new folder in our /home directory called software . We will then follow the developer's instructions (https://github.com/lh3/bwa) for using git clone to clone the software and then build the tool using make . cd ~/tutorial-bwa cd software git clone https://github.com/lh3/bwa.git cd bwa make Next, BWA needs to be added to our PATH variables, to test if the installation worked: export PATH=$PATH:/home/$USER/tutorial-bwa/software/bwa/ To check that BWA has been installed correctly, type bwa . You should receive output similar to the following: Program: bwa (alignment via Burrows-Wheeler transformation) Version: 0.7.17-r1198-dirty Contact: Heng Li <hli@ds.dfci.harvard.edu> Usage: bwa <command> [options] Command: index index sequences in the FASTA format mem BWA-MEM algorithm fastmap identify super-maximal exact matches ... Now that we have successfully installed bwa , we will create a portable compressed tarball of this software so that it is smaller and quicker to transport when we submit our jobs to the OSPool. cd ~/tutorial-bwa/software tar -czvf bwa.tar.gz bwa Checking the size of this compressed tarball using ls -lh bwa.tar.gz reveals the file is approximately 4MB. The tarball should stay in /home.","title":"Install and Prepare BWA"},{"location":"software_examples/bioinformatics/tutorial-bwa/#download-data-to-analyze","text":"Now that we have installed BWA, we need to download data to analyze. For this tutorial, we will be downloading data used in the Data Carpentry workshop. This data includes both the genome of Escherichia coli (E. coli) and paired-end RNA sequencing reads obtained from a study carried out by Blount et al. published in PNAS . Additional information about how the data was modified in preparation for this analysis can be found on the Data Carpentry's workshop website . cd ~/tutorial-bwa ./download_data.sh Investigating the size of the downloaded genome by typing: ls -lh data/ref_genome/ reveals the file is 1.4 MB. Therefore, this file should remain in /home and does not need to be moved to /public. We should also check the trimmed fastq paired-end read files: ls -lh data/trimmed_fastq_small Once everything is downloaded, make sure you're still in the tutorial-bwa directory. cd ~/tutorial-bwa","title":"Download Data to Analyze"},{"location":"software_examples/bioinformatics/tutorial-bwa/#run-a-single-test-job","text":"Now that we have all items in our analysis ready, it is time to submit a single test job to map our RNA reads to the E. coli genome. For a single test job, we will choose a single sample to analyze. In the following example, we will align both the forward and reverse reads of SRR2584863 to the E. coli genome. Using a text editor such as nano or vim , we can create an example submit file for this test job called bwa-test.sub containing the following information: universe = vanilla executable = bwa-test.sh # arguments = # need to transfer bwa.tar.gz file, the reference # genome, and the trimmed fastq files transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/SRR2584863_1.trim.sub.fastq, data/trimmed_fastq_small/SRR2584863_2.trim.sub.fastq should_transfer_files = YES when_to_transfer_output = ON_EXIT log = logs/bwa_test_job.log output = logs/bwa_test_job.out error = logs/bwa_test_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 1GB requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue 1 You will notice that the .log, .out, and .error files will be saved to a folder called logs . We need to create this folder using mkdir logs before we submit our job. We will call the script for this analysis bwa-test.sh and it should contain the following information: #!/bin/bash # Script name: bwa-test.sh echo \"Unpacking software\" tar -xzf bwa.tar.gz echo \"Setting PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH echo \"Indexing E. coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for SRR2584863\" bwa mem ecoli_rel606.fasta.gz SRR2584863_1.trim.sub.fastq SRR2584863_2.trim.sub.fastq > SRR2584863.aligned.sam echo \"Done with bwa alignment for SRR2584863!\" echo \"Cleaning up files generated from genome indexing\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa We can submit this single test job to HTCondor by typing: condor_submit bwa-test.sub To check the status of the job, we can use condor_q . Upon the completion of the test job, we should investigate the output to ensure that it is what we expected and also review the .log file to help optimize future resource requests in preparation for scaling up. For example, when we investigate the bwa_test_job.log file created in this analysis, at the bottom of the file we see a resource table: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 253770 1048576 27945123 Memory (MB) : 144 2048 2500 Here we see that we used less than half of both the disk space and memory we requested. In future jobs, we should request a smaller amount of each resource, such as 0.5 GB of disk space and 0.5 GB of memory. Prior to scaling up our analysis, we should run additional test jobs using these resource requests to ensure that they are sufficient to allow our job to complete successfully.","title":"Run a Single Test Job"},{"location":"software_examples/bioinformatics/tutorial-bwa/#scaling-up-to-analyze-multiple-samples","text":"In preparation for scaling up, please review our guide on how to scale up after a successful test job and how to easily submit multiple jobs with a single submit file . After reviewing how to submit multiple jobs with a single submit file, it is possible to determine that the most appropriate way to submit multiple jobs for this analysis is to use queue <var> from <list.txt> . To use this option, we first need to create a file with just the sample names/IDs that we want to analyze. To do this, we want to cut all information after the \"_\" symbol to remove the forward/reverse read information and file extensions. For example, we want SRR2584863_1.trim.sub.fastq to become just SRR2584863. We will save the sample names in a file called samples.txt : cd ~/tutorial-bwa cd data/trimmed_fastq_small/ ls *.fastq | cut -f 1 -d '_' | uniq > samples.txt cd ~/tutorial-bwa Now, we can create a new submit file called bwa-alignment.sub to queue a new job for each sample. To make it simpler to start, you can copy the bwa-test.sub file ( cp bwa-test.sub bwa-alignment.sub ) and modify it. universe = vanilla executable = bwa-alignment.sh arguments = $(sample) transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/$(sample)_1.trim.sub.fastq, data/trimmed_fastq_small/$(sample)_2.trim.sub.fastq transfer_output_remaps = \"$(sample).aligned.sam=results/$(sample).aligned.sam\" should_transfer_files = YES when_to_transfer_output = ON_EXIT log = logs/bwa_$(sample)_job.log output = logs/bwa_$(sample)_job.out error = logs/bwa_$(sample)_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 0.5GB request_disk = 0.5GB requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue sample from data/trimmed_fastq_small/samples.txt We will need to create an additional folder to store our aligned sequencing files in a folder called results : mkdir results To store the aligned sequencing files in the results folder, we can add the transfer_output_remaps feature to our submit file. This feature allows us to specify a name and a path to save our output files in the format of \"file1 = path/to/save/file2\", where file1 is the origional name of the document and file2 is the name that we want to save the file using. In the example above, we do not change the name of the resulting output files. This feature also helps us keep an organized working space, rather than having all of our resulting sequencing files be saved to our /home directory. Once our submit file has been updated, we can update our script to look like and call it something like bwa-alignment.sh : #!/bin/bash # Script name: bwa-alignment.sh echo \"Unpackage software\" tar -xzf bwa.tar.gz echo \"Set PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH # Renaming first argument SAMPLE=$1 echo \"Index E.coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for ${SAMPLE}\" bwa mem ecoli_rel606.fasta.gz ${SAMPLE}_1.trim.sub.fastq ${SAMPLE}_2.trim.sub.fastq > ${SAMPLE}.aligned.sam echo \"Done with bwa alignment for ${SAMPLE}!\" echo \"Cleaning up workspace\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa Once ready, we can submit our job to HTCondor by using condor_submit bwa-alignment.sub . When we type condor_q , we see that three jobs have entered the queue (one for each of our three experimental samples). When our jobs are completed, we can confirm that our alignment output results files were created by typing: ls -lh results/* We can also investigate our log, error, and output files in the logs folder to ensure we obtained the resulting output of these files that we expected. For more information about running bioinformatics workflows on the OSG, we recommend our BLAST tutorial as well as our Samtools instillation guide.","title":"Scaling Up to Analyze Multiple Samples"},{"location":"software_examples/bioinformatics/tutorial-fastqc/","text":"Bioinformatics Tutorial: Quality Assessment of Data with FastQC \u00b6 The first step of most biofinformatic analyses is to assess the quality of the data you have recieved. In this example, we are working with real DNA sequencing data from a research project studying E. coli. We will use a common software, FastQC , to assess the quality of the data. Before we start, let us download the materials for this tutorial if we don't already have them: git clone https://github.com/OSGConnect/tutorial-fastqc Then let's navigate inside the tutorial-fastqc directory: cd ~/tutorial-fastqc We can confirm our location by printing our working directory using pwd : pwd We should see /home/<username>/tutorial-fastqc . Step 1: Download data \u00b6 First, we need to download our sequencing data to that we want to analyze for our research project. For this tutorial, we will be downloading data used in the Data Carpentry workshop. This data includes both the genome of Escherichia coli (E. coli) and paired-end RNA sequencing reads obtained from a study carried out by Blount et al. published in PNAS . Additional information about how the data was modified in preparation for this analysis can be found on the Data Carpentry's workshop website . We have a script called download_data.sh that will download our bioinformatic data. Let's go ahead and run this script to download our data. ./download_data.sh Our sequencing data files, all ending in .fastq, can now be found in a folder called /data. Step 2: Prepare software \u00b6 Now that we have our data, we need to install the software we want to use to analyze it. There are different ways to install and use software, including installing from source, using pre-compiled binaries, and containers. In the biology domains, many software packages are already available as pre-built containers. We can fetch one of these containers and have HTCondor set it up for our job, which means we do not have to install the FastQC software or it's dependencies. We will use a Docker container built by the State Public Health Bioinformatics Community (staphb), and convert it to an apptainer container by creating an apptainer definition file: ls software/ cat software/fastqc.def And then running a command to build an apptainer container (which we won't run, but is listed here for future reference): $ apptainer build fastqc.sif software/fastqc.def Instead, we will download our ready-to-go apptainer .sif file: ./download_software.sh ls software/ Step 3: Prepare an Executable \u00b6 We need to create an executable to pass to our HTCondor jobs, so that HTCondor knows what to run on our behalf. Let's take a look at our executable, fastqc.sh : cat fastqc.sh Step 4: Prepare HTCondor Submit File to Run One Job \u00b6 Now we create our HTCondor submit file, which tells HTCondor what to run and how many resources to make available to our job: cat fastqc.submit Step 5: Submit One HTCondor Job and Check Results \u00b6 We are ready to submit our first job! condor_submit fastqc.submit We can check on the status of our job in HTCondor's queue using: condor_q By using transfer_output_remaps in our submit file, we told HTCondor to store our FastQC output files in the results directory. Let's take a look at our scientific results: ls results/ It's always good practice to look at our standard error, standard out, and HTCondor log files to catch unexpected output: ls logs/ Step 6: Scale Out Your Analysis \u00b6 Create A List of All Files We Want Analyzed \u00b6 To queue a job to analyze each of our sequencing data files, we will take advantage of HTCondor's queue statement. First, let's create a list of files we want analyzed: ls data/ | cut -f1 -d \".\" > list_of_samples.txt Let us take a look at the contents of this file: cat list_of_samples.txt Edits the Submit File to Queue a Job to Analyze Each Biological Sample HTCondor has different queue syntaxes to help researchers automatically queue many jobs. We will use queue <variable> from <list.txt> to queue a job for each of of our samples in list_of_samples.txt . Once we define <variable> , we can also use it elsewhere in the submit file. Let's replace each occurence of the sample identifier with the $(sample) variable, and then iterating through our list of samples as shown in list_of_samples.txt . cat many-fastqc.submit # HTCondor Submit File: fastqc.submit # Provide our executable and arguments executable = fastqc.sh arguments = $(sample).trim.sub.fastq # Provide the container for our software universe = container container_image = software/fastqc.sif # List files that need to be transferred to the job transfer_input_files = data/$(sample).trim.sub.fastq should_transfer_files = YES # Tell HTCondor to transfer output to our /results directory transfer_output_files = $(sample).trim.sub_fastqc.html transfer_output_remaps = \"$(sample).trim.sub_fastqc.html = results/$(sample).trim.sub_fastqc.html\" # Track job information log = logs/fastqc.log output = logs/fastqc.out error = logs/fastqc.err # Resource Requests request_cpus = 1 request_memory = 1GB request_disk = 1GB # Tell HTCondor to run our job once: queue sample from list_of_samples.txt And then submit many jobs using this single submit file! condor_submit many-fastqc.submit Notice that using a single submit file , we now have multiple jobs in the queue . We can check on the status of our multiple jobs in HTCondor's queue by using: condor_q When ready, we can check our results in our results/ directory: ls results/ Step 7: Return the output to your local computer \u00b6 Once you are done with your computational analysis, you will want to move the results to your local computer or to a long term storage location. Let's practice copying our .html files to our local laptop. First, open a new terminal. Do not log into your OSPool account. Instead, navigate to where you want the files to go on your computer. We will store them in our Downloads folder. cd ~/Downloads Then use scp (\"secure copy\") command to copy our results folder and it's contents: scp -r username@hostname:/home/username/tutorial-fastqc/results ./ For many files, it will be easiest to create a compressed tarball (.tar.gz file) of your files and transfer that instead of each file individually. An example of this could be scp -r username@ap40.uw.osg-htc.org:/home/username/results ./ Now, open the .html files using your internet browser on your local computer. Congratulations on finishing the first step of a sequencing analysis pipeline! \u00b6","title":"FastQC Quality Control"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#bioinformatics-tutorial-quality-assessment-of-data-with-fastqc","text":"The first step of most biofinformatic analyses is to assess the quality of the data you have recieved. In this example, we are working with real DNA sequencing data from a research project studying E. coli. We will use a common software, FastQC , to assess the quality of the data. Before we start, let us download the materials for this tutorial if we don't already have them: git clone https://github.com/OSGConnect/tutorial-fastqc Then let's navigate inside the tutorial-fastqc directory: cd ~/tutorial-fastqc We can confirm our location by printing our working directory using pwd : pwd We should see /home/<username>/tutorial-fastqc .","title":"Bioinformatics Tutorial: Quality Assessment of Data with FastQC"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-1-download-data","text":"First, we need to download our sequencing data to that we want to analyze for our research project. For this tutorial, we will be downloading data used in the Data Carpentry workshop. This data includes both the genome of Escherichia coli (E. coli) and paired-end RNA sequencing reads obtained from a study carried out by Blount et al. published in PNAS . Additional information about how the data was modified in preparation for this analysis can be found on the Data Carpentry's workshop website . We have a script called download_data.sh that will download our bioinformatic data. Let's go ahead and run this script to download our data. ./download_data.sh Our sequencing data files, all ending in .fastq, can now be found in a folder called /data.","title":"Step 1: Download data"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-2-prepare-software","text":"Now that we have our data, we need to install the software we want to use to analyze it. There are different ways to install and use software, including installing from source, using pre-compiled binaries, and containers. In the biology domains, many software packages are already available as pre-built containers. We can fetch one of these containers and have HTCondor set it up for our job, which means we do not have to install the FastQC software or it's dependencies. We will use a Docker container built by the State Public Health Bioinformatics Community (staphb), and convert it to an apptainer container by creating an apptainer definition file: ls software/ cat software/fastqc.def And then running a command to build an apptainer container (which we won't run, but is listed here for future reference): $ apptainer build fastqc.sif software/fastqc.def Instead, we will download our ready-to-go apptainer .sif file: ./download_software.sh ls software/","title":"Step 2: Prepare software"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-3-prepare-an-executable","text":"We need to create an executable to pass to our HTCondor jobs, so that HTCondor knows what to run on our behalf. Let's take a look at our executable, fastqc.sh : cat fastqc.sh","title":"Step 3: Prepare an Executable"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-4-prepare-htcondor-submit-file-to-run-one-job","text":"Now we create our HTCondor submit file, which tells HTCondor what to run and how many resources to make available to our job: cat fastqc.submit","title":"Step 4: Prepare HTCondor Submit File to Run One Job"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-5-submit-one-htcondor-job-and-check-results","text":"We are ready to submit our first job! condor_submit fastqc.submit We can check on the status of our job in HTCondor's queue using: condor_q By using transfer_output_remaps in our submit file, we told HTCondor to store our FastQC output files in the results directory. Let's take a look at our scientific results: ls results/ It's always good practice to look at our standard error, standard out, and HTCondor log files to catch unexpected output: ls logs/","title":"Step 5: Submit One HTCondor Job and Check Results"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-6-scale-out-your-analysis","text":"","title":"Step 6: Scale Out Your Analysis"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#create-a-list-of-all-files-we-want-analyzed","text":"To queue a job to analyze each of our sequencing data files, we will take advantage of HTCondor's queue statement. First, let's create a list of files we want analyzed: ls data/ | cut -f1 -d \".\" > list_of_samples.txt Let us take a look at the contents of this file: cat list_of_samples.txt Edits the Submit File to Queue a Job to Analyze Each Biological Sample HTCondor has different queue syntaxes to help researchers automatically queue many jobs. We will use queue <variable> from <list.txt> to queue a job for each of of our samples in list_of_samples.txt . Once we define <variable> , we can also use it elsewhere in the submit file. Let's replace each occurence of the sample identifier with the $(sample) variable, and then iterating through our list of samples as shown in list_of_samples.txt . cat many-fastqc.submit # HTCondor Submit File: fastqc.submit # Provide our executable and arguments executable = fastqc.sh arguments = $(sample).trim.sub.fastq # Provide the container for our software universe = container container_image = software/fastqc.sif # List files that need to be transferred to the job transfer_input_files = data/$(sample).trim.sub.fastq should_transfer_files = YES # Tell HTCondor to transfer output to our /results directory transfer_output_files = $(sample).trim.sub_fastqc.html transfer_output_remaps = \"$(sample).trim.sub_fastqc.html = results/$(sample).trim.sub_fastqc.html\" # Track job information log = logs/fastqc.log output = logs/fastqc.out error = logs/fastqc.err # Resource Requests request_cpus = 1 request_memory = 1GB request_disk = 1GB # Tell HTCondor to run our job once: queue sample from list_of_samples.txt And then submit many jobs using this single submit file! condor_submit many-fastqc.submit Notice that using a single submit file , we now have multiple jobs in the queue . We can check on the status of our multiple jobs in HTCondor's queue by using: condor_q When ready, we can check our results in our results/ directory: ls results/","title":"Create A List of All Files We Want Analyzed"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#step-7-return-the-output-to-your-local-computer","text":"Once you are done with your computational analysis, you will want to move the results to your local computer or to a long term storage location. Let's practice copying our .html files to our local laptop. First, open a new terminal. Do not log into your OSPool account. Instead, navigate to where you want the files to go on your computer. We will store them in our Downloads folder. cd ~/Downloads Then use scp (\"secure copy\") command to copy our results folder and it's contents: scp -r username@hostname:/home/username/tutorial-fastqc/results ./ For many files, it will be easiest to create a compressed tarball (.tar.gz file) of your files and transfer that instead of each file individually. An example of this could be scp -r username@ap40.uw.osg-htc.org:/home/username/results ./ Now, open the .html files using your internet browser on your local computer.","title":"Step 7: Return the output to your local computer"},{"location":"software_examples/bioinformatics/tutorial-fastqc/#congratulations-on-finishing-the-first-step-of-a-sequencing-analysis-pipeline","text":"","title":"Congratulations on finishing the first step of a sequencing analysis pipeline!"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/","text":"Running a Molecule Docking Job with AutoDock Vina \u00b6 AutoDock Vina is a molecular docking program useful for computer aided drug design. In this tutorial, we will learn how to run AutoDock Vina on the OSPool. Tutorial Files \u00b6 It is easiest to start with the git clone command to download the materials for this tutorial. Type: $ git clone https://github.com/OSGConnect/tutorial-AutoDockVina This will create a directory tutorial-AutoDockVina . Change into the directory and look at the available files: $ cd tutorial-AutoDockVina $ ls $ ls data/ You should see the following: data/ receptor_config.txt # Configuration file (input) receptor.pdbqt # Receptor coordinates and atomic charges (input) ligand.pdbqt # Ligand coordinates and atomic charges (input) logs/ # Empty folder for job log files vina_job.submit # Job submission file vina_run.sh # Execution script We need to download the AutoDock program separately into the this directory as well. Go to the AutoDock Vina website and click on the Download link at the top of the page. This will then lead you to the GitHub Downloads page . Download the Linux x86_64 version of the program; you can do this directly to the current directory by using the wget command and the download link. If you use the -O option shown below, it will rename the program to match what is used in the rest of the guide. $ wget https://github.com/ccsb-scripps/AutoDock-Vina/releases/download/v1.2.5/vina_1.2.5_linux_x86_64 -O vina Once downloaded, we also need to give the program executable permissions. We can test that it worked by running vina with the help flag: $ chmod +x vina $ ./vina --help Files Need to Submit the Job \u00b6 The file vina_job.submit is the job submission file and contains the description of the job in HTCondor language. Specifically, it includes an \"executable\" (the script HTCondor will use in the job to run vina), a list of the files needed to run the job (shown in \"transfer_input_files\"), and indications of where to write logging information and what resources and requirements the job needs. Change needed: If your downloaded program file has a different name, change the name in the transfer_input_files line below. executable = vina_run.sh transfer_input_files = data/, vina should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = logs/job.$(Cluster).$(Process).out error = logs/job.$(Cluster).$(Process).error log = logs/job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1GB request_disk = 512MB queue 1 Next we see the execution script vina_run.sh . The execution script and its commands are executed on a worker node out in the Open Science Pool. Change needed: If your vina program file has a different name, change it in the script below: #!/bin/bash # Run vina ./vina --config receptor_config.txt \\ --ligand ligand.pdbqt --out receptor-ligand.pdbqt Submit the Docking Job \u00b6 We submit the job using condor_submit command as follows $ condor_submit vina_job.submit Now you have submitted the AutoDock Vina job on the OSPool. The present job should be finished quickly (less than 10 mins). You can check the status of the submitted job by using the condor_q command as follows: $ condor_q After job completion, you will see the output file receptor-ligand.pdbqt . Next Steps \u00b6 After running this example, you may want to scale up to testing multiple molecules or ligands. What to Consider \u00b6 Decide how many docking runs you want to try per job. If one molecule can be tested in a few seconds, you can probably run a few hundred in a job that runs in about an hour. How should you divide up the input data in this case? Do you need individual input files for each molecule, or can you use one to share? Should the molecule files all get copied to every job or just the jobs where they're needed? You can separate groups of files by putting them in separate directories or tar.gz files to help with this. Look at this guide to see different ways that you can use HTCondor to submit multiple jobs at once. If you want to use a different (or additional) docking programs, you can include them in the same job by downloading and including those software files in your job submission. Example of Multiple Runs \u00b6 Included in this directory is one approach to analyzing multiple ligands, by submitting multiple jobs. For the given files we are assuming that there are multiple directories with input files we want to run ( run01 , run02 , run03 , etc.) and each job will process all of the ligands in one of these \"run\" folders. In the script, vina_multi.sh , we had added a for loop in order to process all the ligands that were included with the job. We will also place those results into a single folder to make it easier to organize them back on the access point: #!/bin/bash # Make a directory for results mkdir results # Run vina on multiple ligands for LIGAND in *ligand.pdbqt do ./vina --config receptor_config.txt \\ --ligand ${LIGAND} --out results/receptor-${LIGAND} done Note that this for loop assumes that all of the ligands have a naming scheme that we can match using a wildcard (the * symbol). In the submit file, we have added a line called transfer_output_files to transfer back the results folder from each job. We have also replaced the single input directory data with a variable inputdir , representing one of the run directories. The value of that variable is set via the queue statement at the end of the submit file: executable = vina_multi.sh transfer_input_files = $(inputdir)/, vina transfer_output_files = results # ... other job options queue inputdir matching run* Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the help desk and community forums .","title":"Running a Molecule Docking Job with AutoDock Vina"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#running-a-molecule-docking-job-with-autodock-vina","text":"AutoDock Vina is a molecular docking program useful for computer aided drug design. In this tutorial, we will learn how to run AutoDock Vina on the OSPool.","title":"Running a Molecule Docking Job with AutoDock Vina"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#tutorial-files","text":"It is easiest to start with the git clone command to download the materials for this tutorial. Type: $ git clone https://github.com/OSGConnect/tutorial-AutoDockVina This will create a directory tutorial-AutoDockVina . Change into the directory and look at the available files: $ cd tutorial-AutoDockVina $ ls $ ls data/ You should see the following: data/ receptor_config.txt # Configuration file (input) receptor.pdbqt # Receptor coordinates and atomic charges (input) ligand.pdbqt # Ligand coordinates and atomic charges (input) logs/ # Empty folder for job log files vina_job.submit # Job submission file vina_run.sh # Execution script We need to download the AutoDock program separately into the this directory as well. Go to the AutoDock Vina website and click on the Download link at the top of the page. This will then lead you to the GitHub Downloads page . Download the Linux x86_64 version of the program; you can do this directly to the current directory by using the wget command and the download link. If you use the -O option shown below, it will rename the program to match what is used in the rest of the guide. $ wget https://github.com/ccsb-scripps/AutoDock-Vina/releases/download/v1.2.5/vina_1.2.5_linux_x86_64 -O vina Once downloaded, we also need to give the program executable permissions. We can test that it worked by running vina with the help flag: $ chmod +x vina $ ./vina --help","title":"Tutorial Files"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#files-need-to-submit-the-job","text":"The file vina_job.submit is the job submission file and contains the description of the job in HTCondor language. Specifically, it includes an \"executable\" (the script HTCondor will use in the job to run vina), a list of the files needed to run the job (shown in \"transfer_input_files\"), and indications of where to write logging information and what resources and requirements the job needs. Change needed: If your downloaded program file has a different name, change the name in the transfer_input_files line below. executable = vina_run.sh transfer_input_files = data/, vina should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = logs/job.$(Cluster).$(Process).out error = logs/job.$(Cluster).$(Process).error log = logs/job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1GB request_disk = 512MB queue 1 Next we see the execution script vina_run.sh . The execution script and its commands are executed on a worker node out in the Open Science Pool. Change needed: If your vina program file has a different name, change it in the script below: #!/bin/bash # Run vina ./vina --config receptor_config.txt \\ --ligand ligand.pdbqt --out receptor-ligand.pdbqt","title":"Files Need to Submit the Job"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#submit-the-docking-job","text":"We submit the job using condor_submit command as follows $ condor_submit vina_job.submit Now you have submitted the AutoDock Vina job on the OSPool. The present job should be finished quickly (less than 10 mins). You can check the status of the submitted job by using the condor_q command as follows: $ condor_q After job completion, you will see the output file receptor-ligand.pdbqt .","title":"Submit the Docking Job"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#next-steps","text":"After running this example, you may want to scale up to testing multiple molecules or ligands.","title":"Next Steps"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#what-to-consider","text":"Decide how many docking runs you want to try per job. If one molecule can be tested in a few seconds, you can probably run a few hundred in a job that runs in about an hour. How should you divide up the input data in this case? Do you need individual input files for each molecule, or can you use one to share? Should the molecule files all get copied to every job or just the jobs where they're needed? You can separate groups of files by putting them in separate directories or tar.gz files to help with this. Look at this guide to see different ways that you can use HTCondor to submit multiple jobs at once. If you want to use a different (or additional) docking programs, you can include them in the same job by downloading and including those software files in your job submission.","title":"What to Consider"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#example-of-multiple-runs","text":"Included in this directory is one approach to analyzing multiple ligands, by submitting multiple jobs. For the given files we are assuming that there are multiple directories with input files we want to run ( run01 , run02 , run03 , etc.) and each job will process all of the ligands in one of these \"run\" folders. In the script, vina_multi.sh , we had added a for loop in order to process all the ligands that were included with the job. We will also place those results into a single folder to make it easier to organize them back on the access point: #!/bin/bash # Make a directory for results mkdir results # Run vina on multiple ligands for LIGAND in *ligand.pdbqt do ./vina --config receptor_config.txt \\ --ligand ${LIGAND} --out results/receptor-${LIGAND} done Note that this for loop assumes that all of the ligands have a naming scheme that we can match using a wildcard (the * symbol). In the submit file, we have added a line called transfer_output_files to transfer back the results folder from each job. We have also replaced the single input directory data with a variable inputdir , representing one of the run directories. The value of that variable is set via the queue statement at the end of the submit file: executable = vina_multi.sh transfer_input_files = $(inputdir)/, vina transfer_output_files = results # ... other job options queue inputdir matching run*","title":"Example of Multiple Runs"},{"location":"software_examples/drug_discovery/tutorial-AutoDockVina/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"software_examples/freesurfer/Introduction/","text":"FreeSurfer \u00b6 Overview \u00b6 FreeSurfer is a software package to analyze MRI scans of human brains. OSG used to have a hosted service, called Fsurf. This is no longer available. Instead, OSG provides a container image, and one of our collaborators provides an optional workflow using that container. Container image: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest and defined at https://github.com/opensciencegrid/osgvo-freesurfer FreeSurfer Workflow The container can be used with simple jobs as described below. Prerequisites \u00b6 To use the FreeSurfer on the Open Science Pool (OSPool), you need: Your own FreeSurfer license file (see: https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License ) An account on an OSPool access point. Privacy and Confidentiality of Subjects \u00b6 In order to protect the privacy of your participants\u2019 scans, we require that you submit only defaced and fully deidentified scans for processing . Single Job \u00b6 The following example job has three files: job.submit , freesurfer-wrapper.sh and license.txt job.submit contents: Requirements = HAS_SINGULARITY == True +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest\" executable = freesurfer-wrapper.sh transfer_input_files = license.txt, sub-THP0001_ses-THP0001UCI1_run-01_T1w.nii.gz error = job.$(Cluster).$(Process).error output = job.$(Cluster).$(Process).output log = job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1 GB request_disk = 4 GB queue 1 freesurfer-wrapper.sh contents: #!/bin/bash set -e # freesurfer environment . /opt/setup.sh # license file comes with the job export FS_LICENSE=`pwd`/license.txt export SUBJECTS_DIR=$PWD recon-all -subject THP0001 -i sub-THP0001_ses-THP0001UCI1_run-01_T1w.nii.gz -autorecon1 -cw256 # tar up the subjects directory so it gets transferred back tar czf THP0001.tar.gz THP0001 rm -rf THP0001 license.txt should have the license data obtained from the Freesurfer project.","title":"FreeSurfer"},{"location":"software_examples/freesurfer/Introduction/#freesurfer","text":"","title":"FreeSurfer"},{"location":"software_examples/freesurfer/Introduction/#overview","text":"FreeSurfer is a software package to analyze MRI scans of human brains. OSG used to have a hosted service, called Fsurf. This is no longer available. Instead, OSG provides a container image, and one of our collaborators provides an optional workflow using that container. Container image: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest and defined at https://github.com/opensciencegrid/osgvo-freesurfer FreeSurfer Workflow The container can be used with simple jobs as described below.","title":"Overview"},{"location":"software_examples/freesurfer/Introduction/#prerequisites","text":"To use the FreeSurfer on the Open Science Pool (OSPool), you need: Your own FreeSurfer license file (see: https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License ) An account on an OSPool access point.","title":"Prerequisites"},{"location":"software_examples/freesurfer/Introduction/#privacy-and-confidentiality-of-subjects","text":"In order to protect the privacy of your participants\u2019 scans, we require that you submit only defaced and fully deidentified scans for processing .","title":"Privacy and Confidentiality of Subjects"},{"location":"software_examples/freesurfer/Introduction/#single-job","text":"The following example job has three files: job.submit , freesurfer-wrapper.sh and license.txt job.submit contents: Requirements = HAS_SINGULARITY == True +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest\" executable = freesurfer-wrapper.sh transfer_input_files = license.txt, sub-THP0001_ses-THP0001UCI1_run-01_T1w.nii.gz error = job.$(Cluster).$(Process).error output = job.$(Cluster).$(Process).output log = job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1 GB request_disk = 4 GB queue 1 freesurfer-wrapper.sh contents: #!/bin/bash set -e # freesurfer environment . /opt/setup.sh # license file comes with the job export FS_LICENSE=`pwd`/license.txt export SUBJECTS_DIR=$PWD recon-all -subject THP0001 -i sub-THP0001_ses-THP0001UCI1_run-01_T1w.nii.gz -autorecon1 -cw256 # tar up the subjects directory so it gets transferred back tar czf THP0001.tar.gz THP0001 rm -rf THP0001 license.txt should have the license data obtained from the Freesurfer project.","title":"Single Job"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/","text":"Working with Tensorflow, GPUs, and containers \u00b6 In this tutorial, we explore GPUs and containers on OSG, using the popular Tensorflow sofware package. Tensorflow is a good example here as the software is too complex to bundle up and ship with your job. Containers solve this problem by defining a full OS image, containing not only the complex software package, but dependencies and environment configuration as well. https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. Defining container images \u00b6 Defining containers is fully described in the Docker and Singularity Containers section. Here we will just provide an overview of how you could take something like an existing Tensorflow image provided by OSG staff, and extend it by adding your own modules to it. Let's assume you like Tensorflow version 2.3. The definition of this image can be found in Github: Dockerfile . You don't really need to understand how an image was built in order to use it. As described in the containers documentation, make sure the HTCondor submit file has: Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" If you want to extend an existing image, you can just inherit from the parent image available on DockerHub here . For example, if you just need some additional Python packages, your new Dockerfile could look like: FROM opensciencegrid/tensorflow:2.3 RUN python3 -m pip install some_package_name You can then docker build and docker push it so that your new image is available on DockerHub. Note that OSG does not provide any infrastructure for these steps. You will have to complete them on your own computer or using the DockerHub build infrastructure. Adding a container to the OSG CVMFS distribution mechanism \u00b6 How to add a container image to the OSG CVMFS distribution mechanism is also described in Docker and Singularity Containers , but a quick scan of the cvmfs-singularity-sync and specifically the docker_images.txt file show us that the tensorflow images are listed as: opensciencegrid/tensorflow:* opensciencegrid/tensorflow-gpu:* Those two lines means that all tags from those two DockerHub repositories should be mapped to /cvmfs/singularity.opensciencegrid.org/ . On the login node, try running: ls /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ This is the image in its expanded form - something we can execute with Singularity! Testing the container on the submit host \u00b6 First, download the files contained in this tutorial to the login node using the git clone command and cd into the tutorial directory that is created: git clone https://github.com/OSGConnect/tutorial-tensorflow-containers cd tutorial-tensorflow-containers Before submitting jobs to the OSG, it is always a good idea to test your code so that you understand runtime requirements. The containers can be tested on the OSGConnect submit hosts with singularity shell , which will drop you into a container and let you exlore it interactively. To explore the Tensorflow 2.3 image, run: singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ Note how the command line prompt changes, providing you an indicator that you are inside the image. You can exit any time by running exit . Another important thing to note is that your $HOME directory is automatically mounted inside the interactive container - allowing you to access your codes and test it out. First, start with a simple python3 import test to make sure tensorflow is available: $ python3 Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tensorflow 2021-01-15 17:32:33.901607: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:32:33.901735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. >>> Tensorflow will warn you that no GPUs where found. This is expected as we do not have GPUs attached to our login nodes, and it is fine as Tensorflow works fine with regular CPUs (slower of course). Exit out of Python3 with CTRL+D and then we can run a Tensorflow testcode which can be found in this tutorial: $ python3 test.py 2021-01-15 17:37:43.152892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:43.153021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2021-01-15 17:37:44.899967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:44.900063: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303) 2021-01-15 17:37:44.900130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login05.osgconnect.net): /proc/driver/nvidia/version does not exist 2021-01-15 17:37:44.900821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-01-15 17:37:44.912483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz 2021-01-15 17:37:44.915548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-01-15 17:37:44.915645: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-01-15 17:37:44.921895: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) We will again see a bunch of warnings regarding GPUs not being available, but as we can see by the /job:localhost/replica:0/task:0/device:CPU:0 line, the code ran on one of the CPUs. When testing your own code like this, take note of how much memory, disk and runtime is required - it is needed in the next step. Once you are done with testing, use CTRL+D or run exit to exit out of the container. Note that you can not submit jobs from within the container. Running a CPU job \u00b6 If Tensorflow can run on GPUs, you might be wondering why we might want to run it on slower CPUs? One reason is that CPUs are plentiful while GPUs are still somewhat scarce. If you have a lot of shorter Tensorflow jobs, they might complete faster on available CPUs, rather than wait in the queue for the faster, less available, GPUs. The good news is that Tensorflow code should work in both enviroments automatically, so if your code runs too slow on CPUs, moving to GPUs should be easy. To submit our job, we need a submit file and a job wrapper script. The submit file is a basic OSGConnect flavored HTCondor file, specifying that we want the job to run in a container. cpu-job.submit contains: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 0 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 And job-wrapper.sh: #!/bin/bash set -e # set TMPDIR variable export TMPDIR=$_CONDOR_SCRATCH_DIR echo echo \"I'm running on\" $(hostname -f) echo \"OSG site: $OSG_SITE_NAME\" echo python3 test.py 2>&1 The job can now be submitted with condor_submit cpu-job.submit . Once the job is done, check the files named after the job id for the outputs. Running a GPU job \u00b6 When moving the job to be run on a GPU, all we have to do is update two lines in the submit file: set request_gpus to 1 and specify a GPU enabled container image for +SingularityImage . The updated submit file can be found in gpu-job.submit with the contents: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 1 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 Submit a job with condor_submit gpu-job.submit . Once the job is complete, check the .out file for a line stating the code was run under a GPU. Something similar to: 2021-02-02 23:25:19.022467: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 The GPU:0 parts shows that a GPU was found and used for the computation.","title":"Working with Tensorflow, GPUs, and containers"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/#working-with-tensorflow-gpus-and-containers","text":"In this tutorial, we explore GPUs and containers on OSG, using the popular Tensorflow sofware package. Tensorflow is a good example here as the software is too complex to bundle up and ship with your job. Containers solve this problem by defining a full OS image, containing not only the complex software package, but dependencies and environment configuration as well. https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.","title":"Working with Tensorflow, GPUs, and containers"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/#defining-container-images","text":"Defining containers is fully described in the Docker and Singularity Containers section. Here we will just provide an overview of how you could take something like an existing Tensorflow image provided by OSG staff, and extend it by adding your own modules to it. Let's assume you like Tensorflow version 2.3. The definition of this image can be found in Github: Dockerfile . You don't really need to understand how an image was built in order to use it. As described in the containers documentation, make sure the HTCondor submit file has: Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" If you want to extend an existing image, you can just inherit from the parent image available on DockerHub here . For example, if you just need some additional Python packages, your new Dockerfile could look like: FROM opensciencegrid/tensorflow:2.3 RUN python3 -m pip install some_package_name You can then docker build and docker push it so that your new image is available on DockerHub. Note that OSG does not provide any infrastructure for these steps. You will have to complete them on your own computer or using the DockerHub build infrastructure.","title":"Defining container images"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/#adding-a-container-to-the-osg-cvmfs-distribution-mechanism","text":"How to add a container image to the OSG CVMFS distribution mechanism is also described in Docker and Singularity Containers , but a quick scan of the cvmfs-singularity-sync and specifically the docker_images.txt file show us that the tensorflow images are listed as: opensciencegrid/tensorflow:* opensciencegrid/tensorflow-gpu:* Those two lines means that all tags from those two DockerHub repositories should be mapped to /cvmfs/singularity.opensciencegrid.org/ . On the login node, try running: ls /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ This is the image in its expanded form - something we can execute with Singularity!","title":"Adding a container to the OSG CVMFS distribution mechanism"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/#testing-the-container-on-the-submit-host","text":"First, download the files contained in this tutorial to the login node using the git clone command and cd into the tutorial directory that is created: git clone https://github.com/OSGConnect/tutorial-tensorflow-containers cd tutorial-tensorflow-containers Before submitting jobs to the OSG, it is always a good idea to test your code so that you understand runtime requirements. The containers can be tested on the OSGConnect submit hosts with singularity shell , which will drop you into a container and let you exlore it interactively. To explore the Tensorflow 2.3 image, run: singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ Note how the command line prompt changes, providing you an indicator that you are inside the image. You can exit any time by running exit . Another important thing to note is that your $HOME directory is automatically mounted inside the interactive container - allowing you to access your codes and test it out. First, start with a simple python3 import test to make sure tensorflow is available: $ python3 Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tensorflow 2021-01-15 17:32:33.901607: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:32:33.901735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. >>> Tensorflow will warn you that no GPUs where found. This is expected as we do not have GPUs attached to our login nodes, and it is fine as Tensorflow works fine with regular CPUs (slower of course). Exit out of Python3 with CTRL+D and then we can run a Tensorflow testcode which can be found in this tutorial: $ python3 test.py 2021-01-15 17:37:43.152892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:43.153021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2021-01-15 17:37:44.899967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:44.900063: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303) 2021-01-15 17:37:44.900130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login05.osgconnect.net): /proc/driver/nvidia/version does not exist 2021-01-15 17:37:44.900821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-01-15 17:37:44.912483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz 2021-01-15 17:37:44.915548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-01-15 17:37:44.915645: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-01-15 17:37:44.921895: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) We will again see a bunch of warnings regarding GPUs not being available, but as we can see by the /job:localhost/replica:0/task:0/device:CPU:0 line, the code ran on one of the CPUs. When testing your own code like this, take note of how much memory, disk and runtime is required - it is needed in the next step. Once you are done with testing, use CTRL+D or run exit to exit out of the container. Note that you can not submit jobs from within the container.","title":"Testing the container on the submit host"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/#running-a-cpu-job","text":"If Tensorflow can run on GPUs, you might be wondering why we might want to run it on slower CPUs? One reason is that CPUs are plentiful while GPUs are still somewhat scarce. If you have a lot of shorter Tensorflow jobs, they might complete faster on available CPUs, rather than wait in the queue for the faster, less available, GPUs. The good news is that Tensorflow code should work in both enviroments automatically, so if your code runs too slow on CPUs, moving to GPUs should be easy. To submit our job, we need a submit file and a job wrapper script. The submit file is a basic OSGConnect flavored HTCondor file, specifying that we want the job to run in a container. cpu-job.submit contains: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 0 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 And job-wrapper.sh: #!/bin/bash set -e # set TMPDIR variable export TMPDIR=$_CONDOR_SCRATCH_DIR echo echo \"I'm running on\" $(hostname -f) echo \"OSG site: $OSG_SITE_NAME\" echo python3 test.py 2>&1 The job can now be submitted with condor_submit cpu-job.submit . Once the job is done, check the files named after the job id for the outputs.","title":"Running a CPU job"},{"location":"software_examples/machine_learning/tutorial-tensorflow-containers/#running-a-gpu-job","text":"When moving the job to be run on a GPU, all we have to do is update two lines in the submit file: set request_gpus to 1 and specify a GPU enabled container image for +SingularityImage . The updated submit file can be found in gpu-job.submit with the contents: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 1 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 Submit a job with condor_submit gpu-job.submit . Once the job is complete, check the .out file for a line stating the code was run under a GPU. Something similar to: 2021-02-02 23:25:19.022467: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 The GPU:0 parts shows that a GPU was found and used for the computation.","title":"Running a GPU job"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/","text":"Scaling up compute resources \u00b6 Scaling up the computational resources is a big advantage for doing certain large-scale calculations on OSG. Consider the extensive sampling for a multi-dimensional Monte Carlo integration or molecular dynamics simulation with several initial conditions. These types of calculations require submitting a lot of jobs. In the previous example, we submitted the job to a single-worker machine. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs will enable us to realize the full potential of distributed high throughput computing on the OSG. In this section, we will see how to scale up the calculations with a simple example. Once we understand the basic HTCondor script, it is easy to scale up. Background \u00b6 For this example, we will use computational methods to estimate pi. First, we will define a square inscribed by a unit circle from which we will randomly sample points. The ratio of the points outside the circle to the points in the circle is calculated which approaches pi/4. This method converges extremely slowly, which makes it great for a CPU-intensive exercise (but bad for a real estimation!). Set up a Matlab Job \u00b6 First, we'll need to create a working directory, you can either run $ tutorial Matlab-ScalingUp or $ git clone https://github.com/OSGConnect/tutorial-Matlab-ScalingUp to copy all the necessary files. Otherwise, you can create the files type the following: $ mkdir tutorial-Matlab-ScalingUp $ cd tutorial-Matlab-ScalingUp Matlab Script \u00b6 Create an Matlab script by typing the following into a file called mcpi.m : % Monte Carlo method for estimating pi % Generate N random points in a unit square function[] =mcpi(N) x = rand(N,1); % x coordinates y = rand(N,1); % y coordinates % Count how many points are inside a unit circle inside = 0; % counter for i = 1:N % loop over points if x(i)^2 + y(i)^2 <= 1 % check if inside circle inside = inside + 1; % increment counter end end % Estimate pi as the ratio of points inside circle to total points pi_est = 4 * inside / N; % pi estimate % Display the result fprintf(pi_est); end Compilation \u00b6 OSG does not have a license to use the MATLAB compiler . On a Linux server with a MATLAB license, invoke the compiler mcc . We turn off all graphical options ( -nodisplay ), disable Java ( -nojvm ), and instruct MATLAB to run this application as a single-threaded application ( -singleCompThread ): mcc -m -R -singleCompThread -R -nodisplay -R -nojvm mcpi.m The flag -m means C language translation during compilation, and the flag -R indicates runtime options. The compilation would produce the files: `mcpi, run_mcpi.sh, mccExcludedFiles.log` and `readme.txt` The file mcpi is the standalone executable. The file run_mcpi.sh is MATLAB generated shell script. mccExcludedFiles.log is the log file and readme.txt contains the information about the compilation process. We just need the standalone binary file mcpi . Running standalone binary applications on OSG \u00b6 To see which releases are available on OSG visit our available containers page : Tutorial files \u00b6 Let us say you have created the standalone binary mcpi . Transfer the file mcpi to your Access Point. Alternatively, you may also use the readily available files by using the git clone command: $ git clone https://github.com/OSGConnect/tutorial-Matlab-ScalingUp # Copies input and script files to the directory tutorial-Matlab-ScalingUp. This will create a directory tutorial-Matlab-ScalingUp . Inside the directory, you will see the following files mcpi # compiled executable binary of mcpi.m mcpi.m # matlab program mcpi.submit # condor job description file mcpi.sh # execution script Executing the MATLAB application binary \u00b6 The compilation and execution environment need to the same. The file mcpi is a standalone binary of the matlab program mcpi.m which was compiled using MATLAB 2020b on a Linux platform. The Access Point and many of the worker nodes on OSG are based on Linux platform. In addition to the platform requirement, we also need to have the same MATLAB Runtime version. Load the MATLAB runtime for 2020b version via apptainer/singularity command. On the terminal prompt, type $ apptainer shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b The above command sets up the environment to run the matlab/2020b runtime applications. Now execute the binary $apptainer/singularity> ./mcpi 10 If you get the an output of the estimated value of pi, the binary execution is successful. Now, exit from the apptainer/singularity environment typing exit . Next, we see how to submit the job on a remote execute point using HTCondor. Job execution and submission files \u00b6 Let us take a look at mcpi.submit file: universe = vanilla # One OSG Connect vanilla, the preffered job universe is \"vanilla\" +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b\" executable = mcpi arguments = $(Process) Output = Log/job.$(Process).out\u22c5 # standard output Error = Log/job.$(Process).err # standard error Log = Log/job.$(Process).log # log information about job execution requirements = HAS_SINGULARITY == TRUE queue 100 # Submit 100 jobs Before we submit the job, make sure that the directory Log exists on the current working directory. Because HTCondor looks for Log directory to copy the standard output, error and log files as specified in the job description file. From your work directory, type $ mkdir -p Log Absence of Log directory may send the jobs to held state. Job submmision \u00b6 We submit the job using the condor_submit command as follows $ condor_submit mcpi.submit //Submit the condor job description file \"mcpi.submit\" Now you have submitted an ensemble of 100 MATLAB jobs. Each job prints the value of pi on the standard output. Check the status of the submitted job, $ condor_q username # The status of the job is printed on the screen. Here, username is your login name. Post Process\u22c5 \u00b6 Once the jobs are completed, you can use the information in the output files to calculate an average of all of our computed estimates of Pi. To see this, we can use the command: $ cat log/mcpi*.out* | awk '{ sum += $2; print $2\" \"NR} END { print \"---------------\\n Grand Average = \" sum/NR }' Key Points \u00b6 Scaling up the computational resources on OSG is crucial to taking full advantage of distributed computing. Changing the value of Queue allows the user to scale up the resources. Arguments allows you to pass parameters to a job script. $(Cluster) and $(Process) can be used to name log files uniquely. Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@osg-htc.org .","title":"Scaling up MATLAB"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#scaling-up-compute-resources","text":"Scaling up the computational resources is a big advantage for doing certain large-scale calculations on OSG. Consider the extensive sampling for a multi-dimensional Monte Carlo integration or molecular dynamics simulation with several initial conditions. These types of calculations require submitting a lot of jobs. In the previous example, we submitted the job to a single-worker machine. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs will enable us to realize the full potential of distributed high throughput computing on the OSG. In this section, we will see how to scale up the calculations with a simple example. Once we understand the basic HTCondor script, it is easy to scale up.","title":"Scaling up compute resources"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#background","text":"For this example, we will use computational methods to estimate pi. First, we will define a square inscribed by a unit circle from which we will randomly sample points. The ratio of the points outside the circle to the points in the circle is calculated which approaches pi/4. This method converges extremely slowly, which makes it great for a CPU-intensive exercise (but bad for a real estimation!).","title":"Background"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#set-up-a-matlab-job","text":"First, we'll need to create a working directory, you can either run $ tutorial Matlab-ScalingUp or $ git clone https://github.com/OSGConnect/tutorial-Matlab-ScalingUp to copy all the necessary files. Otherwise, you can create the files type the following: $ mkdir tutorial-Matlab-ScalingUp $ cd tutorial-Matlab-ScalingUp","title":"Set up a Matlab Job"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#matlab-script","text":"Create an Matlab script by typing the following into a file called mcpi.m : % Monte Carlo method for estimating pi % Generate N random points in a unit square function[] =mcpi(N) x = rand(N,1); % x coordinates y = rand(N,1); % y coordinates % Count how many points are inside a unit circle inside = 0; % counter for i = 1:N % loop over points if x(i)^2 + y(i)^2 <= 1 % check if inside circle inside = inside + 1; % increment counter end end % Estimate pi as the ratio of points inside circle to total points pi_est = 4 * inside / N; % pi estimate % Display the result fprintf(pi_est); end","title":"Matlab Script"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#compilation","text":"OSG does not have a license to use the MATLAB compiler . On a Linux server with a MATLAB license, invoke the compiler mcc . We turn off all graphical options ( -nodisplay ), disable Java ( -nojvm ), and instruct MATLAB to run this application as a single-threaded application ( -singleCompThread ): mcc -m -R -singleCompThread -R -nodisplay -R -nojvm mcpi.m The flag -m means C language translation during compilation, and the flag -R indicates runtime options. The compilation would produce the files: `mcpi, run_mcpi.sh, mccExcludedFiles.log` and `readme.txt` The file mcpi is the standalone executable. The file run_mcpi.sh is MATLAB generated shell script. mccExcludedFiles.log is the log file and readme.txt contains the information about the compilation process. We just need the standalone binary file mcpi .","title":"Compilation"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#running-standalone-binary-applications-on-osg","text":"To see which releases are available on OSG visit our available containers page :","title":"Running standalone binary applications on OSG"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#tutorial-files","text":"Let us say you have created the standalone binary mcpi . Transfer the file mcpi to your Access Point. Alternatively, you may also use the readily available files by using the git clone command: $ git clone https://github.com/OSGConnect/tutorial-Matlab-ScalingUp # Copies input and script files to the directory tutorial-Matlab-ScalingUp. This will create a directory tutorial-Matlab-ScalingUp . Inside the directory, you will see the following files mcpi # compiled executable binary of mcpi.m mcpi.m # matlab program mcpi.submit # condor job description file mcpi.sh # execution script","title":"Tutorial files"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#executing-the-matlab-application-binary","text":"The compilation and execution environment need to the same. The file mcpi is a standalone binary of the matlab program mcpi.m which was compiled using MATLAB 2020b on a Linux platform. The Access Point and many of the worker nodes on OSG are based on Linux platform. In addition to the platform requirement, we also need to have the same MATLAB Runtime version. Load the MATLAB runtime for 2020b version via apptainer/singularity command. On the terminal prompt, type $ apptainer shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b The above command sets up the environment to run the matlab/2020b runtime applications. Now execute the binary $apptainer/singularity> ./mcpi 10 If you get the an output of the estimated value of pi, the binary execution is successful. Now, exit from the apptainer/singularity environment typing exit . Next, we see how to submit the job on a remote execute point using HTCondor.","title":"Executing the MATLAB application binary"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#job-execution-and-submission-files","text":"Let us take a look at mcpi.submit file: universe = vanilla # One OSG Connect vanilla, the preffered job universe is \"vanilla\" +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b\" executable = mcpi arguments = $(Process) Output = Log/job.$(Process).out\u22c5 # standard output Error = Log/job.$(Process).err # standard error Log = Log/job.$(Process).log # log information about job execution requirements = HAS_SINGULARITY == TRUE queue 100 # Submit 100 jobs Before we submit the job, make sure that the directory Log exists on the current working directory. Because HTCondor looks for Log directory to copy the standard output, error and log files as specified in the job description file. From your work directory, type $ mkdir -p Log Absence of Log directory may send the jobs to held state.","title":"Job execution and submission files"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#job-submmision","text":"We submit the job using the condor_submit command as follows $ condor_submit mcpi.submit //Submit the condor job description file \"mcpi.submit\" Now you have submitted an ensemble of 100 MATLAB jobs. Each job prints the value of pi on the standard output. Check the status of the submitted job, $ condor_q username # The status of the job is printed on the screen. Here, username is your login name.","title":"Job submmision"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#post-process","text":"Once the jobs are completed, you can use the information in the output files to calculate an average of all of our computed estimates of Pi. To see this, we can use the command: $ cat log/mcpi*.out* | awk '{ sum += $2; print $2\" \"NR} END { print \"---------------\\n Grand Average = \" sum/NR }'","title":"Post Process\u22c5"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#key-points","text":"Scaling up the computational resources on OSG is crucial to taking full advantage of distributed computing. Changing the value of Queue allows the user to scale up the resources. Arguments allows you to pass parameters to a job script. $(Cluster) and $(Process) can be used to name log files uniquely.","title":"Key Points"},{"location":"software_examples/matlab_runtime/tutorial-Matlab-ScalingUp/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@osg-htc.org .","title":"Getting Help"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/","text":"Basics of compiled MATLAB applications - Hello World example \u00b6 MATLAB\u00ae is a licensed high level language and modeling toolkit. The MATLAB Compiler\u2122 lets you share MATLAB programs as standalone applications. MATLAB Compiler is invoked with mcc . The compiler supports most toolboxes and user-developed interfaces. For more details, check the list of supported toolboxes and ineligible programs . All applications created with MATLAB Compiler use MATLAB Compiler Runtime\u2122 (MCR) , which enables royalty-free deployment and use. We assume you have access to a server that has MATLAB compiler because the compiler is not available on OSG Connect. MATLAB Runtime is available on OSG Connect. Although the compiled binaries are portable, they need to have a compatible, OS-specific matlab runtime to interpret the binary. We recommend the compilation of your matlab program against matlab versions that match the OSG containers , with the compilation executed on a server with Scientific Linux so that the compiled binaries are portable on OSG machines. In this tutorial, we learn the basics of compiling MATLAB programs on a licensed linux machine and running the compiled binaries using a matlab compiled runtime (MCR) in the OSG containers. MATLAB script: hello_world.m \u00b6 Lets start with a simple MATLAB script hello_world.m that prints Hello World! to standard output. function helloworld fprintf('\\n=============') fprintf('\\nHello, World!\\n') fprintf('=============\\n') end Compilation \u00b6 OSG connect does not have a license to use the MATLAB compiler . On a Linux server with a MATLAB license, invoke the compiler mcc . We turn off all graphical options ( -nodisplay ), disable Java ( -nojvm ), and instruct MATLAB to run this application as a single-threaded application ( -singleCompThread ): mcc -m -R -singleCompThread -R -nodisplay -R -nojvm hello_world.m The flag -m means C language translation during compilation, and the flag -R indicates runtime options. The compilation would produce the files: `hello_world, run_hello_world.sh, mccExcludedFiles.log` and `readme.txt` The file hello_world is the standalone executable. The file run_hello_world.sh is MATLAB generated shell script. mccExcludedFiles.log is the log file and readme.txt contains the information about the compilation process. We just need the standalone binary file hello_world . Running standalone binary applications on OSG \u00b6 To see which releases are available on OSG visit our available containers page : Tutorial files \u00b6 Let us say you have created the standalone binary hello_world . Transfer the file hello_world to your Access Point. Alternatively, you may also use the readily available files by using the git clone command: $ git clone https://github.com/OSGConnect/tutorial-matlab-HelloWorld # Copies input and script files to the directory tutorial-matlab-HelloWorld. This will create a directory tutorial-matlab-HelloWorld . Inside the directory, you will see the following files hello_world # compiled executable binary of hello_world.m hello_world.m # matlab program hello_world.submit # condor job description file hello_world.sh # execution script Executing the MATLAB application binary \u00b6 The compilation and execution environment need to the same. The file hello_world is a standalone binary of the matlab program hello_world.m which was compiled using MATLAB 2018b on a Linux platform. The Access Point and many of the worker nodes on OSG are based on Linux platform. In addition to the platform requirement, we also need to have the same MATLAB Runtime version. Load the MATLAB runtime for 2018b version via apptainer/singularity command. On the terminal prompt, type $ apptainer shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b The above command sets up the environment to run the matlab/2018b runtime applications. Now execute the binary $apptainer/singularity> ./hello_world (would produce the following output) ============= Hello, World! ============= If you get the above output, the binary execution is successful. Now, exit from the apptainer/singularity environment typing exit . Next, we see how to submit the job on a remote execute point using HTcondor. Job execution and submission files \u00b6 Let us take a look at hello_world.submit file: universe = vanilla # One OSG Connect vanilla, the preffered job universe is \"vanilla\" +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b\" executable = hello_world Output = Log/job.$(Process).out\u22c5 # standard output Error = Log/job.$(Process).err # standard error Log = Log/job.$(Process).log # log information about job execution requirements = HAS_SINGULARITY == TRUE queue 10 # Submit 10 jobs Before we submit the job, make sure that the directory Log exists on the current working directory. Because HTcondor looks for Log directory to copy the standard output, error and log files as specified in the job description file. From your work directory, type $ mkdir -p Log Absence of Log directory would send the jobs to held state. Job submmision \u00b6 We submit the job using the condor_submit command as follows $ condor_submit hello_world.submit //Submit the condor job description file \"hello_world.submit\" Now you have submitted an ensemble of 10 MATLAB jobs. Each job prints hello world on the standard output. Check the status of the submitted job, $ condor_q username # The status of the job is printed on the screen. Here, username is your login name. Job outputs \u00b6 The hello_world.m script sends the output to standard output. In the condor job description file, we expressed that the standard output is written on the Log/job.$(ProcessID).out . After job completion, ten output files are produced with the hello world message under the directory Log . What's next? \u00b6 Sure, it is not very exciting to print the same message on 10 output files. In the subsequent MATLAB examples, we see how to scale up MATLAB computation on HTC environment. Getting help \u00b6 For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the help desk and community forums .","title":"Basics of compiled MATLAB applications - Hello World example"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#basics-of-compiled-matlab-applications-hello-world-example","text":"MATLAB\u00ae is a licensed high level language and modeling toolkit. The MATLAB Compiler\u2122 lets you share MATLAB programs as standalone applications. MATLAB Compiler is invoked with mcc . The compiler supports most toolboxes and user-developed interfaces. For more details, check the list of supported toolboxes and ineligible programs . All applications created with MATLAB Compiler use MATLAB Compiler Runtime\u2122 (MCR) , which enables royalty-free deployment and use. We assume you have access to a server that has MATLAB compiler because the compiler is not available on OSG Connect. MATLAB Runtime is available on OSG Connect. Although the compiled binaries are portable, they need to have a compatible, OS-specific matlab runtime to interpret the binary. We recommend the compilation of your matlab program against matlab versions that match the OSG containers , with the compilation executed on a server with Scientific Linux so that the compiled binaries are portable on OSG machines. In this tutorial, we learn the basics of compiling MATLAB programs on a licensed linux machine and running the compiled binaries using a matlab compiled runtime (MCR) in the OSG containers.","title":"Basics of compiled MATLAB applications - Hello World example"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#matlab-script-hello_worldm","text":"Lets start with a simple MATLAB script hello_world.m that prints Hello World! to standard output. function helloworld fprintf('\\n=============') fprintf('\\nHello, World!\\n') fprintf('=============\\n') end","title":"MATLAB script: hello_world.m"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#compilation","text":"OSG connect does not have a license to use the MATLAB compiler . On a Linux server with a MATLAB license, invoke the compiler mcc . We turn off all graphical options ( -nodisplay ), disable Java ( -nojvm ), and instruct MATLAB to run this application as a single-threaded application ( -singleCompThread ): mcc -m -R -singleCompThread -R -nodisplay -R -nojvm hello_world.m The flag -m means C language translation during compilation, and the flag -R indicates runtime options. The compilation would produce the files: `hello_world, run_hello_world.sh, mccExcludedFiles.log` and `readme.txt` The file hello_world is the standalone executable. The file run_hello_world.sh is MATLAB generated shell script. mccExcludedFiles.log is the log file and readme.txt contains the information about the compilation process. We just need the standalone binary file hello_world .","title":"Compilation"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#running-standalone-binary-applications-on-osg","text":"To see which releases are available on OSG visit our available containers page :","title":"Running standalone binary applications on OSG"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#tutorial-files","text":"Let us say you have created the standalone binary hello_world . Transfer the file hello_world to your Access Point. Alternatively, you may also use the readily available files by using the git clone command: $ git clone https://github.com/OSGConnect/tutorial-matlab-HelloWorld # Copies input and script files to the directory tutorial-matlab-HelloWorld. This will create a directory tutorial-matlab-HelloWorld . Inside the directory, you will see the following files hello_world # compiled executable binary of hello_world.m hello_world.m # matlab program hello_world.submit # condor job description file hello_world.sh # execution script","title":"Tutorial files"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#executing-the-matlab-application-binary","text":"The compilation and execution environment need to the same. The file hello_world is a standalone binary of the matlab program hello_world.m which was compiled using MATLAB 2018b on a Linux platform. The Access Point and many of the worker nodes on OSG are based on Linux platform. In addition to the platform requirement, we also need to have the same MATLAB Runtime version. Load the MATLAB runtime for 2018b version via apptainer/singularity command. On the terminal prompt, type $ apptainer shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b The above command sets up the environment to run the matlab/2018b runtime applications. Now execute the binary $apptainer/singularity> ./hello_world (would produce the following output) ============= Hello, World! ============= If you get the above output, the binary execution is successful. Now, exit from the apptainer/singularity environment typing exit . Next, we see how to submit the job on a remote execute point using HTcondor.","title":"Executing the MATLAB application binary"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#job-execution-and-submission-files","text":"Let us take a look at hello_world.submit file: universe = vanilla # One OSG Connect vanilla, the preffered job universe is \"vanilla\" +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b\" executable = hello_world Output = Log/job.$(Process).out\u22c5 # standard output Error = Log/job.$(Process).err # standard error Log = Log/job.$(Process).log # log information about job execution requirements = HAS_SINGULARITY == TRUE queue 10 # Submit 10 jobs Before we submit the job, make sure that the directory Log exists on the current working directory. Because HTcondor looks for Log directory to copy the standard output, error and log files as specified in the job description file. From your work directory, type $ mkdir -p Log Absence of Log directory would send the jobs to held state.","title":"Job execution and submission files"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#job-submmision","text":"We submit the job using the condor_submit command as follows $ condor_submit hello_world.submit //Submit the condor job description file \"hello_world.submit\" Now you have submitted an ensemble of 10 MATLAB jobs. Each job prints hello world on the standard output. Check the status of the submitted job, $ condor_q username # The status of the job is printed on the screen. Here, username is your login name.","title":"Job submmision"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#job-outputs","text":"The hello_world.m script sends the output to standard output. In the condor job description file, we expressed that the standard output is written on the Log/job.$(ProcessID).out . After job completion, ten output files are produced with the hello world message under the directory Log .","title":"Job outputs"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#whats-next","text":"Sure, it is not very exciting to print the same message on 10 output files. In the subsequent MATLAB examples, we see how to scale up MATLAB computation on HTC environment.","title":"What's next?"},{"location":"software_examples/matlab_runtime/tutorial-matlab-HelloWorld/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@osg-htc.org or visit the help desk and community forums .","title":"Getting help"},{"location":"software_examples/other_languages_tools/conda-container/","text":"Conda with Containers \u00b6 The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. There are two ways of using Conda on the OSPool: with a tarball , or via a custom Apptainer/Singularity container. Either works well, but the container solution might be better if your Conda environment contains non-Python tools. Overview \u00b6 When should you use Miniconda as an installation method in OSG? Your software has specific conda-centric installation instructions. The above is true and the software has a lot of dependencies. You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation for your jobs, create an Apptainer/Singularity definition file and build it (general instructions here ). Apptainer/Singularity Definition File \u00b6 The definition file tells Apptainer/Singularity how the container should be built, and what the environment setup should take place when the container is instantiated. In the following example, the container is based on Ubuntu 22.04. A few base operating system tools are installed, then Miniconda, followed by a set of conda commands to define the Conda environment. The %environment is used to ensure jobs are getting the environment activated before the job runs. To build your own custom image, start by modifing the conda install line to include the packages you need. Bootstrap: docker From: ubuntu:22.04 %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate %post # base os apt-get update -y apt-get install -y build-essential wget # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda activate conda install -y -c conda-forge numpy cowpy conda update --all The next step is to build the image. Run: $ apptainer build my-container.sif image.def You can explore the container locally to make sure it works as expected with the shell subcommand: $ apptainer shell my-container.sif This example will give you an interactive shell. You can explore the container and test your code with your own inputs from your /home directory, which is automatically mounted (but note - $HOME will not be available to your jobs later). Once you are down exploring, exit the container by running exit or with CTRL+D It is important to use the correct transfer mechanism to get the image to your job. Please make sure you use OSDF and version your container in the filename. For example: $ cp my-container.sif /ospool/protected/<username>/my-container-v1.sif Submit Jobs \u00b6 An example submit file could look like: # File Name: conda_submission.sub # specify the newly built image +SingularityImage = \"osdf:///ospool/protected/<username>/my-container-v1.sif\" # Specify your executable (single binary or a script that runs several # commands) and arguments to be passed to jobs. # $(Process) will be a integer number for each job, starting with \"0\" # and increasing for the relevant number of jobs. executable = science.py arguments = $(Process) # Specify the name of the log, standard error, and standard output (or \"screen output\") files. log = science_with_conda.log error = science_with_conda.err output = science_with_conda.out # Transfer any file needed for our job to complete. transfer_input_files = # Specify Job duration category as \"Medium\" (expected runtime <10 hr) or \"Long\" (expected runtime <20 hr). +JobDurationCategory = \u201cMedium\u201d # Tell HTCondor requirements your job needs, # what amount of compute resources each job will need on the computer where it runs. requirements = request_cpus = 1 request_memory = 1GB request_disk = 5GB # Tell HTCondor to run 1 instance of our job: queue 1 Specifying Exact Dependency Versions \u00b6 An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. An example environment.yml file: channels: - conda-forge - defaults dependencies: - cowpy - numpy=1.25.0 To use the environment.yml in the build, modify the image definition to copy the file, and then replace the conda install with a conda env create . Also note that it is good style to name the environment. We call it science in this example: Bootstrap: docker From: ubuntu:22.04 %files environment.yml %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate science %post # base os apt-get update -y apt-get install -y build-essential wget # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda activate conda env create -n science -f environment.yml conda update --all If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. More information on conda environments can be found in their documentation .","title":"Conda with Containers"},{"location":"software_examples/other_languages_tools/conda-container/#conda-with-containers","text":"The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. There are two ways of using Conda on the OSPool: with a tarball , or via a custom Apptainer/Singularity container. Either works well, but the container solution might be better if your Conda environment contains non-Python tools.","title":"Conda with Containers"},{"location":"software_examples/other_languages_tools/conda-container/#overview","text":"When should you use Miniconda as an installation method in OSG? Your software has specific conda-centric installation instructions. The above is true and the software has a lot of dependencies. You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation for your jobs, create an Apptainer/Singularity definition file and build it (general instructions here ).","title":"Overview"},{"location":"software_examples/other_languages_tools/conda-container/#apptainersingularity-definition-file","text":"The definition file tells Apptainer/Singularity how the container should be built, and what the environment setup should take place when the container is instantiated. In the following example, the container is based on Ubuntu 22.04. A few base operating system tools are installed, then Miniconda, followed by a set of conda commands to define the Conda environment. The %environment is used to ensure jobs are getting the environment activated before the job runs. To build your own custom image, start by modifing the conda install line to include the packages you need. Bootstrap: docker From: ubuntu:22.04 %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate %post # base os apt-get update -y apt-get install -y build-essential wget # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda activate conda install -y -c conda-forge numpy cowpy conda update --all The next step is to build the image. Run: $ apptainer build my-container.sif image.def You can explore the container locally to make sure it works as expected with the shell subcommand: $ apptainer shell my-container.sif This example will give you an interactive shell. You can explore the container and test your code with your own inputs from your /home directory, which is automatically mounted (but note - $HOME will not be available to your jobs later). Once you are down exploring, exit the container by running exit or with CTRL+D It is important to use the correct transfer mechanism to get the image to your job. Please make sure you use OSDF and version your container in the filename. For example: $ cp my-container.sif /ospool/protected/<username>/my-container-v1.sif","title":"Apptainer/Singularity Definition File"},{"location":"software_examples/other_languages_tools/conda-container/#submit-jobs","text":"An example submit file could look like: # File Name: conda_submission.sub # specify the newly built image +SingularityImage = \"osdf:///ospool/protected/<username>/my-container-v1.sif\" # Specify your executable (single binary or a script that runs several # commands) and arguments to be passed to jobs. # $(Process) will be a integer number for each job, starting with \"0\" # and increasing for the relevant number of jobs. executable = science.py arguments = $(Process) # Specify the name of the log, standard error, and standard output (or \"screen output\") files. log = science_with_conda.log error = science_with_conda.err output = science_with_conda.out # Transfer any file needed for our job to complete. transfer_input_files = # Specify Job duration category as \"Medium\" (expected runtime <10 hr) or \"Long\" (expected runtime <20 hr). +JobDurationCategory = \u201cMedium\u201d # Tell HTCondor requirements your job needs, # what amount of compute resources each job will need on the computer where it runs. requirements = request_cpus = 1 request_memory = 1GB request_disk = 5GB # Tell HTCondor to run 1 instance of our job: queue 1","title":"Submit Jobs"},{"location":"software_examples/other_languages_tools/conda-container/#specifying-exact-dependency-versions","text":"An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. An example environment.yml file: channels: - conda-forge - defaults dependencies: - cowpy - numpy=1.25.0 To use the environment.yml in the build, modify the image definition to copy the file, and then replace the conda install with a conda env create . Also note that it is good style to name the environment. We call it science in this example: Bootstrap: docker From: ubuntu:22.04 %files environment.yml %environment # set up environment for when using the container . /opt/conda/etc/profile.d/conda.sh conda activate science %post # base os apt-get update -y apt-get install -y build-essential wget # install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /opt/conda rm Miniconda3-latest-Linux-x86_64.sh # install conda components - add the packages you need here . /opt/conda/etc/profile.d/conda.sh conda activate conda env create -n science -f environment.yml conda update --all If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. More information on conda environments can be found in their documentation .","title":"Specifying Exact Dependency Versions"},{"location":"software_examples/other_languages_tools/conda-tarball/","text":"Conda with Tarballs \u00b6 The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. There are two ways of using Conda on the OSPool: with a tarball as described in this guide, or by installing Conda inside a custom Apptainer/Singularity container . Either works well, but the container solution might be better if your Conda environment requires access to non-Python tools. Overview \u00b6 When should you use Miniconda as an installation method in OSG? Your software has specific conda-centric installation instructions. The above is true and the software has a lot of dependencies. You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation for your jobs, create your installation environment on the access point and send a zipped version to your jobs. Install Miniconda and Package for Jobs \u00b6 In this approach, we will create an entire software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs. 1. Create a Miniconda Installation \u00b6 After logging into your access point, download the latest Linux miniconda installer and run it. For example, [alice@ap00]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [alice@ap00]$ sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d - If you enter \"no\", you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future. - If you enter \"yes\", miniconda will edit your .bashrc file and PATH environment variable so that you do not need to define a path to Miniconda each time you log in. If you choose \"yes\", before proceeding, you must log off and close your terminal for these changes to go into effect. Once you close your terminal, you can reopen it, log in to your access point, and proceed with the rest of the instructions below. 2. Create a conda \"Environment\" With Your Packages \u00b6 (If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@ap00]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@ap00]$ conda create -n env-name (base)[alice@ap00]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@ap00]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@ap00]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and call the environment py-data-sci , you would use this sequence of commands: (base)[alice@ap00]$ conda create -n py-data-sci (base)[alice@ap00]$ conda activate py-data-sci (py-data-sci)[alice@ap00]$ conda install pandas matplotlib (py-data-sci)[alice@ap00]$ conda deactivate (base)[alice@ap00]$ More About Miniconda \u00b6 See the official conda documentation for more information on creating and managing environments with conda . 3. Create Software Package \u00b6 Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@ap00]$ Then, run this command to install the conda pack tool: (base)[alice@ap00]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: (base)[alice@ap00]$ conda pack -n env-name (base)[alice@ap00]$ chmod 644 env-name.tar.gz (base)[alice@ap00]$ ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz . 4. Check Size of Conda Environment Tar Archive \u00b6 The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >1GB in size, you should move the file to either your /public or /protected folder, and transfer it to/from jobs using the osdf:/// link, as described in Overview: Data Staging and Transfer to Jobs . This is the most efficient way to transfer large files to/from jobs. 5. Create a Job Executable \u00b6 The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). For future reference, let's call this executable conda_science.sh . #!/bin/bash # File Name: science_with_conda.sh # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py 6. Submit Jobs \u00b6 In your HTCondor submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 1GB, please move the file to either your /protected or /public directories and use the osdf:/// file delivery mechanism as described above. An example submit file could look like: # File Name: conda_submission.sub # Specify your executable (single binary or a script that runs several # commands) and arguments to be passed to jobs. # $(Process) will be a integer number for each job, starting with \"0\" # and increasing for the relevant number of jobs. executable = science_with_conda.sh arguments = $(Process) # Specify the name of the log, standard error, and standard output (or \"screen output\") files. log = science_with_conda.log error = science_with_conda.err output = science_with_conda.out # Transfer any file needed for our job to complete. transfer_input_files = osdf:///ospool/apXX/data/alice/env-name.tar.gz, hello.py In the line above, the `XX` in `apXX` should be replaced with the numbers corresponding to your access point. # Specify Job duration category as \"Medium\" (expected runtime <10 hr) or \"Long\" (expected runtime <20 hr). +JobDurationCategory = \u201cMedium\u201d # Tell HTCondor requirements (e.g., operating system) your job needs, # what amount of compute resources each job will need on the computer where it runs. requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_cpus = 1 request_memory = 1GB request_disk = 5GB # Tell HTCondor to run 1 instance of our job: queue 1 Specifying Exact Dependency Versions \u00b6 An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run [alice@ap00]$ conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is [alice@ap00]$ conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run [alice@ap00]$ conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science Pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science Pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package above. More information on conda environments can be found in their documentation .","title":"Conda with Tarballs"},{"location":"software_examples/other_languages_tools/conda-tarball/#conda-with-tarballs","text":"The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. There are two ways of using Conda on the OSPool: with a tarball as described in this guide, or by installing Conda inside a custom Apptainer/Singularity container . Either works well, but the container solution might be better if your Conda environment requires access to non-Python tools.","title":"Conda with Tarballs"},{"location":"software_examples/other_languages_tools/conda-tarball/#overview","text":"When should you use Miniconda as an installation method in OSG? Your software has specific conda-centric installation instructions. The above is true and the software has a lot of dependencies. You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation for your jobs, create your installation environment on the access point and send a zipped version to your jobs.","title":"Overview"},{"location":"software_examples/other_languages_tools/conda-tarball/#install-miniconda-and-package-for-jobs","text":"In this approach, we will create an entire software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs.","title":"Install Miniconda and Package for Jobs"},{"location":"software_examples/other_languages_tools/conda-tarball/#1-create-a-miniconda-installation","text":"After logging into your access point, download the latest Linux miniconda installer and run it. For example, [alice@ap00]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [alice@ap00]$ sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d - If you enter \"no\", you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future. - If you enter \"yes\", miniconda will edit your .bashrc file and PATH environment variable so that you do not need to define a path to Miniconda each time you log in. If you choose \"yes\", before proceeding, you must log off and close your terminal for these changes to go into effect. Once you close your terminal, you can reopen it, log in to your access point, and proceed with the rest of the instructions below.","title":"1. Create a Miniconda Installation"},{"location":"software_examples/other_languages_tools/conda-tarball/#2-create-a-conda-environment-with-your-packages","text":"(If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@ap00]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@ap00]$ conda create -n env-name (base)[alice@ap00]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@ap00]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@ap00]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and call the environment py-data-sci , you would use this sequence of commands: (base)[alice@ap00]$ conda create -n py-data-sci (base)[alice@ap00]$ conda activate py-data-sci (py-data-sci)[alice@ap00]$ conda install pandas matplotlib (py-data-sci)[alice@ap00]$ conda deactivate (base)[alice@ap00]$","title":"2. Create a conda \"Environment\" With Your Packages"},{"location":"software_examples/other_languages_tools/conda-tarball/#more-about-miniconda","text":"See the official conda documentation for more information on creating and managing environments with conda .","title":"More About Miniconda"},{"location":"software_examples/other_languages_tools/conda-tarball/#3-create-software-package","text":"Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@ap00]$ Then, run this command to install the conda pack tool: (base)[alice@ap00]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: (base)[alice@ap00]$ conda pack -n env-name (base)[alice@ap00]$ chmod 644 env-name.tar.gz (base)[alice@ap00]$ ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz .","title":"3. Create Software Package"},{"location":"software_examples/other_languages_tools/conda-tarball/#4-check-size-of-conda-environment-tar-archive","text":"The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >1GB in size, you should move the file to either your /public or /protected folder, and transfer it to/from jobs using the osdf:/// link, as described in Overview: Data Staging and Transfer to Jobs . This is the most efficient way to transfer large files to/from jobs.","title":"4. Check Size of Conda Environment Tar Archive"},{"location":"software_examples/other_languages_tools/conda-tarball/#5-create-a-job-executable","text":"The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). For future reference, let's call this executable conda_science.sh . #!/bin/bash # File Name: science_with_conda.sh # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py","title":"5. Create a Job Executable"},{"location":"software_examples/other_languages_tools/conda-tarball/#6-submit-jobs","text":"In your HTCondor submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 1GB, please move the file to either your /protected or /public directories and use the osdf:/// file delivery mechanism as described above. An example submit file could look like: # File Name: conda_submission.sub # Specify your executable (single binary or a script that runs several # commands) and arguments to be passed to jobs. # $(Process) will be a integer number for each job, starting with \"0\" # and increasing for the relevant number of jobs. executable = science_with_conda.sh arguments = $(Process) # Specify the name of the log, standard error, and standard output (or \"screen output\") files. log = science_with_conda.log error = science_with_conda.err output = science_with_conda.out # Transfer any file needed for our job to complete. transfer_input_files = osdf:///ospool/apXX/data/alice/env-name.tar.gz, hello.py In the line above, the `XX` in `apXX` should be replaced with the numbers corresponding to your access point. # Specify Job duration category as \"Medium\" (expected runtime <10 hr) or \"Long\" (expected runtime <20 hr). +JobDurationCategory = \u201cMedium\u201d # Tell HTCondor requirements (e.g., operating system) your job needs, # what amount of compute resources each job will need on the computer where it runs. requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_cpus = 1 request_memory = 1GB request_disk = 5GB # Tell HTCondor to run 1 instance of our job: queue 1","title":"6. Submit Jobs"},{"location":"software_examples/other_languages_tools/conda-tarball/#specifying-exact-dependency-versions","text":"An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run [alice@ap00]$ conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is [alice@ap00]$ conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run [alice@ap00]$ conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science Pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science Pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package above. More information on conda environments can be found in their documentation .","title":"Specifying Exact Dependency Versions"},{"location":"software_examples/other_languages_tools/java-on-osg/","text":"Using Java in Jobs \u00b6 Overview \u00b6 If your code uses Java via a .jar file, it is easy to bring along your own copy of the Java Development Kit (JDK) which allows you to run your .jar file anywhere on the Open Science Pool. Steps to Use Java in Jobs \u00b6 Get a copy of Java/JDK. You can access the the Java Development Kit (JDK) from the JDK website . First select the link to the JDK that is listed as \"Ready for Use\" and then download the Linux/x64 version of the tar.gz file using a Unix command such as wget from your /home directory. For example, $ wget https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz The downloaded file should end up in your home directory on the OSPool access point. Include Java in Input Files. Add the downloaded tar file to the transfer_input_files line of your submit file, along with the .jar file and any other input files the job needs: transfer_input_files = openjdk-17.0.1_linux-x64_bin.tar.gz, program.jar, other_input Setup Java inside the job. Write a script that unpacks the JDK tar file, sets the environment to find the java software, and then runs your program. This script will be your job\\'s executable. See this example for what the script should look like: #!/bin/bash # unzip the JDK tar -xzf openjdk-17.0.1_linux-x64_bin.tar.gz # Add the unzipped JDK folder to the environment export PATH=$PWD/jdk-17.0.1/bin:$PATH export JAVA_HOME=$PWD/jdk-17.0.1 # run your .jar file java -jar program.jar Note that the exact name of the unzipped JDK folder and the JDK tar.gz file will vary depending on the version you downloaded. You should unzip the JDK tar.gz file in your home directory to find out the correct directory name to add to the script.","title":"Using Java in Jobs "},{"location":"software_examples/other_languages_tools/java-on-osg/#using-java-in-jobs","text":"","title":"Using Java in Jobs"},{"location":"software_examples/other_languages_tools/java-on-osg/#overview","text":"If your code uses Java via a .jar file, it is easy to bring along your own copy of the Java Development Kit (JDK) which allows you to run your .jar file anywhere on the Open Science Pool.","title":"Overview"},{"location":"software_examples/other_languages_tools/java-on-osg/#steps-to-use-java-in-jobs","text":"Get a copy of Java/JDK. You can access the the Java Development Kit (JDK) from the JDK website . First select the link to the JDK that is listed as \"Ready for Use\" and then download the Linux/x64 version of the tar.gz file using a Unix command such as wget from your /home directory. For example, $ wget https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz The downloaded file should end up in your home directory on the OSPool access point. Include Java in Input Files. Add the downloaded tar file to the transfer_input_files line of your submit file, along with the .jar file and any other input files the job needs: transfer_input_files = openjdk-17.0.1_linux-x64_bin.tar.gz, program.jar, other_input Setup Java inside the job. Write a script that unpacks the JDK tar file, sets the environment to find the java software, and then runs your program. This script will be your job\\'s executable. See this example for what the script should look like: #!/bin/bash # unzip the JDK tar -xzf openjdk-17.0.1_linux-x64_bin.tar.gz # Add the unzipped JDK folder to the environment export PATH=$PWD/jdk-17.0.1/bin:$PATH export JAVA_HOME=$PWD/jdk-17.0.1 # run your .jar file java -jar program.jar Note that the exact name of the unzipped JDK folder and the JDK tar.gz file will vary depending on the version you downloaded. You should unzip the JDK tar.gz file in your home directory to find out the correct directory name to add to the script.","title":"Steps to Use Java in Jobs"},{"location":"software_examples/other_languages_tools/julia-on-osg/","text":"Using Julia on the OSPool \u00b6 Overview \u00b6 This guide provides an introduction to running Julia code on the Open Science Pool. The Quickstart Instructions provide an outline of job submission. The following sections provide more details about installing Julia packages ( Install Julia Packages ) and creating a complete job submission ( Submit Julia Jobs ). This guide assumes that you have a script written in Julia and can identify the additional Julia packages needed to run the script. If you are using many Julia packages or have other software dependencies as part of your job, you may want to manage your software via a container instead of using the tar.gz file method described in this guide. The Research Computing Facilitation (RCF) team maintains a Julia container that can be used as a starting point for creating a customized container with added packages. See our Docker and Singularity/Apptainer Guide for more details. Quickstart Instructions \u00b6 Download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . Tip: use wget to download directly to your /home directory on the access point, OR use transfer_input_files = url in your HTCondor submit files. Install your Julia packages on the access point, else skip to the next step. For more details, see the section on installing Julia packages below: Installing Julia Packages Submit a job that executes a Julia script using the Julia precompiled binary with base Julia and Standard Library, via a shell script like the following as the job's executable: #!/bin/bash # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH=$_CONDOR_SCRATCH_DIR/julia-#-#-#/bin:$PATH # run Julia script julia my-script.jl For more details on the job submission, see the section below: Submit Julia Jobs Install Julia Packages \u00b6 If your work requires additional Julia packages, you will need to peform a one-time installation of these packages within a Julia project. A copy of the project can then be saved for use in subsequent job submissions. For more details, please see Julia's documentation at Julia Pkg.jl . Download Julia and set up a \"project\" \u00b6 If you have not already downloaded a copy of Julia, download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . We will need a copy of the original tar.gz file for running jobs, but to install packages, we also need an unpacked version of the software. Run the following commands to extract the Julia software and add Julia to your PATH : $ tar -xzf julia-#.#.#-linux-x86_64.tar.gz $ export PATH=$PWD/julia-#.#.#/bin:$PATH After these steps, you should be able to run Julia from the command line, e.g. $ julia --version Now create a project directory to install your packages (we've called it my-project/ below) and tell Julia its name: $ mkdir my-project $ export JULIA_DEPOT_PATH=$PWD/my-project If you already have a directory with Julia packages on the login node, you can add to it by skipping the mkdir step above and going straight to setting the JULIA_DEPOT_PATH variable. You can choose whatever name to use for this directory -- if you have different projects that you use for different jobs, you could use a more descriptive name than \"my-project\". Install Packages \u00b6 We will now use Julia to install any needed packages to the project directory we created in the previous step. Open Julia with the --project option set to the project directory: $ julia --project=my-project Once you've started up the Julia REPL (interpreter), start the Pkg REPL, used to install packages, by typing ] . Then install and test packages by using Julia's add Package syntax. _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.0.5 (2019-09-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> ] (my-project) pkg> add Package (my-project) pkg> test Package If you have multiple packages to install they can be combined into a single command, e.g. (my-project) pkg> add Package1 Package2 Package3 . If you encounter issues getting packages to install successfully, please contact us at support@osg-htc.org Once you are done, you can exit the Pkg REPL by typing the DELETE key and then typing exit() (my-project) pkg> julia> exit() Your packages will have been installed to the my_project directory; we want to compress this folder so that it is easier to copy to jobs. $ tar -czf my-project.tar.gz my-project/ Submit Julia Jobs \u00b6 To submit a job that runs a Julia script, create a bash script and HTCondor submit file following the examples in this section. These example assume that you have downloaded a copy of Julia for Linux as a tar.gz file and if using packages, you have gone through the steps above to install them and create an additional tar.gz file of the installed packages. Create Executable Bash Script \u00b6 Your job will use a bash script as the HTCondor executable . This script will contain all the steps needed to unpack the Julia binaries and execute your Julia script ( script.jl below). What follows are two example bash scripts, one which can be used to execute a script with base Julia only, and one that will use packages you installed to a project directory (see Install Julia Packages ). Example Bash Script For Base Julia Only \u00b6 If your Julia script can run without additional packages (other than base Julia and the Julia Standard library) use the example script directly below. #!/bin/bash # julia-job.sh # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH=$_CONDOR_SCRATCH_DIR/julia-#.#.#/bin:$PATH # run Julia script julia script.jl Example Bash Script For Julia With Installed Packages \u00b6 #!/bin/bash # julia-job.sh # extract Julia tar.gz file and project tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz tar -xzf my-project.tar.gz # add Julia binary to PATH export PATH=$_CONDOR_SCRATCH_DIR/julia-#.#.#/bin:$PATH # add Julia packages to DEPOT variable export JULIA_DEPOT_PATH=$_CONDOR_SCRATCH_DIR/my-project # run Julia script julia --project=my-project script.jl Create HTCondor Submit File \u00b6 After creating a bash script named julia-job.sh to run Julia, then create a submit file to submit the job. More details about setting up a submit file, including a submit file template, can be found in our quickstart guide: Quickstart Tutorial # File Name = julia-job.sub executable = julia-job.sh transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 If your Julia script needs to use packages installed for a project, be sure to include my-project.tar.gz as an input file in julia-job.sub . For project tarballs that are <1 GB, you can follow the below example: transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl, my-project.tar.gz Modify the CPU/memory request lines to match what is needed by the job. Test a few jobs for disk space/memory usage in order to make sure your requests for a large batch are accurate! Disk space and memory usage can be found in the log file after the job completes.","title":"Using Julia on the OSPool "},{"location":"software_examples/other_languages_tools/julia-on-osg/#using-julia-on-the-ospool","text":"","title":"Using Julia on the OSPool"},{"location":"software_examples/other_languages_tools/julia-on-osg/#overview","text":"This guide provides an introduction to running Julia code on the Open Science Pool. The Quickstart Instructions provide an outline of job submission. The following sections provide more details about installing Julia packages ( Install Julia Packages ) and creating a complete job submission ( Submit Julia Jobs ). This guide assumes that you have a script written in Julia and can identify the additional Julia packages needed to run the script. If you are using many Julia packages or have other software dependencies as part of your job, you may want to manage your software via a container instead of using the tar.gz file method described in this guide. The Research Computing Facilitation (RCF) team maintains a Julia container that can be used as a starting point for creating a customized container with added packages. See our Docker and Singularity/Apptainer Guide for more details.","title":"Overview"},{"location":"software_examples/other_languages_tools/julia-on-osg/#quickstart-instructions","text":"Download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . Tip: use wget to download directly to your /home directory on the access point, OR use transfer_input_files = url in your HTCondor submit files. Install your Julia packages on the access point, else skip to the next step. For more details, see the section on installing Julia packages below: Installing Julia Packages Submit a job that executes a Julia script using the Julia precompiled binary with base Julia and Standard Library, via a shell script like the following as the job's executable: #!/bin/bash # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH=$_CONDOR_SCRATCH_DIR/julia-#-#-#/bin:$PATH # run Julia script julia my-script.jl For more details on the job submission, see the section below: Submit Julia Jobs","title":"Quickstart Instructions"},{"location":"software_examples/other_languages_tools/julia-on-osg/#install-julia-packages","text":"If your work requires additional Julia packages, you will need to peform a one-time installation of these packages within a Julia project. A copy of the project can then be saved for use in subsequent job submissions. For more details, please see Julia's documentation at Julia Pkg.jl .","title":"Install Julia Packages"},{"location":"software_examples/other_languages_tools/julia-on-osg/#download-julia-and-set-up-a-project","text":"If you have not already downloaded a copy of Julia, download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . We will need a copy of the original tar.gz file for running jobs, but to install packages, we also need an unpacked version of the software. Run the following commands to extract the Julia software and add Julia to your PATH : $ tar -xzf julia-#.#.#-linux-x86_64.tar.gz $ export PATH=$PWD/julia-#.#.#/bin:$PATH After these steps, you should be able to run Julia from the command line, e.g. $ julia --version Now create a project directory to install your packages (we've called it my-project/ below) and tell Julia its name: $ mkdir my-project $ export JULIA_DEPOT_PATH=$PWD/my-project If you already have a directory with Julia packages on the login node, you can add to it by skipping the mkdir step above and going straight to setting the JULIA_DEPOT_PATH variable. You can choose whatever name to use for this directory -- if you have different projects that you use for different jobs, you could use a more descriptive name than \"my-project\".","title":"Download Julia and set up a \"project\""},{"location":"software_examples/other_languages_tools/julia-on-osg/#install-packages","text":"We will now use Julia to install any needed packages to the project directory we created in the previous step. Open Julia with the --project option set to the project directory: $ julia --project=my-project Once you've started up the Julia REPL (interpreter), start the Pkg REPL, used to install packages, by typing ] . Then install and test packages by using Julia's add Package syntax. _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.0.5 (2019-09-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> ] (my-project) pkg> add Package (my-project) pkg> test Package If you have multiple packages to install they can be combined into a single command, e.g. (my-project) pkg> add Package1 Package2 Package3 . If you encounter issues getting packages to install successfully, please contact us at support@osg-htc.org Once you are done, you can exit the Pkg REPL by typing the DELETE key and then typing exit() (my-project) pkg> julia> exit() Your packages will have been installed to the my_project directory; we want to compress this folder so that it is easier to copy to jobs. $ tar -czf my-project.tar.gz my-project/","title":"Install Packages"},{"location":"software_examples/other_languages_tools/julia-on-osg/#submit-julia-jobs","text":"To submit a job that runs a Julia script, create a bash script and HTCondor submit file following the examples in this section. These example assume that you have downloaded a copy of Julia for Linux as a tar.gz file and if using packages, you have gone through the steps above to install them and create an additional tar.gz file of the installed packages.","title":"Submit Julia Jobs"},{"location":"software_examples/other_languages_tools/julia-on-osg/#create-executable-bash-script","text":"Your job will use a bash script as the HTCondor executable . This script will contain all the steps needed to unpack the Julia binaries and execute your Julia script ( script.jl below). What follows are two example bash scripts, one which can be used to execute a script with base Julia only, and one that will use packages you installed to a project directory (see Install Julia Packages ).","title":"Create Executable Bash Script"},{"location":"software_examples/other_languages_tools/julia-on-osg/#example-bash-script-for-base-julia-only","text":"If your Julia script can run without additional packages (other than base Julia and the Julia Standard library) use the example script directly below. #!/bin/bash # julia-job.sh # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH=$_CONDOR_SCRATCH_DIR/julia-#.#.#/bin:$PATH # run Julia script julia script.jl","title":"Example Bash Script For Base Julia Only"},{"location":"software_examples/other_languages_tools/julia-on-osg/#example-bash-script-for-julia-with-installed-packages","text":"#!/bin/bash # julia-job.sh # extract Julia tar.gz file and project tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz tar -xzf my-project.tar.gz # add Julia binary to PATH export PATH=$_CONDOR_SCRATCH_DIR/julia-#.#.#/bin:$PATH # add Julia packages to DEPOT variable export JULIA_DEPOT_PATH=$_CONDOR_SCRATCH_DIR/my-project # run Julia script julia --project=my-project script.jl","title":"Example Bash Script For Julia With Installed Packages"},{"location":"software_examples/other_languages_tools/julia-on-osg/#create-htcondor-submit-file","text":"After creating a bash script named julia-job.sh to run Julia, then create a submit file to submit the job. More details about setting up a submit file, including a submit file template, can be found in our quickstart guide: Quickstart Tutorial # File Name = julia-job.sub executable = julia-job.sh transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 9\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 If your Julia script needs to use packages installed for a project, be sure to include my-project.tar.gz as an input file in julia-job.sub . For project tarballs that are <1 GB, you can follow the below example: transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl, my-project.tar.gz Modify the CPU/memory request lines to match what is needed by the job. Test a few jobs for disk space/memory usage in order to make sure your requests for a large batch are accurate! Disk space and memory usage can be found in the log file after the job completes.","title":"Create HTCondor Submit File"},{"location":"software_examples/python/manage-python-packages/","text":"Run Python Scripts on the OSPool \u00b6 Overview \u00b6 This guide will show you two examples of how to run jobs that use Python in the Open Science Pool. The first example will demonstrate how to submit a job that uses base Python. The second example will demonstrate the workflow for jobs that use specific Python packages, including how to install a custom set of Python packages to your home directory and how to add them to a Python job submission. Before getting started, you should know which Python packages you need to run your job. Running Base Python on the Open Science Pool \u00b6 Create a bash script to run Python \u00b6 To submit jobs that use a module to run base Python, first create a bash executable - for this example we'll call it run_py.sh - which will run our Python script called myscript.py . For example, run_py.sh : #!/bin/bash # Run the Python script python3 myscript.py If you need to use Python 2, replace the python3 above with python2 . Create an HTCondor submit file \u00b6 In order to submit run_py.sh as part of a job, we need to create an HTCondor submit file. This should include the following: run_py.sh specified as the executable use transfer_input_files to bring our Python script myscript.py to wherever the job runs include a standard container image that has Python installed. All together, the submit file will look something like this: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py log = job.log output = job.out error = job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Once everything is set up, the job can be submitted in the usual way, by running the condor_submit command with the name of the submit file. Running Python Jobs That Use Additional Packages \u00b6 It's likely that you'll need additional Python packages that are not present in the base Python installations. This portion of the guide describes how to install your packages to a custom directory and then include them as part of your jobs. Install Python packages \u00b6 While connected to your login node, start the base Singularity container that has a copy of Python inside: $ singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Next, create a directory for your files and set the PYTHONPATH Singularity> mkdir my_env Singularity> export PYTHONPATH=$PWD/my_env You can swap out my_env for a more descriptive name like scipy or word-analysis . Now we can use pip to install Python packages. Singularity> pip3 install --target=$PWD/my_env numpy ......some download message... Installing collected packages: numpy Installing collected packages: numpy Successfully installed numpy-1.16.3 Install each package that you need for your job using the pip install command. If you would like to test the package installation, you can run the python3 command and then try importing the packages you just installed. To exit the Python console, type \"quit()\" Once you are done, you can leave the virtual environment: Singularity> exit All of the packages that were just installed should be contained in a sub-directory of the my_env directory. To use these packages in a job, the entire my_env directory will be transfered as a tar.gz file. So our final step is to compress the directory, as follows: $ tar -czf my_env.tar.gz my_env Create executable script to use installed packages \u00b6 In addition to loading the appropriate Python module, we will need to add a few steps to our bash executable to set-up the virtual environment we just created. That will look something like this: #!/bin/bash # Unpack your envvironment (with your packages), and activate it tar -xzf my_env.tar.gz export PYTHONPATH=$PWD/my_env # Run the Python script python3 myscript.py Modify the HTCondor submit file to transfer Python packages \u00b6 The submit file for this job will be similar to the base Python job submit file shown above with one addition - we need to include my_env.tar.gz in the list of files specified by transfer_input_files . As an example: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py, my_env.tar.gz log = job.log output = job.out error = job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Other Considerations \u00b6 This guide mainly focuses on the nuts and bolts of running Python, but it's important to remember that additional files needed for your jobs (input data, setting files, etc.) need to be transferred with the job as well. See our Introduction to Data Management on OSG for details on the different ways to deliver inputs to your jobs. When you've prepared a real job submission, make sure to run a test job and then check the log file for disk and memory usage; if you're using significantly more or less than what you requested, make sure you adjust your requests. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Run Python Scripts on the OSPool "},{"location":"software_examples/python/manage-python-packages/#run-python-scripts-on-the-ospool","text":"","title":"Run Python Scripts on the OSPool"},{"location":"software_examples/python/manage-python-packages/#overview","text":"This guide will show you two examples of how to run jobs that use Python in the Open Science Pool. The first example will demonstrate how to submit a job that uses base Python. The second example will demonstrate the workflow for jobs that use specific Python packages, including how to install a custom set of Python packages to your home directory and how to add them to a Python job submission. Before getting started, you should know which Python packages you need to run your job.","title":"Overview"},{"location":"software_examples/python/manage-python-packages/#running-base-python-on-the-open-science-pool","text":"","title":"Running Base Python on the Open Science Pool"},{"location":"software_examples/python/manage-python-packages/#create-a-bash-script-to-run-python","text":"To submit jobs that use a module to run base Python, first create a bash executable - for this example we'll call it run_py.sh - which will run our Python script called myscript.py . For example, run_py.sh : #!/bin/bash # Run the Python script python3 myscript.py If you need to use Python 2, replace the python3 above with python2 .","title":"Create a bash script to run Python"},{"location":"software_examples/python/manage-python-packages/#create-an-htcondor-submit-file","text":"In order to submit run_py.sh as part of a job, we need to create an HTCondor submit file. This should include the following: run_py.sh specified as the executable use transfer_input_files to bring our Python script myscript.py to wherever the job runs include a standard container image that has Python installed. All together, the submit file will look something like this: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py log = job.log output = job.out error = job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Once everything is set up, the job can be submitted in the usual way, by running the condor_submit command with the name of the submit file.","title":"Create an HTCondor submit file"},{"location":"software_examples/python/manage-python-packages/#running-python-jobs-that-use-additional-packages","text":"It's likely that you'll need additional Python packages that are not present in the base Python installations. This portion of the guide describes how to install your packages to a custom directory and then include them as part of your jobs.","title":"Running Python Jobs That Use Additional Packages"},{"location":"software_examples/python/manage-python-packages/#install-python-packages","text":"While connected to your login node, start the base Singularity container that has a copy of Python inside: $ singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Next, create a directory for your files and set the PYTHONPATH Singularity> mkdir my_env Singularity> export PYTHONPATH=$PWD/my_env You can swap out my_env for a more descriptive name like scipy or word-analysis . Now we can use pip to install Python packages. Singularity> pip3 install --target=$PWD/my_env numpy ......some download message... Installing collected packages: numpy Installing collected packages: numpy Successfully installed numpy-1.16.3 Install each package that you need for your job using the pip install command. If you would like to test the package installation, you can run the python3 command and then try importing the packages you just installed. To exit the Python console, type \"quit()\" Once you are done, you can leave the virtual environment: Singularity> exit All of the packages that were just installed should be contained in a sub-directory of the my_env directory. To use these packages in a job, the entire my_env directory will be transfered as a tar.gz file. So our final step is to compress the directory, as follows: $ tar -czf my_env.tar.gz my_env","title":"Install Python packages"},{"location":"software_examples/python/manage-python-packages/#create-executable-script-to-use-installed-packages","text":"In addition to loading the appropriate Python module, we will need to add a few steps to our bash executable to set-up the virtual environment we just created. That will look something like this: #!/bin/bash # Unpack your envvironment (with your packages), and activate it tar -xzf my_env.tar.gz export PYTHONPATH=$PWD/my_env # Run the Python script python3 myscript.py","title":"Create executable script to use installed packages"},{"location":"software_examples/python/manage-python-packages/#modify-the-htcondor-submit-file-to-transfer-python-packages","text":"The submit file for this job will be similar to the base Python job submit file shown above with one addition - we need to include my_env.tar.gz in the list of files specified by transfer_input_files . As an example: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py, my_env.tar.gz log = job.log output = job.out error = job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1","title":"Modify the HTCondor submit file to transfer Python packages"},{"location":"software_examples/python/manage-python-packages/#other-considerations","text":"This guide mainly focuses on the nuts and bolts of running Python, but it's important to remember that additional files needed for your jobs (input data, setting files, etc.) need to be transferred with the job as well. See our Introduction to Data Management on OSG for details on the different ways to deliver inputs to your jobs. When you've prepared a real job submission, make sure to run a test job and then check the log file for disk and memory usage; if you're using significantly more or less than what you requested, make sure you adjust your requests.","title":"Other Considerations"},{"location":"software_examples/python/manage-python-packages/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"software_examples/python/tutorial-ScalingUp-Python/","text":"Scaling Up With HTCondor\u2019s Queue Command \u00b6 Many large scale computations require the ability to process multiple jobs concurrently. Consider the extensive sampling done for a multi-dimensional Monte Carlo integration, parameter sweep for a given model or molecular dynamics simulation with several initial conditions. These calculations require submitting many jobs. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs is essential to realize the full potential of distributed high throughput computing on the OSG. The HTCondor's queue command can run multiple jobs from a single job description file. In this tutorial, we will see how to scale up the calculations for a simple python example using the HTCondor\u2019s queue command. Once we understand the basic HTCondor script to run a single job, it is easy to scale up. To download the materials for this tutorial, use the command $ git clone https://github.com/OSGConnect/tutorial-ScalingUp-Python Inside the tutorial-ScalingUp-python directory, all the required files are available. This includes the sample python program, job description file and executable files. Move into the directory with $ cd tutorial-ScalingUp-Python Python script and the optimization function \u00b6 Let us take a look at our objective function that we are trying to optimize. f = (1 - x)**2 + (y - x**2)**2 This a two dimensional Rosenbrock function. Clearly, the minimum is located at (1,1). The Rosenbrock function is one of the test functions used to test the robustness of an optimization method. Here, we are going to use the brute force optimization approach to evaluate the two dimensional Rosenbrock function on grids of points. The boundary values for the grid points are randomly assigned inside the python script. However, these default values may be replaced by user supplied values. To run the calculations with the random boundary values, the script is executed without any argument: python3 rosen_brock_brute_opt.py To run the calculations with the user supplied values, the script is executed with input arguments: python3 rosen_brock_brute_opt.py x_low x_high y_low y_high where x_low and x_high are low and high values along x direction, and y_low and y_high are the low and high values along the y direction. For example, the boundary of x direction is (-3, 3) and the boundary of y direction is (-2, 3). python3 rosen_brock_brute_opt.py -3 3 -2 2 sets the boundary of x direction to (-3, 3) and the boundary of y direction to (-2, 3). The directory Example1 runs the python script with the default random values. The directories Example2 , and Example3 deal with supplying the boundary values as input arguments. The python script requires the SciPy package, which is typically not included in standard installations of Python 3. Therefore, we will use a container that has Python 3 and SciPy installed. If you'd like to test the script, you can do so with apptainer shell /cvmfs/singularity.opensciencegrid.org/htc/rocky:8 and then run one of the above commands. Submitting Jobs Concurrently \u00b6 Now let us take a look at job description file. cd Example1 cat ScalingUp-PythonCals.submit If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to add the $(Cluster) and $(Process) variables to the file names. You can see this below in the names given to the standard output, standard error and HTCondor log files: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" executable = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue 10 Note the queue 10 . This tells Condor to queue 10 copies of this job as one cluster. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 10 job(s) submitted to cluster 329837. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_script.sh Note that all ten jobs will have run with random arguments because we did not supply any from the submit file. What if we wanted to supply those arguments so that we could reproduce this analysis if needed? The next example shows how to do this. Providing Different Inputs to Jobs \u00b6 In the previous example, we did not pass any argument to the program and the program generated random boundary conditions. If we have some guess about what could be a better boundary condition, it is a good idea to supply the boundary condition as arguments. It is possible to use a single file to supply multiple arguments. We can take the job description file from the previous example, and modify it to include arguments. The modified job description file is available in the Example2 directory. Take a look at the job description file ScalingUp-PythonCals.submit . $ cd ../Example2 $ cat ScalingUp-PythonCals.submit +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" executable = ../rosen_brock_brute_opt.py arguments = $(x_low) $(x_high) $(y_low) $(y_high) log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue x_low x_high y_low y_high from job_values.txt A major part of the job description file looks same as the previous example. The main difference is the addition of arguments keyword, which looks like this: arguments = $(x_low) $(x_high) $(y_low) $(y_high) The given arguments $(x_low) , $(x_high) , etc. are actually variables that represent the values we want to use. These values are set in the queue command at the end of the file: queue x_low x_high y_low y_high from job_values.txt Take a look at job_values.txt: $ cat job_values.txt -9 9 -9 9 -8 8 -8 8 -7 7 -7 7 -6 6 -6 6 -5 5 -5 5 -4 4 -4 4 -3 3 -3 3 -2 2 -2 2 -1 1 -1 1 The submit file's queue statement will read in this file and assign each value in a row to the four variables shown in the queue statement. Each row corresponds to the submission of a unique job with those four values. Let us submit the above job to see this: $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329840. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh Another Example of Different Inputs \u00b6 In the previous example, we split the input information into four variables that were included in the arguments line. However, we could have set the arguments line directly, without intermediate values. This is shown in Example 3: $ cd ../Example3 $ cat ScalingUp-PythonCals.submit +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" executable = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue arguments from job_values.txt Here, arguments has disappeared from the top of the file because we've included it in the queue statement at the end. The job_values.txt file has the same values as before; in this syntax, HTCondor will submit a job for each row of values and the job's arguments will be those four values. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329839. Apply your condor_q and connect watch knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh","title":"Scaling Up With HTCondor\u2019s Queue Command"},{"location":"software_examples/python/tutorial-ScalingUp-Python/#scaling-up-with-htcondors-queue-command","text":"Many large scale computations require the ability to process multiple jobs concurrently. Consider the extensive sampling done for a multi-dimensional Monte Carlo integration, parameter sweep for a given model or molecular dynamics simulation with several initial conditions. These calculations require submitting many jobs. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs is essential to realize the full potential of distributed high throughput computing on the OSG. The HTCondor's queue command can run multiple jobs from a single job description file. In this tutorial, we will see how to scale up the calculations for a simple python example using the HTCondor\u2019s queue command. Once we understand the basic HTCondor script to run a single job, it is easy to scale up. To download the materials for this tutorial, use the command $ git clone https://github.com/OSGConnect/tutorial-ScalingUp-Python Inside the tutorial-ScalingUp-python directory, all the required files are available. This includes the sample python program, job description file and executable files. Move into the directory with $ cd tutorial-ScalingUp-Python","title":"Scaling Up With HTCondor\u2019s Queue Command"},{"location":"software_examples/python/tutorial-ScalingUp-Python/#python-script-and-the-optimization-function","text":"Let us take a look at our objective function that we are trying to optimize. f = (1 - x)**2 + (y - x**2)**2 This a two dimensional Rosenbrock function. Clearly, the minimum is located at (1,1). The Rosenbrock function is one of the test functions used to test the robustness of an optimization method. Here, we are going to use the brute force optimization approach to evaluate the two dimensional Rosenbrock function on grids of points. The boundary values for the grid points are randomly assigned inside the python script. However, these default values may be replaced by user supplied values. To run the calculations with the random boundary values, the script is executed without any argument: python3 rosen_brock_brute_opt.py To run the calculations with the user supplied values, the script is executed with input arguments: python3 rosen_brock_brute_opt.py x_low x_high y_low y_high where x_low and x_high are low and high values along x direction, and y_low and y_high are the low and high values along the y direction. For example, the boundary of x direction is (-3, 3) and the boundary of y direction is (-2, 3). python3 rosen_brock_brute_opt.py -3 3 -2 2 sets the boundary of x direction to (-3, 3) and the boundary of y direction to (-2, 3). The directory Example1 runs the python script with the default random values. The directories Example2 , and Example3 deal with supplying the boundary values as input arguments. The python script requires the SciPy package, which is typically not included in standard installations of Python 3. Therefore, we will use a container that has Python 3 and SciPy installed. If you'd like to test the script, you can do so with apptainer shell /cvmfs/singularity.opensciencegrid.org/htc/rocky:8 and then run one of the above commands.","title":"Python script and the optimization function"},{"location":"software_examples/python/tutorial-ScalingUp-Python/#submitting-jobs-concurrently","text":"Now let us take a look at job description file. cd Example1 cat ScalingUp-PythonCals.submit If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to add the $(Cluster) and $(Process) variables to the file names. You can see this below in the names given to the standard output, standard error and HTCondor log files: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" executable = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue 10 Note the queue 10 . This tells Condor to queue 10 copies of this job as one cluster. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 10 job(s) submitted to cluster 329837. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_script.sh Note that all ten jobs will have run with random arguments because we did not supply any from the submit file. What if we wanted to supply those arguments so that we could reproduce this analysis if needed? The next example shows how to do this.","title":"Submitting Jobs Concurrently"},{"location":"software_examples/python/tutorial-ScalingUp-Python/#providing-different-inputs-to-jobs","text":"In the previous example, we did not pass any argument to the program and the program generated random boundary conditions. If we have some guess about what could be a better boundary condition, it is a good idea to supply the boundary condition as arguments. It is possible to use a single file to supply multiple arguments. We can take the job description file from the previous example, and modify it to include arguments. The modified job description file is available in the Example2 directory. Take a look at the job description file ScalingUp-PythonCals.submit . $ cd ../Example2 $ cat ScalingUp-PythonCals.submit +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" executable = ../rosen_brock_brute_opt.py arguments = $(x_low) $(x_high) $(y_low) $(y_high) log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue x_low x_high y_low y_high from job_values.txt A major part of the job description file looks same as the previous example. The main difference is the addition of arguments keyword, which looks like this: arguments = $(x_low) $(x_high) $(y_low) $(y_high) The given arguments $(x_low) , $(x_high) , etc. are actually variables that represent the values we want to use. These values are set in the queue command at the end of the file: queue x_low x_high y_low y_high from job_values.txt Take a look at job_values.txt: $ cat job_values.txt -9 9 -9 9 -8 8 -8 8 -7 7 -7 7 -6 6 -6 6 -5 5 -5 5 -4 4 -4 4 -3 3 -3 3 -2 2 -2 2 -1 1 -1 1 The submit file's queue statement will read in this file and assign each value in a row to the four variables shown in the queue statement. Each row corresponds to the submission of a unique job with those four values. Let us submit the above job to see this: $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329840. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh","title":"Providing Different Inputs to Jobs"},{"location":"software_examples/python/tutorial-ScalingUp-Python/#another-example-of-different-inputs","text":"In the previous example, we split the input information into four variables that were included in the arguments line. However, we could have set the arguments line directly, without intermediate values. This is shown in Example 3: $ cd ../Example3 $ cat ScalingUp-PythonCals.submit +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\" executable = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue arguments from job_values.txt Here, arguments has disappeared from the top of the file because we've included it in the queue statement at the end. The job_values.txt file has the same values as before; in this syntax, HTCondor will submit a job for each row of values and the job's arguments will be those four values. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329839. Apply your condor_q and connect watch knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh","title":"Another Example of Different Inputs"},{"location":"software_examples/python/tutorial-wordfreq/","text":"Wordcount Tutorial for Submitting Multiple Jobs \u00b6 Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. The type of workflow covered in this tutorial can be used to describe workflows that take have different input files or parameters from job to job. To download the materials for this tutorial, type: $ git clone https://github.com/OSGConnect/tutorial-wordfreq Analyzing One Book \u00b6 Test the Command \u00b6 We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt If you run the ls command, you should see a new file with the prefix counts which has the results of this python script. This is the output we want to produce within an HTCondor job. For now, remove the output: $ rm counts.Alice_in_Wonderland.tsv Create a Submit File \u00b6 To submit a single job that runs this command and analyzes the Alice's Adventures in Wonderland book, we need to translate this command into HTCondor submit file syntax. The two main components we care about are (1) the actual command and (2) the needed input files. The command gets turned into the submit file executable and arguments options: executable = wordcount.py arguments = Alice_in_Wonderland.txt The executable is the script that we want to run, and the arguments is everything else that follows the script when we run it, like the test above. The input file for this job is the Alice_in_Wonderland.txt text file. While we provided the name as in the arguments , we need to explicitly tell HTCondor to transfer the corresponding file. We include the file name in the following submit file option: transfer_input_files = Alice_in_Wonderland.txt There are other submit file options that control other aspects of the job, like where to save error and logging information, and how many resources to request per job. This tutorial has a sample submit file ( wordcount.sub ) with most of these submit file options filled in: $ cat wordcount.sub executable = arguments = transfer_input_files = should_transfer_files = Yes when_to_transfer_output = ON_EXIT log = logs/job.$(Cluster).$(Process).log error = logs/job.$(Cluster).$(Process).error output = logs/job.$(Cluster).$(Process).out +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 512MB request_disk = 512MB queue 1 Open (or create) this file with a terminal-based text editor (like vi or nano ) and add the executable, arguments, and input information described above. Submit and Monitor the Job \u00b6 After saving the submit file, submit the job: $ condor_submit wordcount.sub You can check the job's progress using condor_q , which will print out the status of your jobs in the queue. You can also use the command condor_watch_q to monitor the queue in real time (use the keyboard shortcut Ctrl c to exit). Once the job finishes, you should see the same counts.Alice_in_Wonderland.tsv output when you enter ls . Analyzing Multiple Books \u00b6 Now suppose you wanted to analyze multiple books - more than one at a time. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, the arguments and transfer_input_files lines from the previous submit file). This would be overly verbose and tedious. HTCondor has options that make it easy to submit many jobs from one submit file. Make a List of Inputs \u00b6 First we want to make a list of inputs that we want to use for our jobs. This should be a list where each item on the list corresponds to a job. In this example, our inputs are the different text files for different books. We want each job to analyze a different book, so our list should just contain the names of these text files. We can easily create this list by using an ls command and sending the output to a file: $ ls *.txt > book.list The book.list file now contains each of the .txt file names in the current directory. $ cat book.list Alice_in_Wonderland.txt Dracula.txt Huckleberry_Finn.txt Pride_and_Prejudice.txt Ulysses.txt Modify the Submit File \u00b6 Next, we will make changes to our submit file so that it submits a job for each book title in our list (seen in the book.list file). Create a copy of our existing submit file, which we will use for this job submission. $ cp wordcount.sub many-wordcount.sub We want to tell the queue keyword to use our list of inputs to submit jobs. The default syntax looks like this: queue <item> from <list> Open the many-wordcount.sub file with a text editor and go to the end. Following the syntax above, we modify the queue statement to fit our example: queue book from book.list This statement works like a for loop. For every item in the book.list file, HTCondor will create a job using this submit file but replacing every occurrence of $(book) with the item from book.list . The syntax $(variablename) represents a submit variable whose value will be substituted at the time of submission. Therefore, everywhere we used the name of the book in our submit file should be replaced with the variable $(book) (in the previous example, everywhere you entered \"Alice_in_Wonderland.txt\"). So the following lines in the submit file should be changed to use the variable $(book) : arguments = $(book) transfer_input_files = $(book) Submit and Monitor the Job \u00b6 We're now ready to submit all of our jobs. $ condor_submit many-wordcount.sub This will now submit five jobs (one for each book on our list). Once all five have finished running, we should see five \"counts\" files, one for each book in the directory. If you don't see all five \"counts\" files, consider investigating the log files and see if you can identify what caused that to happen.","title":"Wordcount Tutorial for Submitting Multiple Jobs"},{"location":"software_examples/python/tutorial-wordfreq/#wordcount-tutorial-for-submitting-multiple-jobs","text":"Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. The type of workflow covered in this tutorial can be used to describe workflows that take have different input files or parameters from job to job. To download the materials for this tutorial, type: $ git clone https://github.com/OSGConnect/tutorial-wordfreq","title":"Wordcount Tutorial for Submitting Multiple Jobs"},{"location":"software_examples/python/tutorial-wordfreq/#analyzing-one-book","text":"","title":"Analyzing One Book"},{"location":"software_examples/python/tutorial-wordfreq/#test-the-command","text":"We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt If you run the ls command, you should see a new file with the prefix counts which has the results of this python script. This is the output we want to produce within an HTCondor job. For now, remove the output: $ rm counts.Alice_in_Wonderland.tsv","title":"Test the Command"},{"location":"software_examples/python/tutorial-wordfreq/#create-a-submit-file","text":"To submit a single job that runs this command and analyzes the Alice's Adventures in Wonderland book, we need to translate this command into HTCondor submit file syntax. The two main components we care about are (1) the actual command and (2) the needed input files. The command gets turned into the submit file executable and arguments options: executable = wordcount.py arguments = Alice_in_Wonderland.txt The executable is the script that we want to run, and the arguments is everything else that follows the script when we run it, like the test above. The input file for this job is the Alice_in_Wonderland.txt text file. While we provided the name as in the arguments , we need to explicitly tell HTCondor to transfer the corresponding file. We include the file name in the following submit file option: transfer_input_files = Alice_in_Wonderland.txt There are other submit file options that control other aspects of the job, like where to save error and logging information, and how many resources to request per job. This tutorial has a sample submit file ( wordcount.sub ) with most of these submit file options filled in: $ cat wordcount.sub executable = arguments = transfer_input_files = should_transfer_files = Yes when_to_transfer_output = ON_EXIT log = logs/job.$(Cluster).$(Process).log error = logs/job.$(Cluster).$(Process).error output = logs/job.$(Cluster).$(Process).out +JobDurationCategory = \"Medium\" requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 512MB request_disk = 512MB queue 1 Open (or create) this file with a terminal-based text editor (like vi or nano ) and add the executable, arguments, and input information described above.","title":"Create a Submit File"},{"location":"software_examples/python/tutorial-wordfreq/#submit-and-monitor-the-job","text":"After saving the submit file, submit the job: $ condor_submit wordcount.sub You can check the job's progress using condor_q , which will print out the status of your jobs in the queue. You can also use the command condor_watch_q to monitor the queue in real time (use the keyboard shortcut Ctrl c to exit). Once the job finishes, you should see the same counts.Alice_in_Wonderland.tsv output when you enter ls .","title":"Submit and Monitor the Job"},{"location":"software_examples/python/tutorial-wordfreq/#analyzing-multiple-books","text":"Now suppose you wanted to analyze multiple books - more than one at a time. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, the arguments and transfer_input_files lines from the previous submit file). This would be overly verbose and tedious. HTCondor has options that make it easy to submit many jobs from one submit file.","title":"Analyzing Multiple Books"},{"location":"software_examples/python/tutorial-wordfreq/#make-a-list-of-inputs","text":"First we want to make a list of inputs that we want to use for our jobs. This should be a list where each item on the list corresponds to a job. In this example, our inputs are the different text files for different books. We want each job to analyze a different book, so our list should just contain the names of these text files. We can easily create this list by using an ls command and sending the output to a file: $ ls *.txt > book.list The book.list file now contains each of the .txt file names in the current directory. $ cat book.list Alice_in_Wonderland.txt Dracula.txt Huckleberry_Finn.txt Pride_and_Prejudice.txt Ulysses.txt","title":"Make a List of Inputs"},{"location":"software_examples/python/tutorial-wordfreq/#modify-the-submit-file","text":"Next, we will make changes to our submit file so that it submits a job for each book title in our list (seen in the book.list file). Create a copy of our existing submit file, which we will use for this job submission. $ cp wordcount.sub many-wordcount.sub We want to tell the queue keyword to use our list of inputs to submit jobs. The default syntax looks like this: queue <item> from <list> Open the many-wordcount.sub file with a text editor and go to the end. Following the syntax above, we modify the queue statement to fit our example: queue book from book.list This statement works like a for loop. For every item in the book.list file, HTCondor will create a job using this submit file but replacing every occurrence of $(book) with the item from book.list . The syntax $(variablename) represents a submit variable whose value will be substituted at the time of submission. Therefore, everywhere we used the name of the book in our submit file should be replaced with the variable $(book) (in the previous example, everywhere you entered \"Alice_in_Wonderland.txt\"). So the following lines in the submit file should be changed to use the variable $(book) : arguments = $(book) transfer_input_files = $(book)","title":"Modify the Submit File"},{"location":"software_examples/python/tutorial-wordfreq/#submit-and-monitor-the-job_1","text":"We're now ready to submit all of our jobs. $ condor_submit many-wordcount.sub This will now submit five jobs (one for each book on our list). Once all five have finished running, we should see five \"counts\" files, one for each book in the directory. If you don't see all five \"counts\" files, consider investigating the log files and see if you can identify what caused that to happen.","title":"Submit and Monitor the Job"},{"location":"software_examples/r/tutorial-R/","text":"Run R scripts on the OSPool \u00b6 This tutorial describes how to run a simple R script on the OSPool. We'll first run the program locally as a test. After that we'll create a submit file, submit it to the OSPool using an OSPool Access Point, and look at the results when the jobs finish. Set Up Directory and R Script \u00b6 First we'll need to create a working directory with our materials. You can either run $ git clone https://github.com/OSGConnect/tutorial-R to download the materials, OR create them yourself by typing the following: $ mkdir tutorial-R; cd tutorial-R Let's create a small script to use as a test example. Create the file hello_world.R using a text editor like nano or vim that contains the following: #!/usr/bin/env Rscript print(\"Hello World!\") The header #!/usr/bin/env Rscript indicates that if this script is run on its own, it needs to be executed using the R language (instead of Python, or bash, for example). We will run one more command that makes the script executable , meaning that it can be run directly from the command line: $ chmod +x hello_world.R Access R on the Access Point \u00b6 R is run using containers on the OSPool. To test it out on the Access Point, we can run: $ apptainer shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Other Supported R Versions \u00b6 To see a list of all containers containing R, look at the list of OSPool Supported Containers The previous command sometimes takes a minute or so to start. Once it starts, you should see the following prompt: Singularity :~/tutorial-R> Now, we can try to run R by typing R in our terminal: Singularity :~/tutorial-R> R R version 3.5.1 (2018-07-02) -- \"Feather Spray\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > You can quit out with q() . > q() Save workspace image? [y/n/c]: n Singularity :~/tutorial-R> Great! R works. We'll leave the container running for the next step. See below on how to exit from the container. Test an R Script \u00b6 To run the R script we created earlier , we just need to execute it like so: Singularity :~/tutorial-R> ./hello_world.R If this works, we will have [1] \"Hello World!\" printed to our terminal. Once we have this output, we'll exit the container for now with exit : Singularity :~/tutorial-R> exit $ Build the HTCondor Job \u00b6 Let's build a HTCondor submit file to run our script. Using a text editor, create a file called R.submit with the following text inside it: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = hello_world.R # arguments log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 The path you put in the +SingularityImage option should match whatever you used to test R above. We list the R script as the executable. The R.submit file may have included a few lines that you are unfamiliar with. For example, $(Cluster) and $(Process) are variables that will be replaced with the job's cluster and process numbers - these are automatically assigned by. This is useful when you have many jobs submitted in the same file. Any output and errors will be placed in a separate file for each job. Submit and View Output \u00b6 Finally, submit the job! $ condor_submit R.submit Submitting job(s). 1 job(s) submitted to cluster 3796250. $ condor_q alice -- Schedd: ap40.uw.osg-htc.org: <192.170.227.22:9618?... @ 04/13/23 09:51:04 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 3796250 4/13 09:50 _ _ 1 1 3796250.0 ... You can follow the status of your job cluster with the condor_watch_q command, which shows condor_q output that refreshes each 5 seconds. Press control-C to stop watching. Since our jobs prints to standard out, we can check the output files. Let's see what one looks like: $ cat R.out.3796250.0 [1] \"Hello World!\" Related Guides for Running R Code \u00b6 Use Custom Libraries with R Scale Up your R jobs","title":"Run R scripts on the OSPool"},{"location":"software_examples/r/tutorial-R/#run-r-scripts-on-the-ospool","text":"This tutorial describes how to run a simple R script on the OSPool. We'll first run the program locally as a test. After that we'll create a submit file, submit it to the OSPool using an OSPool Access Point, and look at the results when the jobs finish.","title":"Run R scripts on the OSPool"},{"location":"software_examples/r/tutorial-R/#set-up-directory-and-r-script","text":"First we'll need to create a working directory with our materials. You can either run $ git clone https://github.com/OSGConnect/tutorial-R to download the materials, OR create them yourself by typing the following: $ mkdir tutorial-R; cd tutorial-R Let's create a small script to use as a test example. Create the file hello_world.R using a text editor like nano or vim that contains the following: #!/usr/bin/env Rscript print(\"Hello World!\") The header #!/usr/bin/env Rscript indicates that if this script is run on its own, it needs to be executed using the R language (instead of Python, or bash, for example). We will run one more command that makes the script executable , meaning that it can be run directly from the command line: $ chmod +x hello_world.R","title":"Set Up Directory and R Script"},{"location":"software_examples/r/tutorial-R/#access-r-on-the-access-point","text":"R is run using containers on the OSPool. To test it out on the Access Point, we can run: $ apptainer shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0","title":"Access R on the Access Point"},{"location":"software_examples/r/tutorial-R/#other-supported-r-versions","text":"To see a list of all containers containing R, look at the list of OSPool Supported Containers The previous command sometimes takes a minute or so to start. Once it starts, you should see the following prompt: Singularity :~/tutorial-R> Now, we can try to run R by typing R in our terminal: Singularity :~/tutorial-R> R R version 3.5.1 (2018-07-02) -- \"Feather Spray\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > You can quit out with q() . > q() Save workspace image? [y/n/c]: n Singularity :~/tutorial-R> Great! R works. We'll leave the container running for the next step. See below on how to exit from the container.","title":"Other Supported R Versions"},{"location":"software_examples/r/tutorial-R/#test-an-r-script","text":"To run the R script we created earlier , we just need to execute it like so: Singularity :~/tutorial-R> ./hello_world.R If this works, we will have [1] \"Hello World!\" printed to our terminal. Once we have this output, we'll exit the container for now with exit : Singularity :~/tutorial-R> exit $","title":"Test an R Script"},{"location":"software_examples/r/tutorial-R/#build-the-htcondor-job","text":"Let's build a HTCondor submit file to run our script. Using a text editor, create a file called R.submit with the following text inside it: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = hello_world.R # arguments log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 The path you put in the +SingularityImage option should match whatever you used to test R above. We list the R script as the executable. The R.submit file may have included a few lines that you are unfamiliar with. For example, $(Cluster) and $(Process) are variables that will be replaced with the job's cluster and process numbers - these are automatically assigned by. This is useful when you have many jobs submitted in the same file. Any output and errors will be placed in a separate file for each job.","title":"Build the HTCondor Job"},{"location":"software_examples/r/tutorial-R/#submit-and-view-output","text":"Finally, submit the job! $ condor_submit R.submit Submitting job(s). 1 job(s) submitted to cluster 3796250. $ condor_q alice -- Schedd: ap40.uw.osg-htc.org: <192.170.227.22:9618?... @ 04/13/23 09:51:04 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice ID: 3796250 4/13 09:50 _ _ 1 1 3796250.0 ... You can follow the status of your job cluster with the condor_watch_q command, which shows condor_q output that refreshes each 5 seconds. Press control-C to stop watching. Since our jobs prints to standard out, we can check the output files. Let's see what one looks like: $ cat R.out.3796250.0 [1] \"Hello World!\"","title":"Submit and View Output"},{"location":"software_examples/r/tutorial-R/#related-guides-for-running-r-code","text":"Use Custom Libraries with R Scale Up your R jobs","title":"Related Guides for Running R Code"},{"location":"software_examples/r/tutorial-R-addlibSNA/","text":"Use R Packages in your R Jobs \u00b6 Often we may need to add R external libraries that are not part of the base R installation. This tutorial describes how to create custom R libraries for use in jobs on the OSPool. Background \u00b6 The material in this tutorial builds upon the Run R Scripts on the OSPool tutorial. If you are not already familiar with how to run R jobs on the OSPool, please see that tutorial first for a general introduction. Setup Directory and R Script \u00b6 First we'll need to create a working directory, you can either run $ git clone https://github.com/OSGConnect/tutorial-R-addlib or type the following: $ mkdir tutorial-R-addlib $ cd tutorial-R-addlib Similar to the general R tutorial, we will create a script to use as a test example. If you did not clone the tutorial, create a script called hello_world.R that contains the following: #!/usr/bin/env Rscript library(cowsay) say(\"Hello World!\", \"cow\") We will run one more command that makes the script executable , meaning that it can be run directly from the command line: $ chmod +x hello_world.R Create a Custom Container with R Packages \u00b6 Using the same container that we used for the general R tutorial, we will add the package we want to use (in this case, the cowsay package) to create a new container that we can use for our jobs. The new container will be generated from a \"definition\" file. If it isn't already present, create a file called cowsay.def that has the following lines: Bootstrap: docker From: opensciencegrid/osgvo-r:3.5.0 %post R -e \"install.packages('cowsay', dependencies=TRUE, repos='http://cran.rstudio.com/')\" This file basically says that we want to start with one of the existing OSPool R containers and add the cowsay package from CRAN. To create the new container, set the following variables: $ export TMPDIR=$HOME $ export APPTAINER_CACHE_DIR=$HOME And then run this command: apptainer build cowsay-test.sif cowsay.def It may take 5-10 minutes to run. Once complete, if you run ls , you should see a file in your current directory called cowsay-test.sif . This is the new container. Building containers can be a new skill and slightly different for different packages! We recommend looking at our container guides and container training materials to learn more -- these are both linked from our main guides page. There are also some additional tips at the end of this tutorial on building containers with R packages. Test Custom Container and R Script \u00b6 Start the container you created by running: $ apptainer shell cowsay-test.sif Now we can test our R script: Singularity :~/tutorial-R-addlib> ./hello_world.R If this works, we will have a message with a cow printed to our terminal. Once we have this output, we'll exit the container for now with exit : Singularity :~/tutorial-R-addlib> exit $ Build the HTCondor Job \u00b6 For this job, we want to use the custom container we just created. For efficiency, it is best to transfer this to the job using the OSDF . If you want to use the container you just built, copy it to the appropriate directory listed here, based on which Access Point you are using. Our submit file, R.submit should then look like this: +SingularityImage = \"osdf://osgconnect/public/osg/tutorial-R-addlib/cowsay-test.sif\" executable = hello_world.R # arguments log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 Change the osdf:// link in the submit file to be right for YOUR Access Point and username, if you are using your own container file. Reminder: Files placed in the OSDF can be copied to other data spaces (\"caches\") where they are NOT UPDATED. If you make a new container to use with your jobs, make sure to give it a different name or put it at a different path than the previous container. You will not be able to replace the exact path of the existing container. Submit Jobs and Review Output \u00b6 Now we are ready to submit the job: $ condor_submit R.submit and check the job status: $ condor_q Once the job finished running, check the output file as before. They should look like this: $ cat R.out.0000.0 ----- Hello World! ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || Tips for Building Containers with R Packages \u00b6 There is a lot of variety in how to build custom containers! The two main decisions you need to make are a) what to use as your \"base\" or starting container and what packages to install. There is a useful overview of building containers from our container training, linked on our training page . Base Containers \u00b6 In this guide we used one of the existing OSPool R containers. You can see the other versions of R that we support on our list of OSPool Supported Containers Another good option for a base container are the \"rocker\" Docker containers: Rocker on DockerHub To use a different container as the base container, you just change the top of the definition file. So to use the rocker tidyverse container as my starting point, I would have a definition file header like this: Bootstrap: docker From: rocker/tidyverse:4.1.3 When using containers from DockerHub, it's a good idea to pick a version (look at the \"Tags\" tab for options). Above, this container would be version 4.1.3 of R. Installing Packages \u00b6 The sample definition file from this tutorial installed one package. If you have multiple packages, you can change the \"install.packages\" command to install multiple packages: %post R -e \"install.packages(c('cowsay','here'), dependencies=TRUE, repos='http://cran.rstudio.com/')\" If your base container is one of the \"rocker\" containers , you can use a different tool to install packages that looks like this: %post install2.r cowsay or for multiple packages: %post install2.r cowsay here Remember, you only need to install packages that aren't already in the container. If you start with the tidyverse container, you don't need to install ggplot2 or dplyr - those are already in the container and you would be adding packages on top.","title":"Use External Packages in your R Jobs"},{"location":"software_examples/r/tutorial-R-addlibSNA/#use-r-packages-in-your-r-jobs","text":"Often we may need to add R external libraries that are not part of the base R installation. This tutorial describes how to create custom R libraries for use in jobs on the OSPool.","title":"Use R Packages in your R Jobs"},{"location":"software_examples/r/tutorial-R-addlibSNA/#background","text":"The material in this tutorial builds upon the Run R Scripts on the OSPool tutorial. If you are not already familiar with how to run R jobs on the OSPool, please see that tutorial first for a general introduction.","title":"Background"},{"location":"software_examples/r/tutorial-R-addlibSNA/#setup-directory-and-r-script","text":"First we'll need to create a working directory, you can either run $ git clone https://github.com/OSGConnect/tutorial-R-addlib or type the following: $ mkdir tutorial-R-addlib $ cd tutorial-R-addlib Similar to the general R tutorial, we will create a script to use as a test example. If you did not clone the tutorial, create a script called hello_world.R that contains the following: #!/usr/bin/env Rscript library(cowsay) say(\"Hello World!\", \"cow\") We will run one more command that makes the script executable , meaning that it can be run directly from the command line: $ chmod +x hello_world.R","title":"Setup Directory and R Script"},{"location":"software_examples/r/tutorial-R-addlibSNA/#create-a-custom-container-with-r-packages","text":"Using the same container that we used for the general R tutorial, we will add the package we want to use (in this case, the cowsay package) to create a new container that we can use for our jobs. The new container will be generated from a \"definition\" file. If it isn't already present, create a file called cowsay.def that has the following lines: Bootstrap: docker From: opensciencegrid/osgvo-r:3.5.0 %post R -e \"install.packages('cowsay', dependencies=TRUE, repos='http://cran.rstudio.com/')\" This file basically says that we want to start with one of the existing OSPool R containers and add the cowsay package from CRAN. To create the new container, set the following variables: $ export TMPDIR=$HOME $ export APPTAINER_CACHE_DIR=$HOME And then run this command: apptainer build cowsay-test.sif cowsay.def It may take 5-10 minutes to run. Once complete, if you run ls , you should see a file in your current directory called cowsay-test.sif . This is the new container. Building containers can be a new skill and slightly different for different packages! We recommend looking at our container guides and container training materials to learn more -- these are both linked from our main guides page. There are also some additional tips at the end of this tutorial on building containers with R packages.","title":"Create a Custom Container with R Packages"},{"location":"software_examples/r/tutorial-R-addlibSNA/#test-custom-container-and-r-script","text":"Start the container you created by running: $ apptainer shell cowsay-test.sif Now we can test our R script: Singularity :~/tutorial-R-addlib> ./hello_world.R If this works, we will have a message with a cow printed to our terminal. Once we have this output, we'll exit the container for now with exit : Singularity :~/tutorial-R-addlib> exit $","title":"Test Custom Container and R Script"},{"location":"software_examples/r/tutorial-R-addlibSNA/#build-the-htcondor-job","text":"For this job, we want to use the custom container we just created. For efficiency, it is best to transfer this to the job using the OSDF . If you want to use the container you just built, copy it to the appropriate directory listed here, based on which Access Point you are using. Our submit file, R.submit should then look like this: +SingularityImage = \"osdf://osgconnect/public/osg/tutorial-R-addlib/cowsay-test.sif\" executable = hello_world.R # arguments log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 Change the osdf:// link in the submit file to be right for YOUR Access Point and username, if you are using your own container file. Reminder: Files placed in the OSDF can be copied to other data spaces (\"caches\") where they are NOT UPDATED. If you make a new container to use with your jobs, make sure to give it a different name or put it at a different path than the previous container. You will not be able to replace the exact path of the existing container.","title":"Build the HTCondor Job"},{"location":"software_examples/r/tutorial-R-addlibSNA/#submit-jobs-and-review-output","text":"Now we are ready to submit the job: $ condor_submit R.submit and check the job status: $ condor_q Once the job finished running, check the output file as before. They should look like this: $ cat R.out.0000.0 ----- Hello World! ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || ||","title":"Submit Jobs and Review Output"},{"location":"software_examples/r/tutorial-R-addlibSNA/#tips-for-building-containers-with-r-packages","text":"There is a lot of variety in how to build custom containers! The two main decisions you need to make are a) what to use as your \"base\" or starting container and what packages to install. There is a useful overview of building containers from our container training, linked on our training page .","title":"Tips for Building Containers with R Packages"},{"location":"software_examples/r/tutorial-R-addlibSNA/#base-containers","text":"In this guide we used one of the existing OSPool R containers. You can see the other versions of R that we support on our list of OSPool Supported Containers Another good option for a base container are the \"rocker\" Docker containers: Rocker on DockerHub To use a different container as the base container, you just change the top of the definition file. So to use the rocker tidyverse container as my starting point, I would have a definition file header like this: Bootstrap: docker From: rocker/tidyverse:4.1.3 When using containers from DockerHub, it's a good idea to pick a version (look at the \"Tags\" tab for options). Above, this container would be version 4.1.3 of R.","title":"Base Containers"},{"location":"software_examples/r/tutorial-R-addlibSNA/#installing-packages","text":"The sample definition file from this tutorial installed one package. If you have multiple packages, you can change the \"install.packages\" command to install multiple packages: %post R -e \"install.packages(c('cowsay','here'), dependencies=TRUE, repos='http://cran.rstudio.com/')\" If your base container is one of the \"rocker\" containers , you can use a different tool to install packages that looks like this: %post install2.r cowsay or for multiple packages: %post install2.r cowsay here Remember, you only need to install packages that aren't already in the container. If you start with the tidyverse container, you don't need to install ggplot2 or dplyr - those are already in the container and you would be adding packages on top.","title":"Installing Packages"},{"location":"software_examples/r/tutorial-ScalingUp-R/","text":"Scaling up compute resources \u00b6 Scaling up the computational resources is a big advantage for doing certain large scale calculations on OSPool. Consider the extensive sampling for a multi-dimensional Monte Carlo integration or molecular dynamics simulation with several initial conditions. These type of calculations require submitting a lot of jobs. About a million CPU hours per day are available to OSPool users on an opportunistic basis. Learning how to scale up and control large numbers of jobs is key to realizing the full potential of distributed high throughput computing on the OSPool. In this tutorial, we will see how to scale up calculations for a simple example. To download the materials for this tutorial, use the command $ git clone https://github.com/OSGConnect/tutorial-ScalingUp-R Background \u00b6 For this example, we will use computational methods to estimate \u03c0. First, we will define a square inscribed by a unit circle from which we will randomly sample points. The ratio of the points outside the circle to the points in the circle is calculated, which approaches \u03c0/4. This method converges extremely slowly, which makes it great for a CPU-intensive exercise (but bad for a real estimation!). Set up an R Job \u00b6 If you downloaded the tutorial files, you should see the directory \"tutorial-ScalingUp-R\" when you run the ls command. This directory contains the files used in this tutorial. Alternatively, you can write the necessary files from scratch. In that case, create a working directory using the command $ mkdir tutorial-ScalingUp-R Either way, move into the directory before continuing: $ cd tutorial-ScalingUp-R Create and test an R Script \u00b6 Our code is a simple R script that does the estimation. It takes in a single argument in order to differentiate the jobs. The code for the script is contained in the file mcpi.R . If you didn't download the tutorial files, create an R script called mcpi.R and add the following contents: #!/usr/bin/env Rscript args = commandArgs(trailingOnly = TRUE) iternum = as.numeric(args[[1]]) + 100 montecarloPi <- function(trials) { count = 0 for(i in 1:trials) { if((runif(1,0,1)^2 + runif(1,0,1)^2)<1) { count = count + 1 } } return((count*4)/trials) } montecarloPi(iternum) The header at the top of the file (the line starting with #! ) indicates that this script is meant to be run using R. If we were running a more intensive script, we would want to test our pipeline with a shortened, test script first. If you want to test the script, start an R container, and then run the script using Rscript . For example: $ apptainer shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Singularity :~/tutorial-ScalingUp-R> Rscript mcpi.R 10 [1] 3.14 Singularity :~/tutorial-ScalingUp-R> exit $ Create a Submit File and Log Directories \u00b6 Now that we have our R script written and tested, we can begin building the submit file for our job. If we want to submit several jobs, we need to track log, output, and error files for each job. An easy way to do this is to use the Cluster and Process ID values assigned by HTCondor to create unique files for each job in our overall workflow. In this example, the submit file is called R.submit . If you did not download the tutorial files, create a submit file named R.submit and add the following contents: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = mcpi.R arguments = $(Process) #transfer_input_files = should_transfer_files = YES when_to_transfer_output = ON_EXIT log = logs/job.log.$(Cluster).$(Process) error = logs/job.error.$(Cluster).$(Process) output = output/mcpi.out.$(Cluster).$(Process) request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 100 If you did not download the tutorial files, you will also need to create the logs and output directories to hold the files that will be created for each job. You can create both directories at once with the command $ mkdir logs output There are several items to note about this submit file: The queue 100 statement in the submit file. This tells Condor to enqueue 100 copies of this job as one cluster. The submit variables $(Cluster) and $(Process) . These are used to specify unique output files. HTCondor will replace these with the Cluster and Process ID numbers for each individual process within the cluster. The $(Process) variable is also passed as an argument to our R script. Submit the Jobs \u00b6 Now it is time to submit our job! You'll see something like the following upon submission: $ condor_submit R.submit Submitting job(s)......................... 100 job(s) submitted to cluster 837. Apply your condor_q knowledge to see the progress of these jobs. Check your logs folder to see the error and HTCondor log files and the output folder to see the results of the scripts. Post Process \u00b6 Once the jobs are completed, you can use the information in the output files to calculate an average of all of our computed estimates of \u03c0. To see this, we can use the command: $ cat output/mcpi*.out* | awk '{ sum += $2; print $2\" \"NR} END { print \"---------------\\n Grand Average = \" sum/NR }' Key Points \u00b6 Scaling up the number of jobs is crucial for taking full advantage of the computational resources of the OSPool. Changing the queue statement allows the user to scale up the resources. The arguments option can be used to pass parameters to a job script. The submit variables $(Cluster) and $(Process) can be used to name log files uniquely.","title":"Scaling up compute resources"},{"location":"software_examples/r/tutorial-ScalingUp-R/#scaling-up-compute-resources","text":"Scaling up the computational resources is a big advantage for doing certain large scale calculations on OSPool. Consider the extensive sampling for a multi-dimensional Monte Carlo integration or molecular dynamics simulation with several initial conditions. These type of calculations require submitting a lot of jobs. About a million CPU hours per day are available to OSPool users on an opportunistic basis. Learning how to scale up and control large numbers of jobs is key to realizing the full potential of distributed high throughput computing on the OSPool. In this tutorial, we will see how to scale up calculations for a simple example. To download the materials for this tutorial, use the command $ git clone https://github.com/OSGConnect/tutorial-ScalingUp-R","title":"Scaling up compute resources"},{"location":"software_examples/r/tutorial-ScalingUp-R/#background","text":"For this example, we will use computational methods to estimate \u03c0. First, we will define a square inscribed by a unit circle from which we will randomly sample points. The ratio of the points outside the circle to the points in the circle is calculated, which approaches \u03c0/4. This method converges extremely slowly, which makes it great for a CPU-intensive exercise (but bad for a real estimation!).","title":"Background"},{"location":"software_examples/r/tutorial-ScalingUp-R/#set-up-an-r-job","text":"If you downloaded the tutorial files, you should see the directory \"tutorial-ScalingUp-R\" when you run the ls command. This directory contains the files used in this tutorial. Alternatively, you can write the necessary files from scratch. In that case, create a working directory using the command $ mkdir tutorial-ScalingUp-R Either way, move into the directory before continuing: $ cd tutorial-ScalingUp-R","title":"Set up an R Job"},{"location":"software_examples/r/tutorial-ScalingUp-R/#create-and-test-an-r-script","text":"Our code is a simple R script that does the estimation. It takes in a single argument in order to differentiate the jobs. The code for the script is contained in the file mcpi.R . If you didn't download the tutorial files, create an R script called mcpi.R and add the following contents: #!/usr/bin/env Rscript args = commandArgs(trailingOnly = TRUE) iternum = as.numeric(args[[1]]) + 100 montecarloPi <- function(trials) { count = 0 for(i in 1:trials) { if((runif(1,0,1)^2 + runif(1,0,1)^2)<1) { count = count + 1 } } return((count*4)/trials) } montecarloPi(iternum) The header at the top of the file (the line starting with #! ) indicates that this script is meant to be run using R. If we were running a more intensive script, we would want to test our pipeline with a shortened, test script first. If you want to test the script, start an R container, and then run the script using Rscript . For example: $ apptainer shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Singularity :~/tutorial-ScalingUp-R> Rscript mcpi.R 10 [1] 3.14 Singularity :~/tutorial-ScalingUp-R> exit $","title":"Create and test an R Script"},{"location":"software_examples/r/tutorial-ScalingUp-R/#create-a-submit-file-and-log-directories","text":"Now that we have our R script written and tested, we can begin building the submit file for our job. If we want to submit several jobs, we need to track log, output, and error files for each job. An easy way to do this is to use the Cluster and Process ID values assigned by HTCondor to create unique files for each job in our overall workflow. In this example, the submit file is called R.submit . If you did not download the tutorial files, create a submit file named R.submit and add the following contents: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = mcpi.R arguments = $(Process) #transfer_input_files = should_transfer_files = YES when_to_transfer_output = ON_EXIT log = logs/job.log.$(Cluster).$(Process) error = logs/job.error.$(Cluster).$(Process) output = output/mcpi.out.$(Cluster).$(Process) request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 100 If you did not download the tutorial files, you will also need to create the logs and output directories to hold the files that will be created for each job. You can create both directories at once with the command $ mkdir logs output There are several items to note about this submit file: The queue 100 statement in the submit file. This tells Condor to enqueue 100 copies of this job as one cluster. The submit variables $(Cluster) and $(Process) . These are used to specify unique output files. HTCondor will replace these with the Cluster and Process ID numbers for each individual process within the cluster. The $(Process) variable is also passed as an argument to our R script.","title":"Create a Submit File and Log Directories"},{"location":"software_examples/r/tutorial-ScalingUp-R/#submit-the-jobs","text":"Now it is time to submit our job! You'll see something like the following upon submission: $ condor_submit R.submit Submitting job(s)......................... 100 job(s) submitted to cluster 837. Apply your condor_q knowledge to see the progress of these jobs. Check your logs folder to see the error and HTCondor log files and the output folder to see the results of the scripts.","title":"Submit the Jobs"},{"location":"software_examples/r/tutorial-ScalingUp-R/#post-process","text":"Once the jobs are completed, you can use the information in the output files to calculate an average of all of our computed estimates of \u03c0. To see this, we can use the command: $ cat output/mcpi*.out* | awk '{ sum += $2; print $2\" \"NR} END { print \"---------------\\n Grand Average = \" sum/NR }'","title":"Post Process"},{"location":"software_examples/r/tutorial-ScalingUp-R/#key-points","text":"Scaling up the number of jobs is crucial for taking full advantage of the computational resources of the OSPool. Changing the queue statement allows the user to scale up the resources. The arguments option can be used to pass parameters to a job script. The submit variables $(Cluster) and $(Process) can be used to name log files uniquely.","title":"Key Points"},{"location":"software_examples/r/tutorial-spills-R/","text":"Analyzing Chemical Spills Datasets (.csv files) \u00b6 An OSPool Tutorial \u00b6 Spills of hazardous materials, like petroleum, mercury, and battery acid, that can impact water and land quality are required to be reported to the United State's government by law. In this tutorial, we will analyze records provided by the state of New York on occurrences of spills of hazardous materials that occurred from 1950 to 2019. The data used in this tutorial was collected from https://catalog.data.gov/dataset/spill-incidents/resource/a8f9d3c8-c3fa-4ca1-a97a-55e55ca6f8c0 and modified for teaching purposes. To access all of the materials to complete this tutorial, first log into your OSPool access point and run the following command: git clone https://github.com/OSGConnect/tutorial-spills-R/ . Step 1: Get to Know Hazardous Spills Dataset \u00b6 Let's explore the data files that we will be analyzing. Before we do so, we must make sure we are in the tutorial directory ( tutorial-spills-R/ ). We can do this by printing your working directory ( pwd ): pwd We should see something similar to /home/jovyan/tutorial-spills-R/ , where jovyan could alternatively be your OSG account username. Next, let's navigate to our /data directory and list ( ls ) the files inside of it: cd data/ ls We should see seven .csv files, one for each decade between 1950-2019. To explore the contents of these files, we can use commands like head -n 5 <fileName> to view the first 5 lines of our data files. head -n 5 spills_1980_1989.csv We can also use the navigation bar on the left side of your notebook to double-click and open each comma-separated value (\"csv\") .csv file and see it in a table format, instead of a traditional command line rendering above. Step 2: Prepare the R Executable \u00b6 Next, we need to create an R script to analyze our datasets. An example of an R script can be found in our main tutorial directory, so let's navigate there: cd ../ # change directory to move one up ls # list files cat spill_calculation.r Then let us print the contents of our executable script: cat spill_calculation.r This script will read in different datasets as arguments and then will carry out summary statistics to print out the number of spills recorded per decade and the total size (in gallons) of the hazardous spills. Step 3: Prepare Portable Software \u00b6 Some common software, like R, is provided by OSG using containers. Because of this, you do not need to install R yourself, you will just tell HTCondor what container to use for your jobs. Additionally, this tutorial just uses base-R and no special libraries, but if you need libraries (e.g., tidyverse, ggplot2) you can always install them in your R container. A list of containers and other software provided by OSG staff can be found on our website https://portal.osg-htc.org/documentation/ , along with resources for learning how to add libraries to your container. We will be using the R container for R 3.5.0, which is accessible under /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 , so we must make sure to tell HTCondor to fetch this container when starting each of our jobs. To learn how to tell HTCondor to do this, see below. Step 4: Prepare and Submit an HTCondor Submit File for One Test Job \u00b6 The HTCondor submit file tells the HTCondor how you would like your job to be run on your behalf. For example, you should specify what executable you want to run, if you want a container/the name of that container, the resources you would like available to your job, and any special requirements. Step 4A: Prepare and Submit an HTCondor Submit File \u00b6 A sample submit file to analyze our smallest dataset, spills_1950_1959.csv , might look like: cat R.submit We can submit this job using condor_submit <SubmitFile> : condor_submit R.submit We can check on the status of our job in HTCondor's queue by running: condor_q Once our job is done running, it will leave HTCondor's queue automatically. Step 4B: Review Test Job Results \u00b6 Once our job is done running, we can check the results by looking in our output folder: cat output/spills.out We should see that from 1950-1959, New York recorded five spills that totalled less than 0 recorded gallons. Step 5: Scale Out Your Workflow to Analyze Many Datasets \u00b6 We just prepared and ran one job analyzing the spills_1950_1959.csv dataset! But now, we want to analyze the remaining 6 datasets. Luckily, HTCondor is very helpful when it comes to rapidly queueing many small jobs! To do so, we will update our submit file to use the queue <variable> from <list> syntax. But before we do this, we need to create a list of the files we want to queue a job for: ls data > list_of_datasets.txt cat list_of_datasets.txt Great! Now we have a list of the files we want analyzed, where each file is on it's own seperate line. Step 5A: Update submit file to queue a job for each dataset \u00b6 Now, let's modify the queue line of our submit file to use the new queue syntax. For this, we can choose almost any variable name, so for simplicity, let's choose dataset such that we have queue dataset from list_of_datasets.txt . We can then call this new variable, dataset , elsewhere in our submit file by wrapping it with $() like so: $(dataset) . Our updated submit file might look like this: cat many-R.submit Step 5B: Submit Many Jobs \u00b6 Now we can submit our new submit file using condor_submit again: condor_submit many-R.submit Notice that we have now queued 7 jobs using one submit file! Step 5C: Analysis Completed! \u00b6 We can check on the status of our 7 jobs using condor_q : condor_q Once our jobs are done, we can also review our output files: cat output/*.csv.out In a few minutes, we were able to take our R script and run several jobs to analyze all of our real-world data. Congratulations!","title":"Analyzing .csv Data with R"},{"location":"software_examples/r/tutorial-spills-R/#analyzing-chemical-spills-datasets-csv-files","text":"","title":"Analyzing Chemical Spills Datasets (.csv files)"},{"location":"software_examples/r/tutorial-spills-R/#an-ospool-tutorial","text":"Spills of hazardous materials, like petroleum, mercury, and battery acid, that can impact water and land quality are required to be reported to the United State's government by law. In this tutorial, we will analyze records provided by the state of New York on occurrences of spills of hazardous materials that occurred from 1950 to 2019. The data used in this tutorial was collected from https://catalog.data.gov/dataset/spill-incidents/resource/a8f9d3c8-c3fa-4ca1-a97a-55e55ca6f8c0 and modified for teaching purposes. To access all of the materials to complete this tutorial, first log into your OSPool access point and run the following command: git clone https://github.com/OSGConnect/tutorial-spills-R/ .","title":"An OSPool Tutorial"},{"location":"software_examples/r/tutorial-spills-R/#step-1-get-to-know-hazardous-spills-dataset","text":"Let's explore the data files that we will be analyzing. Before we do so, we must make sure we are in the tutorial directory ( tutorial-spills-R/ ). We can do this by printing your working directory ( pwd ): pwd We should see something similar to /home/jovyan/tutorial-spills-R/ , where jovyan could alternatively be your OSG account username. Next, let's navigate to our /data directory and list ( ls ) the files inside of it: cd data/ ls We should see seven .csv files, one for each decade between 1950-2019. To explore the contents of these files, we can use commands like head -n 5 <fileName> to view the first 5 lines of our data files. head -n 5 spills_1980_1989.csv We can also use the navigation bar on the left side of your notebook to double-click and open each comma-separated value (\"csv\") .csv file and see it in a table format, instead of a traditional command line rendering above.","title":"Step 1: Get to Know Hazardous Spills Dataset"},{"location":"software_examples/r/tutorial-spills-R/#step-2-prepare-the-r-executable","text":"Next, we need to create an R script to analyze our datasets. An example of an R script can be found in our main tutorial directory, so let's navigate there: cd ../ # change directory to move one up ls # list files cat spill_calculation.r Then let us print the contents of our executable script: cat spill_calculation.r This script will read in different datasets as arguments and then will carry out summary statistics to print out the number of spills recorded per decade and the total size (in gallons) of the hazardous spills.","title":"Step 2: Prepare the R Executable"},{"location":"software_examples/r/tutorial-spills-R/#step-3-prepare-portable-software","text":"Some common software, like R, is provided by OSG using containers. Because of this, you do not need to install R yourself, you will just tell HTCondor what container to use for your jobs. Additionally, this tutorial just uses base-R and no special libraries, but if you need libraries (e.g., tidyverse, ggplot2) you can always install them in your R container. A list of containers and other software provided by OSG staff can be found on our website https://portal.osg-htc.org/documentation/ , along with resources for learning how to add libraries to your container. We will be using the R container for R 3.5.0, which is accessible under /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 , so we must make sure to tell HTCondor to fetch this container when starting each of our jobs. To learn how to tell HTCondor to do this, see below.","title":"Step 3: Prepare Portable Software"},{"location":"software_examples/r/tutorial-spills-R/#step-4-prepare-and-submit-an-htcondor-submit-file-for-one-test-job","text":"The HTCondor submit file tells the HTCondor how you would like your job to be run on your behalf. For example, you should specify what executable you want to run, if you want a container/the name of that container, the resources you would like available to your job, and any special requirements.","title":"Step 4: Prepare and Submit an HTCondor Submit File for One Test Job"},{"location":"software_examples/r/tutorial-spills-R/#step-4a-prepare-and-submit-an-htcondor-submit-file","text":"A sample submit file to analyze our smallest dataset, spills_1950_1959.csv , might look like: cat R.submit We can submit this job using condor_submit <SubmitFile> : condor_submit R.submit We can check on the status of our job in HTCondor's queue by running: condor_q Once our job is done running, it will leave HTCondor's queue automatically.","title":"Step 4A: Prepare and Submit an HTCondor Submit File"},{"location":"software_examples/r/tutorial-spills-R/#step-4b-review-test-job-results","text":"Once our job is done running, we can check the results by looking in our output folder: cat output/spills.out We should see that from 1950-1959, New York recorded five spills that totalled less than 0 recorded gallons.","title":"Step 4B: Review Test Job Results"},{"location":"software_examples/r/tutorial-spills-R/#step-5-scale-out-your-workflow-to-analyze-many-datasets","text":"We just prepared and ran one job analyzing the spills_1950_1959.csv dataset! But now, we want to analyze the remaining 6 datasets. Luckily, HTCondor is very helpful when it comes to rapidly queueing many small jobs! To do so, we will update our submit file to use the queue <variable> from <list> syntax. But before we do this, we need to create a list of the files we want to queue a job for: ls data > list_of_datasets.txt cat list_of_datasets.txt Great! Now we have a list of the files we want analyzed, where each file is on it's own seperate line.","title":"Step 5: Scale Out Your Workflow to Analyze Many Datasets"},{"location":"software_examples/r/tutorial-spills-R/#step-5a-update-submit-file-to-queue-a-job-for-each-dataset","text":"Now, let's modify the queue line of our submit file to use the new queue syntax. For this, we can choose almost any variable name, so for simplicity, let's choose dataset such that we have queue dataset from list_of_datasets.txt . We can then call this new variable, dataset , elsewhere in our submit file by wrapping it with $() like so: $(dataset) . Our updated submit file might look like this: cat many-R.submit","title":"Step 5A: Update submit file to queue a job for each dataset"},{"location":"software_examples/r/tutorial-spills-R/#step-5b-submit-many-jobs","text":"Now we can submit our new submit file using condor_submit again: condor_submit many-R.submit Notice that we have now queued 7 jobs using one submit file!","title":"Step 5B: Submit Many Jobs"},{"location":"software_examples/r/tutorial-spills-R/#step-5c-analysis-completed","text":"We can check on the status of our 7 jobs using condor_q : condor_q Once our jobs are done, we can also review our output files: cat output/*.csv.out In a few minutes, we were able to take our R script and run several jobs to analyze all of our real-world data. Congratulations!","title":"Step 5C: Analysis Completed!"},{"location":"support_and_training/support/getting-help-from-RCFs/","text":"Email, Office Hours, and 1-1 Meetings \u00b6 There are multiple ways to get help from OSG\u2019s Research Computing Facilitators. Get in touch anytime! To help researchers effectively utilize large-scale computing, our Research Computing Facilitators (RCFs) are here to answer questions and provide guidance and support. If we're not able to help with a specific problem, we will do our best to connect you with another group or service that can. We don\u2019t expect that you should be able to address all of your questions by consulting our documentation , searching online, or just working through things on your own. Please utilize the methods below if you are stuck or have questions. Help via Email \u00b6 We provide ongoing support via email to support@osg-htc.org . You can typically expect a first response within a few business hours. support@osg-htc.org Virtual Office Hours \u00b6 Drop-in for live help: Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room in the welcome message when you log into an OSG-managed Access Point, or in the signature of a support email from an RCF. Once you arrive in the room, please sign in. Sign-in for office hours Cancellations will be announced via email. If the times above don\u2019t work for you, please email us at our usual support address to schedule a separate meeting. Make an Appointment \u00b6 We are happy to arrange meetings outside of designated Office Hours. Email us to schedule a time to meet! support@osg-htc.org Training Opportunities \u00b6 The RCF team runs regular new user training on the first Tuesday of the month and a special topic training on the third Tuesday of the month. See upcoming training dates, registration information, and materials on our training page. OSPool Training page","title":"Email, Office Hours, and 1-1 Meetings "},{"location":"support_and_training/support/getting-help-from-RCFs/#email-office-hours-and-1-1-meetings","text":"There are multiple ways to get help from OSG\u2019s Research Computing Facilitators. Get in touch anytime! To help researchers effectively utilize large-scale computing, our Research Computing Facilitators (RCFs) are here to answer questions and provide guidance and support. If we're not able to help with a specific problem, we will do our best to connect you with another group or service that can. We don\u2019t expect that you should be able to address all of your questions by consulting our documentation , searching online, or just working through things on your own. Please utilize the methods below if you are stuck or have questions.","title":"Email, Office Hours, and 1-1 Meetings"},{"location":"support_and_training/support/getting-help-from-RCFs/#help-via-email","text":"We provide ongoing support via email to support@osg-htc.org . You can typically expect a first response within a few business hours. support@osg-htc.org","title":"Help via Email"},{"location":"support_and_training/support/getting-help-from-RCFs/#virtual-office-hours","text":"Drop-in for live help: Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room in the welcome message when you log into an OSG-managed Access Point, or in the signature of a support email from an RCF. Once you arrive in the room, please sign in. Sign-in for office hours Cancellations will be announced via email. If the times above don\u2019t work for you, please email us at our usual support address to schedule a separate meeting.","title":"Virtual Office Hours"},{"location":"support_and_training/support/getting-help-from-RCFs/#make-an-appointment","text":"We are happy to arrange meetings outside of designated Office Hours. Email us to schedule a time to meet! support@osg-htc.org","title":"Make an Appointment"},{"location":"support_and_training/support/getting-help-from-RCFs/#training-opportunities","text":"The RCF team runs regular new user training on the first Tuesday of the month and a special topic training on the third Tuesday of the month. See upcoming training dates, registration information, and materials on our training page. OSPool Training page","title":"Training Opportunities"},{"location":"support_and_training/training/materials/","text":"General Overview \u00b6 OSPool Basics: Get Running on the OSPool Learning Objectives Topics covered in this workshop include: An introduction to OSG services and the OSPool Basics of HTCondor job submission Hands-on practice submitting HTCondor jobs Prerequisites/Audience There are no prerequisites for this workshop. This workshop is designed for new HTCondor and OSPool users. Available Materials Presentation Slides Video Recording Wordcount Frequency Tutorial Interactive Lesson Materials Last Updated Winter 2023 HTCondor Tips & Tricks: Using condor_q and condor_history to Learn about Your Jobs Learning Objectives This workshop is designed to introduce researchers to helpful HTCondor tools for learning about their HTCondor jobs. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Materials Last Updated Spring 2023 Principles of Distributed High Throughput Computing Learning Objectives Have you ever wondered about the \u201cwhy\u201d of HTCondor? Join us to hear about the \u201cphilosophy\u201d of high throughput computing and how HTCondor has evolved to make throughput computing possible. This workshop will be led by a core HTCondor developer, Greg Thain, and is a perfect opportunity for longer-term OSPool users to learn more about our underlying technology. Prerequisites/Audience There are no prerequisites for this webinar. Available Materials Presentation Slides Materials Last Updated Spring 2024 Troubleshooting on the OSPool Learning Objectives This session will focus on learning the concepts of debugging and troubleshooting on the OSPool. It will cover some strategies, tips, and tricks that you can use to answer questions such as \"Why are my jobs on hold?\" and \"Why are my jobs stuck on idle?\". At the end of the session is an optional hands-on portion. Prerequisites/Audience Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Some familiarity with shell commands (such as how to edit files, copy/paste in the terminal) and HTCondor commands (such as condor_submit, condor_q) is recommended. Available Materials Video Recording Materials Last Updated Spring 2025 Access the OSPool via Jupyter Interface Learning Objectives This workshop is designed to introduce researchers to the OSPool's new Jupyter interface feature, including how to access and use Jupyter notebooks. Prerequisites/Audience There are no prerequisites for this workshop. Available Materials Presentation Slides Materials Last Updated Fall 2023 Learn About the PATh Facility Learning Objectives The PATh Facility provides dedicated throughput computing capacity to NSF-funded researchers for longer and larger jobs. This training will describe its features and how to get started. If you have found your jobs need more resources (cores, memory, time, data) than is typically available in the OSPool, this resource might be for you! Prerequisites/Audience There are no prerequisites for this webinar. Available Materials Presentation Slides Materials Last Updated Winter 2023 Data \u00b6 Move Your Data with Pelican (and the OSDF) Learning Objectives Pelican is a platform created to enable easier data sharing - within or beyond your institution! This training will cover how Pelican is used to move data within the OSPool and also how you can use Pelican tools to host, upload and download your data. This training is relevant for researchers with large amounts of data, as well as campus representatives, to learn about how Pelican can help with your data movement needs. Prerequisites/Audience There are no prerequisites for this webinar. Available Materials Presentation Slides Video Recording Materials Last Updated Summer 2024 Software \u00b6 Software Portability on the Open Science Pool Learning Objectives This workshop is designed to introduce concepts pertaining to software portability, including containers, different ways to install software, setting file paths, and other important introductory concepts. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides List of Commands Tutorials (used in part) Using Julia on the OSPool High Throughput BWA Read Mapping Materials Last Updated Summer 2023 Using Containerized Software on the Open Science Pool Learning Objectives This workshop is designed to introduce software containers such as Docker, Apptainer, and Singularity. Content covered includes how to create a container, use a container, and techniques for troubleshooting containerized software. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Video Recording Materials Last Updated Fall 2023 Bioinformatics Analyses on the OSPool: A BWA Example Learning Objectives This workshop is designed to show the process of implementing and scaling out a bioinformatics workflow using HTCondor. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Materials Last Updated Summer 2023 Workflows / Advanced Workloads \u00b6 Organizing and Submitting HTC Workloads Learning Objectives This workshop will present useful HTCondor features to help researchers automatically organize their workspaces on High Throughput Computing systems. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Video Recording Wordcount Frequency Tutorial Materials Last Updated Summer 2023 DAGMan: HTCondor's Workflow Manager Learning Objectives Presented by an HTCondor DAGMan developer, this workshop is designed for researchers that would like to learn how to implement DAG workflows and automate workflow management on the OSPool. Prerequisites/Audience A basic understanding of HTCondor job submission and of an HTCondor submit file is highly recommended for this workshop. Available Materials Presentation Slides DAGMan Tutorial Materials Last Updated Winter 2023 Pegasus Workflow Management System on the Open Science Pool Learning Objectives This workshop is designed to introduce Pegasus Workflow Management System, a useful tool for researchers needing to execute a large number of jobs or complex workflows. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Recorded Video Materials Last Updated Fall 2023 Special Environments \u00b6 Special Environments, GPUs Learning Objectives This workshop is designed for researchers interested in learning about using special environments, architectures, or resources such as GPUs. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Materials Last Updated Spring 2023","title":"Past Training Materials"},{"location":"support_and_training/training/materials/#general-overview","text":"OSPool Basics: Get Running on the OSPool Learning Objectives Topics covered in this workshop include: An introduction to OSG services and the OSPool Basics of HTCondor job submission Hands-on practice submitting HTCondor jobs Prerequisites/Audience There are no prerequisites for this workshop. This workshop is designed for new HTCondor and OSPool users. Available Materials Presentation Slides Video Recording Wordcount Frequency Tutorial Interactive Lesson Materials Last Updated Winter 2023 HTCondor Tips & Tricks: Using condor_q and condor_history to Learn about Your Jobs Learning Objectives This workshop is designed to introduce researchers to helpful HTCondor tools for learning about their HTCondor jobs. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Materials Last Updated Spring 2023 Principles of Distributed High Throughput Computing Learning Objectives Have you ever wondered about the \u201cwhy\u201d of HTCondor? Join us to hear about the \u201cphilosophy\u201d of high throughput computing and how HTCondor has evolved to make throughput computing possible. This workshop will be led by a core HTCondor developer, Greg Thain, and is a perfect opportunity for longer-term OSPool users to learn more about our underlying technology. Prerequisites/Audience There are no prerequisites for this webinar. Available Materials Presentation Slides Materials Last Updated Spring 2024 Troubleshooting on the OSPool Learning Objectives This session will focus on learning the concepts of debugging and troubleshooting on the OSPool. It will cover some strategies, tips, and tricks that you can use to answer questions such as \"Why are my jobs on hold?\" and \"Why are my jobs stuck on idle?\". At the end of the session is an optional hands-on portion. Prerequisites/Audience Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Some familiarity with shell commands (such as how to edit files, copy/paste in the terminal) and HTCondor commands (such as condor_submit, condor_q) is recommended. Available Materials Video Recording Materials Last Updated Spring 2025 Access the OSPool via Jupyter Interface Learning Objectives This workshop is designed to introduce researchers to the OSPool's new Jupyter interface feature, including how to access and use Jupyter notebooks. Prerequisites/Audience There are no prerequisites for this workshop. Available Materials Presentation Slides Materials Last Updated Fall 2023 Learn About the PATh Facility Learning Objectives The PATh Facility provides dedicated throughput computing capacity to NSF-funded researchers for longer and larger jobs. This training will describe its features and how to get started. If you have found your jobs need more resources (cores, memory, time, data) than is typically available in the OSPool, this resource might be for you! Prerequisites/Audience There are no prerequisites for this webinar. Available Materials Presentation Slides Materials Last Updated Winter 2023","title":"General Overview"},{"location":"support_and_training/training/materials/#data","text":"Move Your Data with Pelican (and the OSDF) Learning Objectives Pelican is a platform created to enable easier data sharing - within or beyond your institution! This training will cover how Pelican is used to move data within the OSPool and also how you can use Pelican tools to host, upload and download your data. This training is relevant for researchers with large amounts of data, as well as campus representatives, to learn about how Pelican can help with your data movement needs. Prerequisites/Audience There are no prerequisites for this webinar. Available Materials Presentation Slides Video Recording Materials Last Updated Summer 2024","title":"Data"},{"location":"support_and_training/training/materials/#software","text":"Software Portability on the Open Science Pool Learning Objectives This workshop is designed to introduce concepts pertaining to software portability, including containers, different ways to install software, setting file paths, and other important introductory concepts. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides List of Commands Tutorials (used in part) Using Julia on the OSPool High Throughput BWA Read Mapping Materials Last Updated Summer 2023 Using Containerized Software on the Open Science Pool Learning Objectives This workshop is designed to introduce software containers such as Docker, Apptainer, and Singularity. Content covered includes how to create a container, use a container, and techniques for troubleshooting containerized software. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Video Recording Materials Last Updated Fall 2023 Bioinformatics Analyses on the OSPool: A BWA Example Learning Objectives This workshop is designed to show the process of implementing and scaling out a bioinformatics workflow using HTCondor. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Materials Last Updated Summer 2023","title":"Software"},{"location":"support_and_training/training/materials/#workflows-advanced-workloads","text":"Organizing and Submitting HTC Workloads Learning Objectives This workshop will present useful HTCondor features to help researchers automatically organize their workspaces on High Throughput Computing systems. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Video Recording Wordcount Frequency Tutorial Materials Last Updated Summer 2023 DAGMan: HTCondor's Workflow Manager Learning Objectives Presented by an HTCondor DAGMan developer, this workshop is designed for researchers that would like to learn how to implement DAG workflows and automate workflow management on the OSPool. Prerequisites/Audience A basic understanding of HTCondor job submission and of an HTCondor submit file is highly recommended for this workshop. Available Materials Presentation Slides DAGMan Tutorial Materials Last Updated Winter 2023 Pegasus Workflow Management System on the Open Science Pool Learning Objectives This workshop is designed to introduce Pegasus Workflow Management System, a useful tool for researchers needing to execute a large number of jobs or complex workflows. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Recorded Video Materials Last Updated Fall 2023","title":"Workflows / Advanced Workloads"},{"location":"support_and_training/training/materials/#special-environments","text":"Special Environments, GPUs Learning Objectives This workshop is designed for researchers interested in learning about using special environments, architectures, or resources such as GPUs. Prerequisites/Audience There are no prerequisites for this workshop, however, a basic understanding of HTCondor job submission and HTCondor submit files will make it easier to understand the content presented. Available Materials Presentation Slides Materials Last Updated Spring 2023","title":"Special Environments"},{"location":"support_and_training/training/osg-user-school/","text":"Annual, Week-Long OSG User School \u00b6 OSG School 2024 Group Photo Overview \u00b6 During this week-long training event held at the University of Wisconsin-Madison every summer, students learn to use high-throughput computing (HTC) systems \u2014 at their own campus or using the OSG \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, students will learn how HTC systems work, how to run and manage lots of jobs and huge datasets, to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School. Next OSG User School \u00b6 The next OSG User School will be held in the summer of 2025. Applications will likely open in early 2025. Open Materials and Recordings \u00b6 The OSG User School want virtual in 2020 and 2021, which means that we were able to record lectures to complement lecture and exercise materials! OSG Virtual School Pilot, August 2021 OSG Virtual School Pilot, July 2020 Past OSG Schools \u00b6 OSG School, August 5-9, 2024 OSG User School, August 7-11, 2023 OSG User School, July 25-29, 2022 OSG User School, July 15-19, 2019 OSG User School, July 9-13, 2018 OSG User School, July 17-21, 2017 OSG User School, July 25-29, 2016 OSG User School, July 27-31, 2015 OSG User School, July 7-10, 2014","title":"Annual, Week-Long OSG User School "},{"location":"support_and_training/training/osg-user-school/#annual-week-long-osg-user-school","text":"OSG School 2024 Group Photo","title":"Annual, Week-Long OSG User School"},{"location":"support_and_training/training/osg-user-school/#overview","text":"During this week-long training event held at the University of Wisconsin-Madison every summer, students learn to use high-throughput computing (HTC) systems \u2014 at their own campus or using the OSG \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, students will learn how HTC systems work, how to run and manage lots of jobs and huge datasets, to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School.","title":"Overview"},{"location":"support_and_training/training/osg-user-school/#next-osg-user-school","text":"The next OSG User School will be held in the summer of 2025. Applications will likely open in early 2025.","title":"Next OSG User School"},{"location":"support_and_training/training/osg-user-school/#open-materials-and-recordings","text":"The OSG User School want virtual in 2020 and 2021, which means that we were able to record lectures to complement lecture and exercise materials! OSG Virtual School Pilot, August 2021 OSG Virtual School Pilot, July 2020","title":"Open Materials and Recordings"},{"location":"support_and_training/training/osg-user-school/#past-osg-schools","text":"OSG School, August 5-9, 2024 OSG User School, August 7-11, 2023 OSG User School, July 25-29, 2022 OSG User School, July 15-19, 2019 OSG User School, July 9-13, 2018 OSG User School, July 17-21, 2017 OSG User School, July 25-29, 2016 OSG User School, July 27-31, 2015 OSG User School, July 7-10, 2014","title":"Past OSG Schools"},{"location":"support_and_training/training/osgusertraining/","text":"OSG User Training (regular/monthly) \u00b6 All User Training sessions are offered on Tuesdays from 2:30-4pm ET (11:30am - 1pm PT) , on the third Tuesday of the month. The training's are designed as stand alone subjects. You do not need to bring/have your dataset prepared before the training. The only prerequisites are some familiarities with using command line inteface or shell . Having some familiarities with HTCondor job submissions are useful but not required. Registration opens a month before the training date, and closes 24 hours before the event. You can register for all of our trainings via setmore: Register Here Spring 2025 Training Schedule \u00b6 Tuesday, January 21 Troubleshooting on the OSPool Learning Objectives: Topics covered in this workshop include: Categories of job problems How to get more information about a job problem Strategies and considerations for troubleshooting This session will focus on learning the concepts of debugging and troubleshooting on the OSPool. It will cover some strategies, tips, and tricks that you can use to answer questions such as \"Why are my jobs on hold?\" and \"Why are my jobs stuck on idle?\". At the end of the session is an optional hands-on portion. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Some familiarity with shell commands (such as how to edit files, copy/paste in the terminal) and HTCondor commands (such as condor_submit, condor_q) is recommended. Video Recording Tuesday, February 18 Building and Using Containers on the OSPool Learning Objectives: Topics covered in this workshop include: Introduction to containers How to install software in a container How to use a container to deploy your software in an OSPool job Getting your computational program to run on someone else's computer can be a difficult process, especially on the OSPool, where there are many different operating systems with a variety of programs (and versions) installed and you don't have admin permissions. But what if there was a way to make sure your job always ran using your desired operating system, programs, and versions that you chose? In this training, we'll show you how you can achieve this very thing through the use of \"container\" technology. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). Tuesday, March 18 GPUs in the OSPool Learning Objectives: Topics covered in this workshop include: The type and availabitity of GPUs on the OSPool How to run OSPool jobs that use GPUs Recommendations and strategies for using GPUs This session will focus on GPUs in the OSPool, emphasizing their capabilities, availability, and best practices for accessing and utilizing them effectively. The discussion will include common examples and codes to provide practical insights and demonstrate their application. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). Some familiarity with containers is also recommended. Tuesday, April 15 Use Your Data Anywhere Learning Objectives: Topics covered in this workshop include: Overview of data movement on the OSPool How to stage data using the OSDF Using the OSDF in you OSPool jobs In this training, we will introduce attendees to the Open Science Data Federation (OSDF), a data platform that allows you to stage data for both distributed computing (such as on the OSPool) and sharing your data with others. It's even possible to connect your own storage to the OSDF directly, to both share and use yourself. Join us for an overview of the OSDF and hands-on examples of using it. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). Tuesday, May 20 Adapting Workflows for High Throughput Bioinformatics Learning Objectives: Topics covered in this workshop include: Setting up software for the OSPool Organizing your work environment Useful HTCondor submit options In this training, we will create and scale up a BWA bioinformations workflow using HTCondor on the OSPool. In doing so, participants will learn how to set up and use software on the OSPool, how to keep their work environment organized, and strategies for HTCondor job submission. Though the training covers a bioinformatic example, the lessons are applicable to anyone interested in scaling up their OSPool computing. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). For a calendar version of these events see: Google Calendar Download and add to your calendar app Materials \u00b6 All of our training materials are public and provided under the Past Training Materials","title":"Monthly OSG User Training"},{"location":"support_and_training/training/osgusertraining/#osg-user-training-regularmonthly","text":"All User Training sessions are offered on Tuesdays from 2:30-4pm ET (11:30am - 1pm PT) , on the third Tuesday of the month. The training's are designed as stand alone subjects. You do not need to bring/have your dataset prepared before the training. The only prerequisites are some familiarities with using command line inteface or shell . Having some familiarities with HTCondor job submissions are useful but not required. Registration opens a month before the training date, and closes 24 hours before the event. You can register for all of our trainings via setmore: Register Here","title":"OSG User Training (regular/monthly)"},{"location":"support_and_training/training/osgusertraining/#spring-2025-training-schedule","text":"Tuesday, January 21 Troubleshooting on the OSPool Learning Objectives: Topics covered in this workshop include: Categories of job problems How to get more information about a job problem Strategies and considerations for troubleshooting This session will focus on learning the concepts of debugging and troubleshooting on the OSPool. It will cover some strategies, tips, and tricks that you can use to answer questions such as \"Why are my jobs on hold?\" and \"Why are my jobs stuck on idle?\". At the end of the session is an optional hands-on portion. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Some familiarity with shell commands (such as how to edit files, copy/paste in the terminal) and HTCondor commands (such as condor_submit, condor_q) is recommended. Video Recording Tuesday, February 18 Building and Using Containers on the OSPool Learning Objectives: Topics covered in this workshop include: Introduction to containers How to install software in a container How to use a container to deploy your software in an OSPool job Getting your computational program to run on someone else's computer can be a difficult process, especially on the OSPool, where there are many different operating systems with a variety of programs (and versions) installed and you don't have admin permissions. But what if there was a way to make sure your job always ran using your desired operating system, programs, and versions that you chose? In this training, we'll show you how you can achieve this very thing through the use of \"container\" technology. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). Tuesday, March 18 GPUs in the OSPool Learning Objectives: Topics covered in this workshop include: The type and availabitity of GPUs on the OSPool How to run OSPool jobs that use GPUs Recommendations and strategies for using GPUs This session will focus on GPUs in the OSPool, emphasizing their capabilities, availability, and best practices for accessing and utilizing them effectively. The discussion will include common examples and codes to provide practical insights and demonstrate their application. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). Some familiarity with containers is also recommended. Tuesday, April 15 Use Your Data Anywhere Learning Objectives: Topics covered in this workshop include: Overview of data movement on the OSPool How to stage data using the OSDF Using the OSDF in you OSPool jobs In this training, we will introduce attendees to the Open Science Data Federation (OSDF), a data platform that allows you to stage data for both distributed computing (such as on the OSPool) and sharing your data with others. It's even possible to connect your own storage to the OSDF directly, to both share and use yourself. Join us for an overview of the OSDF and hands-on examples of using it. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). Tuesday, May 20 Adapting Workflows for High Throughput Bioinformatics Learning Objectives: Topics covered in this workshop include: Setting up software for the OSPool Organizing your work environment Useful HTCondor submit options In this training, we will create and scale up a BWA bioinformations workflow using HTCondor on the OSPool. In doing so, participants will learn how to set up and use software on the OSPool, how to keep their work environment organized, and strategies for HTCondor job submission. Though the training covers a bioinformatic example, the lessons are applicable to anyone interested in scaling up their OSPool computing. Prerequisites/Audience: Intended for OSPool users who are familiar with logging in and submitting HTCondor jobs to the OSPool. Participants should have some familiarity with shell commands (such as how to edit files, copy/paste in the terminal). For a calendar version of these events see: Google Calendar Download and add to your calendar app","title":"Spring 2025 Training Schedule"},{"location":"support_and_training/training/osgusertraining/#materials","text":"All of our training materials are public and provided under the Past Training Materials","title":"Materials"},{"location":"support_and_training/training/ospool_for_education/","text":"OSPool Resources for Teaching & Education \u00b6 The OSPool provides a free, ready-to-use platform for instructors who are teaching high throughput computing concepts for academic courses, conference workshops, and other events. Instructors can choose for their students to have Guest or Full Accounts on the OSPool. For Guest Accounts, students/attendees can launch an OSPool Notebook at any time and practice job submission with smaller workflows. For Full Accounts, students/attendees will need to request an account (which will be approved within one business day), but then are able to submit large scale high throughput computing workflows free of charge. The table below outlines suggested steps for bringing OSPool resources to your training or event. Please reach out to the facilitation team at any time if you have questions or want to chat about your goals. Explore our Tools Explore HTCondor job submission with a OSPool Guest Account - To launch a guest OSPool Notebook, go to https://notebook.ospool.osg-htc.org using an internet browser. - Visit our OSPool Jupyter Notebooks guide to learn about Guest and Full Accounts Conduct Initial Testing of your Event Materials Using the Guest Account, we recommend conducting initial testing of your event materials to help inform next steps. We provide supplementary materials supplementary materials that you may use to help teach high throughput computing and OSPool-related concepts. Discuss your event goals with a Research Computing Facilitator (Optional) The Facilitation team is here to help discuss your event goals and provide guidance about how to best leverage existing OSG services and resources. Fill out this form and a Facilitator will contact you within one business day about scheduling a short virtual meeting. Evaluate Guest or Full Account for Attendees Option 1: Guest OSPool Accounts You are welcome have your attendees use an OSPool Guest Acccount for the event. This is a good option for events that: - may not know registrants in advance - run less than 4 hours, or can easily recreate files that may be lost upon session time out (4 hours) - want to only use a notebook interface Option 2: Full OSPool Accounts You can request that your students have the ability to submit jobs to the OSPool using Full Accounts. This is a good option for events that: - know registrants in advance - will run for more than 1-2 days - with more than 50 participants - would like jobs to access the full capacity of the OSPool - would like to submit jobs using a notebook or classic terminal interface Prior to the Event If using full accounts, the instructor will provide a list of participants to the OSG Research Facilitation Team, and participants should request an account a few days in advance of the event (does not apply to guest accounts). It is also good practice to test your full workshop code and any software on your account of choice. Start of Event We require all events to provide a short (~5 minute) introduction to OSG Policies . Feedback After your event, please email us to let us know how it went, and the number of participants. Teaching Resources \u00b6 Here are some resources you can use for your event: Worksheets for Public Use \u00b6 Scale Out My Computing Brainstorming Worksheet Slide Presentations for Public Use \u00b6 OSG Policies and Intro for Course Use OSPool Training Slides and Recordings Video Recordings \u00b6 OSPool Training Slides and Recordings HTCondor User Tutorials Partnership to Advance Throughput Computing YouTube channel Frequently Asked Questions (FAQs) \u00b6 Why use OSPool resources for my course/event? OSPool resources provide a free, easy-to-use toolkit for you to use to teach computing concepts at your next course/event. Event attendees do not need an account, but can request continued access to use OSPool resources for their own research. The OSPool staff also offer free assistance with helping you convert an existing workflow to work on the OSPool. We provide guidance about using OSG resources, using HTCondor, and are happy to answer any questions you may have regarding our resources. If I request full accounts for my students/attendees, when will their accounts be deactivated? We work with instructors to choose a date that works well for their event, but typically accounts are deactivated several days after the event completes. If attendees are interested in continuing to use OSPool resources for their research, they can request their account remains active by emailing support@osg-htc.org. Do you have slides and video recordings of workshops that used OSPool resources to help me prepare for my event(s)? Yes! We provide hands-on tutorial materials for topics such as running common software or workflows on the OSPool (e.g., python, R, MATLAB, bioinformatic workflows), recordings of tutorials and introductory materials, presentation slides, and other materials. Some of the materials are linked under the Teaching Resources section above. When should I not use OSPool resources for my course/event? Events are typically bound by the same limitations as regular users/jobs. This means that any event needing to use licensed software or submit individual multi-core jobs or jobs running longer than 20 hours may not be a good fit for our system. Who should I contact with questions or concerns? The OSG Research Computing Facilitation Team is happy to answer any questions or concerns you may have about using OSPool resources for your event(s). Please direct questions to support@osg-htc.org. A Facilitator will respond within one business day.","title":"OSPool Resources for Teaching & Education"},{"location":"support_and_training/training/ospool_for_education/#ospool-resources-for-teaching-education","text":"The OSPool provides a free, ready-to-use platform for instructors who are teaching high throughput computing concepts for academic courses, conference workshops, and other events. Instructors can choose for their students to have Guest or Full Accounts on the OSPool. For Guest Accounts, students/attendees can launch an OSPool Notebook at any time and practice job submission with smaller workflows. For Full Accounts, students/attendees will need to request an account (which will be approved within one business day), but then are able to submit large scale high throughput computing workflows free of charge. The table below outlines suggested steps for bringing OSPool resources to your training or event. Please reach out to the facilitation team at any time if you have questions or want to chat about your goals. Explore our Tools Explore HTCondor job submission with a OSPool Guest Account - To launch a guest OSPool Notebook, go to https://notebook.ospool.osg-htc.org using an internet browser. - Visit our OSPool Jupyter Notebooks guide to learn about Guest and Full Accounts Conduct Initial Testing of your Event Materials Using the Guest Account, we recommend conducting initial testing of your event materials to help inform next steps. We provide supplementary materials supplementary materials that you may use to help teach high throughput computing and OSPool-related concepts. Discuss your event goals with a Research Computing Facilitator (Optional) The Facilitation team is here to help discuss your event goals and provide guidance about how to best leverage existing OSG services and resources. Fill out this form and a Facilitator will contact you within one business day about scheduling a short virtual meeting. Evaluate Guest or Full Account for Attendees Option 1: Guest OSPool Accounts You are welcome have your attendees use an OSPool Guest Acccount for the event. This is a good option for events that: - may not know registrants in advance - run less than 4 hours, or can easily recreate files that may be lost upon session time out (4 hours) - want to only use a notebook interface Option 2: Full OSPool Accounts You can request that your students have the ability to submit jobs to the OSPool using Full Accounts. This is a good option for events that: - know registrants in advance - will run for more than 1-2 days - with more than 50 participants - would like jobs to access the full capacity of the OSPool - would like to submit jobs using a notebook or classic terminal interface Prior to the Event If using full accounts, the instructor will provide a list of participants to the OSG Research Facilitation Team, and participants should request an account a few days in advance of the event (does not apply to guest accounts). It is also good practice to test your full workshop code and any software on your account of choice. Start of Event We require all events to provide a short (~5 minute) introduction to OSG Policies . Feedback After your event, please email us to let us know how it went, and the number of participants.","title":"OSPool Resources for Teaching &amp; Education"},{"location":"support_and_training/training/ospool_for_education/#teaching-resources","text":"Here are some resources you can use for your event:","title":"Teaching Resources"},{"location":"support_and_training/training/ospool_for_education/#worksheets-for-public-use","text":"Scale Out My Computing Brainstorming Worksheet","title":"Worksheets for Public Use"},{"location":"support_and_training/training/ospool_for_education/#slide-presentations-for-public-use","text":"OSG Policies and Intro for Course Use OSPool Training Slides and Recordings","title":"Slide Presentations for Public Use"},{"location":"support_and_training/training/ospool_for_education/#video-recordings","text":"OSPool Training Slides and Recordings HTCondor User Tutorials Partnership to Advance Throughput Computing YouTube channel","title":"Video Recordings"},{"location":"support_and_training/training/ospool_for_education/#frequently-asked-questions-faqs","text":"Why use OSPool resources for my course/event? OSPool resources provide a free, easy-to-use toolkit for you to use to teach computing concepts at your next course/event. Event attendees do not need an account, but can request continued access to use OSPool resources for their own research. The OSPool staff also offer free assistance with helping you convert an existing workflow to work on the OSPool. We provide guidance about using OSG resources, using HTCondor, and are happy to answer any questions you may have regarding our resources. If I request full accounts for my students/attendees, when will their accounts be deactivated? We work with instructors to choose a date that works well for their event, but typically accounts are deactivated several days after the event completes. If attendees are interested in continuing to use OSPool resources for their research, they can request their account remains active by emailing support@osg-htc.org. Do you have slides and video recordings of workshops that used OSPool resources to help me prepare for my event(s)? Yes! We provide hands-on tutorial materials for topics such as running common software or workflows on the OSPool (e.g., python, R, MATLAB, bioinformatic workflows), recordings of tutorials and introductory materials, presentation slides, and other materials. Some of the materials are linked under the Teaching Resources section above. When should I not use OSPool resources for my course/event? Events are typically bound by the same limitations as regular users/jobs. This means that any event needing to use licensed software or submit individual multi-core jobs or jobs running longer than 20 hours may not be a good fit for our system. Who should I contact with questions or concerns? The OSG Research Computing Facilitation Team is happy to answer any questions or concerns you may have about using OSPool resources for your event(s). Please direct questions to support@osg-htc.org. A Facilitator will respond within one business day.","title":"Frequently Asked Questions (FAQs)"},{"location":"support_and_training/training/previous-training-events/","text":"Other Past Training Events \u00b6 Overview \u00b6 We offer on-site training and tutorials on a periodic basis, usually at conferences (including the annual OSG All Hands Meeting) where many researchers and/or research computing staff are gathered. Below are some trainings for which the materials were public. (Apologies if any links/materials aren't accessible anymore, as some of these are external to our own web location. Feel free to let us know via support@osg-htc.org, in case we can fix/remove them.) Workshops/Tutorials \u00b6 Empowering Research Computing at Your Organization Through the OSG (PEARC 21) Organizing and Submitting HTC Workloads (OSG User Training pilot, June 2021) Empower Research Computing at your Organization Through the OSG (RMACC 2021) dHTC Campus Workshop (February 2021) Empowering Research Computing at Your Campus Through the OSG (PEARC 20) Deploy jobs on the Open Science Grid (Gateways/eScience 2019) High Throughput Computation on the Open Science Grid (Internet2 2018 Technology Exchange) Open Science Grid Workshop (The Quilt 2018) High Throughput Computation on the Open Science Grid (RMACC 18) Tutorials at Recent OSG All-Hands Meetings \u00b6 The below were offered on-site at OSG All-Hands Meetings. Note that the last on-site AHM in 2020 was canceled due to the pandemic, though we've linked to the materials. User/Facilitator Training at the OSG All Hands Meeting, University of Oklahoma (OU), March 2020 User Training at the OSG All Hands Meeting, Thomas Jefferson National Accelerator Facility (JLAB), March 2019 User Training at the OSG All Hands Meeting, University of Utah, March 2018","title":"Other Past Training Events "},{"location":"support_and_training/training/previous-training-events/#other-past-training-events","text":"","title":"Other Past Training Events"},{"location":"support_and_training/training/previous-training-events/#overview","text":"We offer on-site training and tutorials on a periodic basis, usually at conferences (including the annual OSG All Hands Meeting) where many researchers and/or research computing staff are gathered. Below are some trainings for which the materials were public. (Apologies if any links/materials aren't accessible anymore, as some of these are external to our own web location. Feel free to let us know via support@osg-htc.org, in case we can fix/remove them.)","title":"Overview"},{"location":"support_and_training/training/previous-training-events/#workshopstutorials","text":"Empowering Research Computing at Your Organization Through the OSG (PEARC 21) Organizing and Submitting HTC Workloads (OSG User Training pilot, June 2021) Empower Research Computing at your Organization Through the OSG (RMACC 2021) dHTC Campus Workshop (February 2021) Empowering Research Computing at Your Campus Through the OSG (PEARC 20) Deploy jobs on the Open Science Grid (Gateways/eScience 2019) High Throughput Computation on the Open Science Grid (Internet2 2018 Technology Exchange) Open Science Grid Workshop (The Quilt 2018) High Throughput Computation on the Open Science Grid (RMACC 18)","title":"Workshops/Tutorials"},{"location":"support_and_training/training/previous-training-events/#tutorials-at-recent-osg-all-hands-meetings","text":"The below were offered on-site at OSG All-Hands Meetings. Note that the last on-site AHM in 2020 was canceled due to the pandemic, though we've linked to the materials. User/Facilitator Training at the OSG All Hands Meeting, University of Oklahoma (OU), March 2020 User Training at the OSG All Hands Meeting, Thomas Jefferson National Accelerator Facility (JLAB), March 2019 User Training at the OSG All Hands Meeting, University of Utah, March 2018","title":"Tutorials at Recent OSG All-Hands Meetings"}]}